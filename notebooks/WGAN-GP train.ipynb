{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc5ec0ef8f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import time as t\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import time as time\n",
    "\n",
    "from torch import autograd\n",
    "\n",
    "import model\n",
    "from keijzer_exogan import *\n",
    "\n",
    "# initialize random seeds\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Local variables\n",
    "\"\"\"\n",
    "workers = 0 # Number of workers for dataloader, 0 when to_vram is enabled\n",
    "batch_size = 64 # 2**11\n",
    "image_size = 32\n",
    "nz = 100 # size of latent vector\n",
    "num_epochs = 10*10**3\n",
    "torch.backends.cudnn.benchmark=True # Uses udnn auto-tuner to find the best algorithm to use for your hardware, speeds up training by almost 50%\n",
    "lr = 1e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9\n",
    "\n",
    "lambda_ = 10\n",
    "\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "selected_gpus = [2,3] # Number of GPUs available. Use 0 for CPU mode.\n",
    "\n",
    "path = '/datb/16011015/ExoGAN_data/selection//' #notice how you dont put the last folder in here...\n",
    "images = np.load(path+'first_chunks_25_percent_images.npy')\n",
    "\n",
    "swap_labels_randomly = False\n",
    "\n",
    "train_d_g_conditional = False # switch between training D and G based on set threshold\n",
    "d_g_conditional_threshold = 0.55 # D_G_z1 < threshold, train G\n",
    "\n",
    "train_d_g_conditional_per_epoch = False\n",
    "\n",
    "train_d_g_conditional_per_n_iters = False\n",
    "train_d_g_n_iters = 2 # When 2, train D 2 times before training G 1 time\n",
    "\n",
    "use_saved_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  64\n",
      "Number of GPUs used:  2\n",
      "Number of images:  50000\n"
     ]
    }
   ],
   "source": [
    "print('Batch size: ', batch_size)\n",
    "ngpu = len(selected_gpus)\n",
    "print('Number of GPUs used: ', ngpu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load data and prepare DataLoader\n",
    "\"\"\"\n",
    "shuffle = True\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(images) # shuffles the images\n",
    "\n",
    "images = images[:int(len(images)*0.1)] # use only first ... percent of the data (0.05)\n",
    "print('Number of images: ', len(images))\n",
    "\n",
    "dataset = numpy_dataset(data=images, to_vram=True) # to_vram pins it to all GPU's\n",
    "#dataset = numpy_dataset(data=images, to_vram=True, transform=transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])) # to_vram pins it to all GPU's\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load and setup models\n",
    "\"\"\"\n",
    "# Initialize cuda\n",
    "device = torch.device(\"cuda:\"+str(selected_gpus[0]) if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Load models\n",
    "netG = model.Generator(ngpu).to(device)\n",
    "netD = model.Discriminator(ngpu).to(device)\n",
    "\n",
    "# Apply weights\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netG.apply(weights_init) # It's not clean/efficient to load these ones first, but it works.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "if use_saved_weights:\n",
    "    try:\n",
    "        # Load saved weights\n",
    "        netG.load_state_dict(torch.load('netG_state_dict2', map_location=device)) #net.module..load_... for parallel model , net.load_... for single gpu model\n",
    "        netD.load_state_dict(torch.load('netD_state_dict2', map_location=device))\n",
    "        print('Succesfully loaded saved weights.')\n",
    "    except:\n",
    "        print('Could not load saved weights, using new ones.')\n",
    "        pass\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, device_ids=selected_gpus, output_device=device)\n",
    "    netD = nn.DataParallel(netD, device_ids=selected_gpus, output_device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define input training stuff (fancy this up)\n",
    "\"\"\"\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2)) # should be sgd\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "switch = True # condition switch, to switch between D and G per epoch\n",
    "previous_switch = 0\n",
    "\n",
    "train_D = True\n",
    "train_G = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data, b_size):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n",
    "    \"\"\"\n",
    "    alpha = torch.rand(b_size, 1)\n",
    "    alpha = alpha.expand(b_size, int(real_data.nelement()/b_size)).contiguous()\n",
    "    alpha = alpha.view(b_size, 1, image_size, image_size)\n",
    "    alpha = alpha.to(device)\n",
    "    \n",
    "    fake_data = fake_data.view(b_size, 1, image_size, image_size)\n",
    "    interpolates = alpha * real_data.detach() + ((1 - alpha) * fake_data.detach())\n",
    "\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates.requires_grad_(True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)                              \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switched to WGAN-GP (https://arxiv.org/pdf/1704.00028) instead of DCGAN for training stability reasons.  \n",
    "Wheras DCGAN collapsed (D(x) staying 0.8-0.9 and D(G(z)) becoming 0.5000) to output black images, WGAN-GP keeps training well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000][0/782] G loss: -0.500 \t D loss: 9.999 \t D(x) = 0.500 \t D(G(z)) = 0.500 \t grad_pen = 9.999 \t t = 2.484 \t\n",
      "[0/10000][256/782] G loss: -0.355 \t D loss: 2.018 \t D(x) = 0.124 \t D(G(z)) = 0.366 \t grad_pen = 1.776 \t t = 21.757 \t\n",
      "[0/10000][512/782] G loss: -0.592 \t D loss: 1.845 \t D(x) = 0.175 \t D(G(z)) = 0.632 \t grad_pen = 1.388 \t t = 21.594 \t\n",
      "[0/10000][768/782] G loss: -0.392 \t D loss: 1.153 \t D(x) = 0.198 \t D(G(z)) = 0.429 \t grad_pen = 0.923 \t t = 21.603 \t\n",
      "[1/10000][0/782] G loss: -0.486 \t D loss: 1.408 \t D(x) = 0.123 \t D(G(z)) = 0.504 \t grad_pen = 1.026 \t t = 1.244 \t\n",
      "[1/10000][256/782] G loss: -0.681 \t D loss: 1.038 \t D(x) = 0.121 \t D(G(z)) = 0.619 \t grad_pen = 0.540 \t t = 21.636 \t\n",
      "[1/10000][512/782] G loss: -0.618 \t D loss: 1.012 \t D(x) = 0.136 \t D(G(z)) = 0.573 \t grad_pen = 0.574 \t t = 21.621 \t\n",
      "[1/10000][768/782] G loss: -0.495 \t D loss: 0.540 \t D(x) = 0.219 \t D(G(z)) = 0.532 \t grad_pen = 0.226 \t t = 21.755 \t\n",
      "[2/10000][0/782] G loss: -0.464 \t D loss: 0.551 \t D(x) = 0.309 \t D(G(z)) = 0.571 \t grad_pen = 0.289 \t t = 1.148 \t\n",
      "[2/10000][256/782] G loss: -0.539 \t D loss: 0.304 \t D(x) = 0.538 \t D(G(z)) = 0.574 \t grad_pen = 0.269 \t t = 21.650 \t\n",
      "[2/10000][512/782] G loss: -0.468 \t D loss: 0.028 \t D(x) = 0.599 \t D(G(z)) = 0.520 \t grad_pen = 0.107 \t t = 21.539 \t\n",
      "[2/10000][768/782] G loss: -0.515 \t D loss: -0.019 \t D(x) = 0.593 \t D(G(z)) = 0.505 \t grad_pen = 0.070 \t t = 21.647 \t\n",
      "[3/10000][0/782] G loss: -0.514 \t D loss: -0.080 \t D(x) = 0.666 \t D(G(z)) = 0.505 \t grad_pen = 0.080 \t t = 1.161 \t\n",
      "[3/10000][256/782] G loss: -0.527 \t D loss: -0.024 \t D(x) = 0.559 \t D(G(z)) = 0.457 \t grad_pen = 0.078 \t t = 21.692 \t\n",
      "[3/10000][512/782] G loss: -0.523 \t D loss: 0.045 \t D(x) = 0.727 \t D(G(z)) = 0.565 \t grad_pen = 0.207 \t t = 21.623 \t\n",
      "[3/10000][768/782] G loss: -0.510 \t D loss: -0.027 \t D(x) = 0.537 \t D(G(z)) = 0.452 \t grad_pen = 0.058 \t t = 21.619 \t\n",
      "[4/10000][0/782] G loss: -0.495 \t D loss: -0.072 \t D(x) = 0.638 \t D(G(z)) = 0.492 \t grad_pen = 0.073 \t t = 1.149 \t\n",
      "[4/10000][256/782] G loss: -0.533 \t D loss: -0.067 \t D(x) = 0.620 \t D(G(z)) = 0.502 \t grad_pen = 0.051 \t t = 21.710 \t\n",
      "[4/10000][512/782] G loss: -0.545 \t D loss: -0.009 \t D(x) = 0.664 \t D(G(z)) = 0.550 \t grad_pen = 0.105 \t t = 21.643 \t\n",
      "[4/10000][768/782] G loss: -0.510 \t D loss: -0.122 \t D(x) = 0.718 \t D(G(z)) = 0.517 \t grad_pen = 0.079 \t t = 21.680 \t\n",
      "[5/10000][0/782] G loss: -0.509 \t D loss: -0.119 \t D(x) = 0.692 \t D(G(z)) = 0.507 \t grad_pen = 0.066 \t t = 1.150 \t\n",
      "[5/10000][256/782] G loss: -0.999 \t D loss: 2.589 \t D(x) = 0.856 \t D(G(z)) = 0.999 \t grad_pen = 2.446 \t t = 21.706 \t\n",
      "[5/10000][512/782] G loss: -0.416 \t D loss: -0.112 \t D(x) = 0.678 \t D(G(z)) = 0.430 \t grad_pen = 0.136 \t t = 21.583 \t\n",
      "[5/10000][768/782] G loss: -0.481 \t D loss: -0.101 \t D(x) = 0.616 \t D(G(z)) = 0.452 \t grad_pen = 0.063 \t t = 21.602 \t\n",
      "[6/10000][0/782] G loss: -0.546 \t D loss: -0.131 \t D(x) = 0.688 \t D(G(z)) = 0.508 \t grad_pen = 0.048 \t t = 1.150 \t\n",
      "[6/10000][256/782] G loss: -0.518 \t D loss: -0.122 \t D(x) = 0.652 \t D(G(z)) = 0.493 \t grad_pen = 0.037 \t t = 21.589 \t\n",
      "[6/10000][512/782] G loss: -0.497 \t D loss: -0.144 \t D(x) = 0.724 \t D(G(z)) = 0.528 \t grad_pen = 0.051 \t t = 21.592 \t\n",
      "[6/10000][768/782] G loss: -0.505 \t D loss: -0.118 \t D(x) = 0.750 \t D(G(z)) = 0.537 \t grad_pen = 0.095 \t t = 21.585 \t\n",
      "[7/10000][0/782] G loss: -0.507 \t D loss: -0.146 \t D(x) = 0.663 \t D(G(z)) = 0.474 \t grad_pen = 0.043 \t t = 1.141 \t\n",
      "[7/10000][256/782] G loss: -0.496 \t D loss: -0.141 \t D(x) = 0.722 \t D(G(z)) = 0.519 \t grad_pen = 0.062 \t t = 21.653 \t\n",
      "[7/10000][512/782] G loss: -0.544 \t D loss: -0.101 \t D(x) = 0.636 \t D(G(z)) = 0.475 \t grad_pen = 0.060 \t t = 21.826 \t\n",
      "[7/10000][768/782] G loss: -0.534 \t D loss: -0.115 \t D(x) = 0.665 \t D(G(z)) = 0.495 \t grad_pen = 0.055 \t t = 21.739 \t\n",
      "[8/10000][0/782] G loss: -0.537 \t D loss: -0.105 \t D(x) = 0.671 \t D(G(z)) = 0.497 \t grad_pen = 0.070 \t t = 1.135 \t\n",
      "[8/10000][256/782] G loss: -0.541 \t D loss: -0.094 \t D(x) = 0.725 \t D(G(z)) = 0.530 \t grad_pen = 0.102 \t t = 21.617 \t\n",
      "[8/10000][512/782] G loss: -0.513 \t D loss: -0.106 \t D(x) = 0.664 \t D(G(z)) = 0.499 \t grad_pen = 0.059 \t t = 21.635 \t\n",
      "[8/10000][768/782] G loss: -0.500 \t D loss: -0.091 \t D(x) = 0.610 \t D(G(z)) = 0.483 \t grad_pen = 0.037 \t t = 21.634 \t\n",
      "[9/10000][0/782] G loss: -0.551 \t D loss: -0.055 \t D(x) = 0.595 \t D(G(z)) = 0.475 \t grad_pen = 0.066 \t t = 1.145 \t\n",
      "[9/10000][256/782] G loss: -0.521 \t D loss: -0.124 \t D(x) = 0.688 \t D(G(z)) = 0.520 \t grad_pen = 0.043 \t t = 21.659 \t\n",
      "[9/10000][512/782] G loss: -0.531 \t D loss: -0.132 \t D(x) = 0.663 \t D(G(z)) = 0.497 \t grad_pen = 0.034 \t t = 21.697 \t\n",
      "[9/10000][768/782] G loss: -0.452 \t D loss: -0.100 \t D(x) = 0.676 \t D(G(z)) = 0.510 \t grad_pen = 0.066 \t t = 21.684 \t\n",
      "[10/10000][0/782] G loss: -0.484 \t D loss: -0.058 \t D(x) = 0.714 \t D(G(z)) = 0.549 \t grad_pen = 0.107 \t t = 1.139 \t\n",
      "[10/10000][256/782] G loss: -0.480 \t D loss: -0.138 \t D(x) = 0.673 \t D(G(z)) = 0.491 \t grad_pen = 0.044 \t t = 21.637 \t\n",
      "[10/10000][512/782] G loss: -0.435 \t D loss: -0.068 \t D(x) = 0.562 \t D(G(z)) = 0.459 \t grad_pen = 0.036 \t t = 21.736 \t\n",
      "[10/10000][768/782] G loss: -0.461 \t D loss: -0.073 \t D(x) = 0.676 \t D(G(z)) = 0.516 \t grad_pen = 0.087 \t t = 21.681 \t\n",
      "[11/10000][0/782] G loss: -0.424 \t D loss: -0.052 \t D(x) = 0.543 \t D(G(z)) = 0.435 \t grad_pen = 0.056 \t t = 1.156 \t\n",
      "[11/10000][256/782] G loss: -0.513 \t D loss: -0.144 \t D(x) = 0.719 \t D(G(z)) = 0.521 \t grad_pen = 0.054 \t t = 21.520 \t\n",
      "[11/10000][512/782] G loss: -0.524 \t D loss: -0.087 \t D(x) = 0.729 \t D(G(z)) = 0.545 \t grad_pen = 0.097 \t t = 21.604 \t\n",
      "[11/10000][768/782] G loss: -0.960 \t D loss: 0.425 \t D(x) = 0.693 \t D(G(z)) = 0.944 \t grad_pen = 0.174 \t t = 21.701 \t\n",
      "[12/10000][0/782] G loss: -0.947 \t D loss: 0.412 \t D(x) = 0.700 \t D(G(z)) = 0.947 \t grad_pen = 0.165 \t t = 1.134 \t\n",
      "[12/10000][256/782] G loss: -0.363 \t D loss: -0.060 \t D(x) = 0.559 \t D(G(z)) = 0.330 \t grad_pen = 0.169 \t t = 21.627 \t\n",
      "[12/10000][512/782] G loss: -0.500 \t D loss: -0.103 \t D(x) = 0.684 \t D(G(z)) = 0.486 \t grad_pen = 0.095 \t t = 21.630 \t\n",
      "[12/10000][768/782] G loss: -0.482 \t D loss: -0.087 \t D(x) = 0.710 \t D(G(z)) = 0.508 \t grad_pen = 0.115 \t t = 21.577 \t\n",
      "[13/10000][0/782] G loss: -0.471 \t D loss: -0.104 \t D(x) = 0.666 \t D(G(z)) = 0.469 \t grad_pen = 0.093 \t t = 1.144 \t\n",
      "[13/10000][256/782] G loss: -0.530 \t D loss: -0.090 \t D(x) = 0.747 \t D(G(z)) = 0.555 \t grad_pen = 0.103 \t t = 21.643 \t\n",
      "[13/10000][512/782] G loss: -0.510 \t D loss: -0.122 \t D(x) = 0.704 \t D(G(z)) = 0.530 \t grad_pen = 0.052 \t t = 21.625 \t\n",
      "[13/10000][768/782] G loss: -0.518 \t D loss: -0.130 \t D(x) = 0.711 \t D(G(z)) = 0.518 \t grad_pen = 0.063 \t t = 21.645 \t\n",
      "[14/10000][0/782] G loss: -0.540 \t D loss: -0.088 \t D(x) = 0.729 \t D(G(z)) = 0.573 \t grad_pen = 0.068 \t t = 1.149 \t\n",
      "[14/10000][256/782] G loss: -0.596 \t D loss: 0.038 \t D(x) = 0.448 \t D(G(z)) = 0.407 \t grad_pen = 0.079 \t t = 21.685 \t\n",
      "[14/10000][512/782] G loss: -0.510 \t D loss: -0.089 \t D(x) = 0.662 \t D(G(z)) = 0.520 \t grad_pen = 0.054 \t t = 21.592 \t\n",
      "[14/10000][768/782] G loss: -0.517 \t D loss: -0.086 \t D(x) = 0.705 \t D(G(z)) = 0.546 \t grad_pen = 0.073 \t t = 21.681 \t\n",
      "[15/10000][0/782] G loss: -0.562 \t D loss: -0.077 \t D(x) = 0.692 \t D(G(z)) = 0.538 \t grad_pen = 0.077 \t t = 1.147 \t\n",
      "[15/10000][256/782] G loss: -0.429 \t D loss: -0.065 \t D(x) = 0.543 \t D(G(z)) = 0.431 \t grad_pen = 0.047 \t t = 21.684 \t\n",
      "[15/10000][512/782] G loss: -0.443 \t D loss: 0.015 \t D(x) = 0.585 \t D(G(z)) = 0.460 \t grad_pen = 0.140 \t t = 21.650 \t\n",
      "[15/10000][768/782] G loss: -0.444 \t D loss: -0.098 \t D(x) = 0.610 \t D(G(z)) = 0.470 \t grad_pen = 0.042 \t t = 21.597 \t\n",
      "[16/10000][0/782] G loss: -0.512 \t D loss: -0.110 \t D(x) = 0.659 \t D(G(z)) = 0.512 \t grad_pen = 0.038 \t t = 1.156 \t\n",
      "[16/10000][256/782] G loss: -0.456 \t D loss: -0.100 \t D(x) = 0.644 \t D(G(z)) = 0.505 \t grad_pen = 0.040 \t t = 21.627 \t\n",
      "[16/10000][512/782] G loss: -0.543 \t D loss: -0.099 \t D(x) = 0.616 \t D(G(z)) = 0.481 \t grad_pen = 0.036 \t t = 21.558 \t\n",
      "[16/10000][768/782] G loss: -0.579 \t D loss: -0.074 \t D(x) = 0.558 \t D(G(z)) = 0.421 \t grad_pen = 0.063 \t t = 21.587 \t\n",
      "[17/10000][0/782] G loss: -0.583 \t D loss: -0.089 \t D(x) = 0.592 \t D(G(z)) = 0.465 \t grad_pen = 0.038 \t t = 1.145 \t\n",
      "[17/10000][256/782] G loss: -0.494 \t D loss: -0.092 \t D(x) = 0.649 \t D(G(z)) = 0.508 \t grad_pen = 0.050 \t t = 21.668 \t\n",
      "[17/10000][512/782] G loss: -0.441 \t D loss: -0.102 \t D(x) = 0.659 \t D(G(z)) = 0.507 \t grad_pen = 0.051 \t t = 21.625 \t\n",
      "[17/10000][768/782] G loss: -0.446 \t D loss: -0.077 \t D(x) = 0.563 \t D(G(z)) = 0.448 \t grad_pen = 0.038 \t t = 21.615 \t\n",
      "[18/10000][0/782] G loss: -0.411 \t D loss: -0.114 \t D(x) = 0.633 \t D(G(z)) = 0.470 \t grad_pen = 0.049 \t t = 1.142 \t\n",
      "[18/10000][256/782] G loss: -0.444 \t D loss: -0.124 \t D(x) = 0.649 \t D(G(z)) = 0.499 \t grad_pen = 0.025 \t t = 21.617 \t\n",
      "[18/10000][512/782] G loss: -0.485 \t D loss: -0.132 \t D(x) = 0.676 \t D(G(z)) = 0.496 \t grad_pen = 0.048 \t t = 21.577 \t\n",
      "[18/10000][768/782] G loss: -0.502 \t D loss: -0.129 \t D(x) = 0.639 \t D(G(z)) = 0.480 \t grad_pen = 0.030 \t t = 21.613 \t\n",
      "[19/10000][0/782] G loss: -0.556 \t D loss: -0.107 \t D(x) = 0.646 \t D(G(z)) = 0.480 \t grad_pen = 0.059 \t t = 1.145 \t\n",
      "[19/10000][256/782] G loss: -0.549 \t D loss: -0.100 \t D(x) = 0.568 \t D(G(z)) = 0.441 \t grad_pen = 0.027 \t t = 21.638 \t\n",
      "[19/10000][512/782] G loss: -0.449 \t D loss: -0.092 \t D(x) = 0.603 \t D(G(z)) = 0.457 \t grad_pen = 0.054 \t t = 21.583 \t\n",
      "[19/10000][768/782] G loss: -0.463 \t D loss: -0.115 \t D(x) = 0.664 \t D(G(z)) = 0.503 \t grad_pen = 0.045 \t t = 21.555 \t\n",
      "[20/10000][0/782] G loss: -0.519 \t D loss: -0.144 \t D(x) = 0.689 \t D(G(z)) = 0.499 \t grad_pen = 0.046 \t t = 1.143 \t\n",
      "[20/10000][256/782] G loss: -0.562 \t D loss: -0.026 \t D(x) = 0.527 \t D(G(z)) = 0.412 \t grad_pen = 0.089 \t t = 21.637 \t\n",
      "[20/10000][512/782] G loss: -0.538 \t D loss: -0.115 \t D(x) = 0.753 \t D(G(z)) = 0.570 \t grad_pen = 0.068 \t t = 21.624 \t\n",
      "[20/10000][768/782] G loss: -0.459 \t D loss: -0.112 \t D(x) = 0.715 \t D(G(z)) = 0.528 \t grad_pen = 0.075 \t t = 21.608 \t\n",
      "[21/10000][0/782] G loss: -0.549 \t D loss: -0.137 \t D(x) = 0.681 \t D(G(z)) = 0.503 \t grad_pen = 0.040 \t t = 1.140 \t\n",
      "[21/10000][256/782] G loss: -0.516 \t D loss: -0.140 \t D(x) = 0.648 \t D(G(z)) = 0.465 \t grad_pen = 0.043 \t t = 21.604 \t\n",
      "[21/10000][512/782] G loss: -0.457 \t D loss: -0.139 \t D(x) = 0.754 \t D(G(z)) = 0.551 \t grad_pen = 0.063 \t t = 21.531 \t\n",
      "[21/10000][768/782] G loss: -0.445 \t D loss: -0.112 \t D(x) = 0.599 \t D(G(z)) = 0.463 \t grad_pen = 0.025 \t t = 21.591 \t\n",
      "[22/10000][0/782] G loss: -0.546 \t D loss: -0.136 \t D(x) = 0.702 \t D(G(z)) = 0.522 \t grad_pen = 0.045 \t t = 1.148 \t\n",
      "[22/10000][256/782] G loss: -0.464 \t D loss: -0.134 \t D(x) = 0.635 \t D(G(z)) = 0.477 \t grad_pen = 0.024 \t t = 21.576 \t\n",
      "[22/10000][512/782] G loss: -0.509 \t D loss: -0.110 \t D(x) = 0.763 \t D(G(z)) = 0.569 \t grad_pen = 0.085 \t t = 21.598 \t\n",
      "[22/10000][768/782] G loss: -0.495 \t D loss: -0.135 \t D(x) = 0.705 \t D(G(z)) = 0.504 \t grad_pen = 0.066 \t t = 21.609 \t\n",
      "[23/10000][0/782] G loss: -0.460 \t D loss: -0.055 \t D(x) = 0.497 \t D(G(z)) = 0.390 \t grad_pen = 0.052 \t t = 1.141 \t\n",
      "[23/10000][256/782] G loss: -0.510 \t D loss: -0.140 \t D(x) = 0.669 \t D(G(z)) = 0.493 \t grad_pen = 0.036 \t t = 21.649 \t\n",
      "[23/10000][512/782] G loss: -0.485 \t D loss: -0.144 \t D(x) = 0.710 \t D(G(z)) = 0.527 \t grad_pen = 0.039 \t t = 21.609 \t\n",
      "[23/10000][768/782] G loss: -0.490 \t D loss: -0.128 \t D(x) = 0.608 \t D(G(z)) = 0.459 \t grad_pen = 0.022 \t t = 21.689 \t\n",
      "[24/10000][0/782] G loss: -0.534 \t D loss: -0.084 \t D(x) = 0.759 \t D(G(z)) = 0.589 \t grad_pen = 0.086 \t t = 1.149 \t\n",
      "[24/10000][256/782] G loss: -0.576 \t D loss: -0.117 \t D(x) = 0.576 \t D(G(z)) = 0.433 \t grad_pen = 0.026 \t t = 21.721 \t\n",
      "[24/10000][512/782] G loss: -0.481 \t D loss: -0.171 \t D(x) = 0.705 \t D(G(z)) = 0.507 \t grad_pen = 0.026 \t t = 21.641 \t\n",
      "[24/10000][768/782] G loss: -0.439 \t D loss: -0.151 \t D(x) = 0.703 \t D(G(z)) = 0.512 \t grad_pen = 0.040 \t t = 21.630 \t\n",
      "[25/10000][0/782] G loss: -0.495 \t D loss: -0.164 \t D(x) = 0.639 \t D(G(z)) = 0.444 \t grad_pen = 0.031 \t t = 1.152 \t\n",
      "[25/10000][256/782] G loss: -0.520 \t D loss: -0.097 \t D(x) = 0.759 \t D(G(z)) = 0.569 \t grad_pen = 0.092 \t t = 21.677 \t\n",
      "[25/10000][512/782] G loss: -0.485 \t D loss: -0.154 \t D(x) = 0.674 \t D(G(z)) = 0.476 \t grad_pen = 0.043 \t t = 21.626 \t\n",
      "[25/10000][768/782] G loss: -0.547 \t D loss: -0.148 \t D(x) = 0.686 \t D(G(z)) = 0.508 \t grad_pen = 0.030 \t t = 21.608 \t\n",
      "[26/10000][0/782] G loss: -0.443 \t D loss: -0.156 \t D(x) = 0.692 \t D(G(z)) = 0.506 \t grad_pen = 0.030 \t t = 1.158 \t\n",
      "[26/10000][256/782] G loss: -0.517 \t D loss: -0.124 \t D(x) = 0.667 \t D(G(z)) = 0.495 \t grad_pen = 0.048 \t t = 21.638 \t\n",
      "[26/10000][512/782] G loss: -0.470 \t D loss: -0.133 \t D(x) = 0.659 \t D(G(z)) = 0.487 \t grad_pen = 0.039 \t t = 21.534 \t\n",
      "[26/10000][768/782] G loss: -0.490 \t D loss: -0.140 \t D(x) = 0.636 \t D(G(z)) = 0.470 \t grad_pen = 0.026 \t t = 21.633 \t\n",
      "[27/10000][0/782] G loss: -0.517 \t D loss: -0.113 \t D(x) = 0.598 \t D(G(z)) = 0.452 \t grad_pen = 0.034 \t t = 1.148 \t\n",
      "[27/10000][256/782] G loss: -0.477 \t D loss: -0.161 \t D(x) = 0.673 \t D(G(z)) = 0.474 \t grad_pen = 0.038 \t t = 21.640 \t\n",
      "[27/10000][512/782] G loss: -0.437 \t D loss: -0.140 \t D(x) = 0.650 \t D(G(z)) = 0.474 \t grad_pen = 0.036 \t t = 21.533 \t\n",
      "[27/10000][768/782] G loss: -0.504 \t D loss: -0.139 \t D(x) = 0.780 \t D(G(z)) = 0.569 \t grad_pen = 0.072 \t t = 21.665 \t\n",
      "[28/10000][0/782] G loss: -0.529 \t D loss: -0.164 \t D(x) = 0.739 \t D(G(z)) = 0.548 \t grad_pen = 0.026 \t t = 1.139 \t\n",
      "[28/10000][256/782] G loss: -0.430 \t D loss: -0.152 \t D(x) = 0.739 \t D(G(z)) = 0.524 \t grad_pen = 0.063 \t t = 21.542 \t\n",
      "[28/10000][512/782] G loss: -0.477 \t D loss: -0.175 \t D(x) = 0.741 \t D(G(z)) = 0.516 \t grad_pen = 0.050 \t t = 21.639 \t\n",
      "[28/10000][768/782] G loss: -0.505 \t D loss: -0.098 \t D(x) = 0.562 \t D(G(z)) = 0.428 \t grad_pen = 0.036 \t t = 21.699 \t\n",
      "[29/10000][0/782] G loss: -0.472 \t D loss: -0.129 \t D(x) = 0.645 \t D(G(z)) = 0.472 \t grad_pen = 0.044 \t t = 1.146 \t\n",
      "[29/10000][256/782] G loss: -0.588 \t D loss: -0.150 \t D(x) = 0.691 \t D(G(z)) = 0.500 \t grad_pen = 0.041 \t t = 21.618 \t\n",
      "[29/10000][512/782] G loss: -0.501 \t D loss: -0.143 \t D(x) = 0.729 \t D(G(z)) = 0.512 \t grad_pen = 0.074 \t t = 21.613 \t\n",
      "[29/10000][768/782] G loss: -0.494 \t D loss: -0.124 \t D(x) = 0.580 \t D(G(z)) = 0.424 \t grad_pen = 0.031 \t t = 21.721 \t\n",
      "[30/10000][0/782] G loss: -0.448 \t D loss: -0.127 \t D(x) = 0.617 \t D(G(z)) = 0.454 \t grad_pen = 0.035 \t t = 1.182 \t\n",
      "[30/10000][256/782] G loss: -0.496 \t D loss: -0.159 \t D(x) = 0.747 \t D(G(z)) = 0.537 \t grad_pen = 0.052 \t t = 21.600 \t\n",
      "[30/10000][512/782] G loss: -0.505 \t D loss: -0.134 \t D(x) = 0.711 \t D(G(z)) = 0.522 \t grad_pen = 0.055 \t t = 21.666 \t\n",
      "[30/10000][768/782] G loss: -0.505 \t D loss: -0.138 \t D(x) = 0.695 \t D(G(z)) = 0.506 \t grad_pen = 0.051 \t t = 21.745 \t\n",
      "[31/10000][0/782] G loss: -0.432 \t D loss: -0.142 \t D(x) = 0.649 \t D(G(z)) = 0.467 \t grad_pen = 0.040 \t t = 1.146 \t\n",
      "[31/10000][256/782] G loss: -0.510 \t D loss: -0.144 \t D(x) = 0.794 \t D(G(z)) = 0.568 \t grad_pen = 0.082 \t t = 21.601 \t\n",
      "[31/10000][512/782] G loss: -0.485 \t D loss: -0.179 \t D(x) = 0.727 \t D(G(z)) = 0.513 \t grad_pen = 0.035 \t t = 21.583 \t\n",
      "[31/10000][768/782] G loss: -0.540 \t D loss: -0.157 \t D(x) = 0.727 \t D(G(z)) = 0.523 \t grad_pen = 0.047 \t t = 21.717 \t\n",
      "[32/10000][0/782] G loss: -0.545 \t D loss: -0.122 \t D(x) = 0.607 \t D(G(z)) = 0.456 \t grad_pen = 0.030 \t t = 1.146 \t\n",
      "[32/10000][256/782] G loss: -0.565 \t D loss: -0.178 \t D(x) = 0.715 \t D(G(z)) = 0.494 \t grad_pen = 0.042 \t t = 21.746 \t\n",
      "[32/10000][512/782] G loss: -0.510 \t D loss: -0.176 \t D(x) = 0.707 \t D(G(z)) = 0.505 \t grad_pen = 0.026 \t t = 21.687 \t\n",
      "[32/10000][768/782] G loss: -0.496 \t D loss: -0.164 \t D(x) = 0.727 \t D(G(z)) = 0.531 \t grad_pen = 0.032 \t t = 21.773 \t\n",
      "[33/10000][0/782] G loss: -0.396 \t D loss: -0.159 \t D(x) = 0.687 \t D(G(z)) = 0.485 \t grad_pen = 0.044 \t t = 1.138 \t\n",
      "[33/10000][256/782] G loss: -0.533 \t D loss: -0.147 \t D(x) = 0.615 \t D(G(z)) = 0.446 \t grad_pen = 0.023 \t t = 21.558 \t\n",
      "[33/10000][512/782] G loss: -0.475 \t D loss: -0.167 \t D(x) = 0.703 \t D(G(z)) = 0.493 \t grad_pen = 0.044 \t t = 21.610 \t\n",
      "[33/10000][768/782] G loss: -0.495 \t D loss: -0.168 \t D(x) = 0.717 \t D(G(z)) = 0.509 \t grad_pen = 0.040 \t t = 21.609 \t\n",
      "[34/10000][0/782] G loss: -0.535 \t D loss: -0.174 \t D(x) = 0.679 \t D(G(z)) = 0.481 \t grad_pen = 0.024 \t t = 1.145 \t\n",
      "[34/10000][256/782] G loss: -0.503 \t D loss: -0.152 \t D(x) = 0.635 \t D(G(z)) = 0.457 \t grad_pen = 0.027 \t t = 21.599 \t\n",
      "[34/10000][512/782] G loss: -0.519 \t D loss: -0.140 \t D(x) = 0.705 \t D(G(z)) = 0.514 \t grad_pen = 0.051 \t t = 21.670 \t\n",
      "[34/10000][768/782] G loss: -0.470 \t D loss: -0.191 \t D(x) = 0.719 \t D(G(z)) = 0.494 \t grad_pen = 0.034 \t t = 21.729 \t\n",
      "[35/10000][0/782] G loss: -0.505 \t D loss: -0.164 \t D(x) = 0.711 \t D(G(z)) = 0.504 \t grad_pen = 0.043 \t t = 1.144 \t\n",
      "[35/10000][256/782] G loss: -0.453 \t D loss: -0.178 \t D(x) = 0.737 \t D(G(z)) = 0.529 \t grad_pen = 0.030 \t t = 21.565 \t\n",
      "[35/10000][512/782] G loss: -0.538 \t D loss: -0.160 \t D(x) = 0.682 \t D(G(z)) = 0.490 \t grad_pen = 0.031 \t t = 21.717 \t\n",
      "[35/10000][768/782] G loss: -0.535 \t D loss: -0.160 \t D(x) = 0.697 \t D(G(z)) = 0.509 \t grad_pen = 0.028 \t t = 21.716 \t\n",
      "[36/10000][0/782] G loss: -0.428 \t D loss: -0.144 \t D(x) = 0.685 \t D(G(z)) = 0.486 \t grad_pen = 0.055 \t t = 1.147 \t\n",
      "[36/10000][256/782] G loss: -0.485 \t D loss: -0.151 \t D(x) = 0.718 \t D(G(z)) = 0.522 \t grad_pen = 0.045 \t t = 21.550 \t\n",
      "[36/10000][512/782] G loss: -0.527 \t D loss: -0.143 \t D(x) = 0.672 \t D(G(z)) = 0.490 \t grad_pen = 0.039 \t t = 21.582 \t\n",
      "[36/10000][768/782] G loss: -0.492 \t D loss: -0.161 \t D(x) = 0.660 \t D(G(z)) = 0.467 \t grad_pen = 0.032 \t t = 21.618 \t\n",
      "[37/10000][0/782] G loss: -0.556 \t D loss: -0.133 \t D(x) = 0.629 \t D(G(z)) = 0.457 \t grad_pen = 0.038 \t t = 1.150 \t\n",
      "[37/10000][256/782] G loss: -0.465 \t D loss: -0.158 \t D(x) = 0.647 \t D(G(z)) = 0.458 \t grad_pen = 0.031 \t t = 21.724 \t\n",
      "[37/10000][512/782] G loss: -0.535 \t D loss: -0.150 \t D(x) = 0.660 \t D(G(z)) = 0.475 \t grad_pen = 0.035 \t t = 21.817 \t\n",
      "[37/10000][768/782] G loss: -0.540 \t D loss: -0.154 \t D(x) = 0.690 \t D(G(z)) = 0.502 \t grad_pen = 0.034 \t t = 21.835 \t\n",
      "[38/10000][0/782] G loss: -0.459 \t D loss: -0.193 \t D(x) = 0.713 \t D(G(z)) = 0.493 \t grad_pen = 0.026 \t t = 1.150 \t\n",
      "[38/10000][256/782] G loss: -0.500 \t D loss: -0.115 \t D(x) = 0.772 \t D(G(z)) = 0.551 \t grad_pen = 0.105 \t t = 21.702 \t\n",
      "[38/10000][512/782] G loss: -0.489 \t D loss: -0.182 \t D(x) = 0.692 \t D(G(z)) = 0.489 \t grad_pen = 0.021 \t t = 21.993 \t\n",
      "[38/10000][768/782] G loss: -0.543 \t D loss: -0.149 \t D(x) = 0.649 \t D(G(z)) = 0.471 \t grad_pen = 0.029 \t t = 21.985 \t\n",
      "[39/10000][0/782] G loss: -0.481 \t D loss: -0.188 \t D(x) = 0.704 \t D(G(z)) = 0.494 \t grad_pen = 0.022 \t t = 1.158 \t\n",
      "[39/10000][256/782] G loss: -0.500 \t D loss: -0.177 \t D(x) = 0.748 \t D(G(z)) = 0.519 \t grad_pen = 0.052 \t t = 21.895 \t\n",
      "[39/10000][512/782] G loss: -0.489 \t D loss: -0.172 \t D(x) = 0.679 \t D(G(z)) = 0.479 \t grad_pen = 0.028 \t t = 21.638 \t\n",
      "[39/10000][768/782] G loss: -0.514 \t D loss: -0.173 \t D(x) = 0.713 \t D(G(z)) = 0.508 \t grad_pen = 0.032 \t t = 21.672 \t\n",
      "[40/10000][0/782] G loss: -0.582 \t D loss: -0.154 \t D(x) = 0.735 \t D(G(z)) = 0.530 \t grad_pen = 0.051 \t t = 1.168 \t\n",
      "[40/10000][256/782] G loss: -0.459 \t D loss: -0.188 \t D(x) = 0.699 \t D(G(z)) = 0.483 \t grad_pen = 0.029 \t t = 21.731 \t\n",
      "[40/10000][512/782] G loss: -0.502 \t D loss: -0.147 \t D(x) = 0.656 \t D(G(z)) = 0.467 \t grad_pen = 0.042 \t t = 21.667 \t\n",
      "[40/10000][768/782] G loss: -0.503 \t D loss: -0.179 \t D(x) = 0.702 \t D(G(z)) = 0.496 \t grad_pen = 0.027 \t t = 21.696 \t\n",
      "[41/10000][0/782] G loss: -0.436 \t D loss: -0.147 \t D(x) = 0.735 \t D(G(z)) = 0.526 \t grad_pen = 0.063 \t t = 1.146 \t\n",
      "[41/10000][256/782] G loss: -0.491 \t D loss: -0.168 \t D(x) = 0.755 \t D(G(z)) = 0.529 \t grad_pen = 0.058 \t t = 21.568 \t\n",
      "[41/10000][512/782] G loss: -0.506 \t D loss: -0.119 \t D(x) = 0.552 \t D(G(z)) = 0.399 \t grad_pen = 0.035 \t t = 21.605 \t\n",
      "[41/10000][768/782] G loss: -0.462 \t D loss: -0.174 \t D(x) = 0.714 \t D(G(z)) = 0.503 \t grad_pen = 0.037 \t t = 21.623 \t\n",
      "[42/10000][0/782] G loss: -0.481 \t D loss: -0.163 \t D(x) = 0.691 \t D(G(z)) = 0.503 \t grad_pen = 0.026 \t t = 1.146 \t\n",
      "[42/10000][256/782] G loss: -0.484 \t D loss: -0.170 \t D(x) = 0.702 \t D(G(z)) = 0.486 \t grad_pen = 0.046 \t t = 21.569 \t\n",
      "[42/10000][512/782] G loss: -0.519 \t D loss: -0.160 \t D(x) = 0.714 \t D(G(z)) = 0.511 \t grad_pen = 0.043 \t t = 21.583 \t\n",
      "[42/10000][768/782] G loss: -0.505 \t D loss: -0.184 \t D(x) = 0.716 \t D(G(z)) = 0.490 \t grad_pen = 0.042 \t t = 21.637 \t\n",
      "[43/10000][0/782] G loss: -0.464 \t D loss: -0.128 \t D(x) = 0.674 \t D(G(z)) = 0.490 \t grad_pen = 0.056 \t t = 1.137 \t\n",
      "[43/10000][256/782] G loss: -0.499 \t D loss: -0.198 \t D(x) = 0.739 \t D(G(z)) = 0.502 \t grad_pen = 0.039 \t t = 21.554 \t\n",
      "[43/10000][512/782] G loss: -0.486 \t D loss: -0.182 \t D(x) = 0.654 \t D(G(z)) = 0.443 \t grad_pen = 0.029 \t t = 21.561 \t\n",
      "[43/10000][768/782] G loss: -0.483 \t D loss: -0.148 \t D(x) = 0.767 \t D(G(z)) = 0.537 \t grad_pen = 0.081 \t t = 21.616 \t\n",
      "[44/10000][0/782] G loss: -0.495 \t D loss: -0.187 \t D(x) = 0.785 \t D(G(z)) = 0.538 \t grad_pen = 0.060 \t t = 1.146 \t\n",
      "[44/10000][256/782] G loss: -0.503 \t D loss: -0.163 \t D(x) = 0.752 \t D(G(z)) = 0.537 \t grad_pen = 0.052 \t t = 21.655 \t\n",
      "[44/10000][512/782] G loss: -0.529 \t D loss: -0.124 \t D(x) = 0.798 \t D(G(z)) = 0.549 \t grad_pen = 0.126 \t t = 21.722 \t\n",
      "[44/10000][768/782] G loss: -0.506 \t D loss: -0.107 \t D(x) = 0.566 \t D(G(z)) = 0.407 \t grad_pen = 0.052 \t t = 21.551 \t\n",
      "[45/10000][0/782] G loss: -0.543 \t D loss: -0.173 \t D(x) = 0.721 \t D(G(z)) = 0.498 \t grad_pen = 0.049 \t t = 1.155 \t\n",
      "[45/10000][256/782] G loss: -0.475 \t D loss: -0.171 \t D(x) = 0.645 \t D(G(z)) = 0.451 \t grad_pen = 0.022 \t t = 21.522 \t\n",
      "[45/10000][512/782] G loss: -0.526 \t D loss: -0.130 \t D(x) = 0.661 \t D(G(z)) = 0.480 \t grad_pen = 0.051 \t t = 21.783 \t\n",
      "[45/10000][768/782] G loss: -0.560 \t D loss: -0.181 \t D(x) = 0.699 \t D(G(z)) = 0.479 \t grad_pen = 0.040 \t t = 21.659 \t\n",
      "[46/10000][0/782] G loss: -0.450 \t D loss: -0.103 \t D(x) = 0.552 \t D(G(z)) = 0.397 \t grad_pen = 0.052 \t t = 1.139 \t\n",
      "[46/10000][256/782] G loss: -0.506 \t D loss: -0.180 \t D(x) = 0.690 \t D(G(z)) = 0.483 \t grad_pen = 0.026 \t t = 21.510 \t\n",
      "[46/10000][512/782] G loss: -0.562 \t D loss: -0.191 \t D(x) = 0.757 \t D(G(z)) = 0.522 \t grad_pen = 0.044 \t t = 21.566 \t\n",
      "[46/10000][768/782] G loss: -0.494 \t D loss: -0.159 \t D(x) = 0.736 \t D(G(z)) = 0.521 \t grad_pen = 0.056 \t t = 21.532 \t\n",
      "[47/10000][0/782] G loss: -0.537 \t D loss: -0.163 \t D(x) = 0.764 \t D(G(z)) = 0.540 \t grad_pen = 0.061 \t t = 1.145 \t\n",
      "[47/10000][256/782] G loss: -0.520 \t D loss: -0.183 \t D(x) = 0.710 \t D(G(z)) = 0.494 \t grad_pen = 0.033 \t t = 21.574 \t\n",
      "[47/10000][512/782] G loss: -0.510 \t D loss: -0.172 \t D(x) = 0.753 \t D(G(z)) = 0.516 \t grad_pen = 0.066 \t t = 21.656 \t\n",
      "[47/10000][768/782] G loss: -0.496 \t D loss: -0.160 \t D(x) = 0.716 \t D(G(z)) = 0.508 \t grad_pen = 0.048 \t t = 21.610 \t\n",
      "[48/10000][0/782] G loss: -0.477 \t D loss: -0.156 \t D(x) = 0.793 \t D(G(z)) = 0.558 \t grad_pen = 0.079 \t t = 1.134 \t\n",
      "[48/10000][256/782] G loss: -0.505 \t D loss: -0.170 \t D(x) = 0.707 \t D(G(z)) = 0.506 \t grad_pen = 0.031 \t t = 21.629 \t\n",
      "[48/10000][512/782] G loss: -0.505 \t D loss: -0.135 \t D(x) = 0.606 \t D(G(z)) = 0.425 \t grad_pen = 0.045 \t t = 21.660 \t\n",
      "[48/10000][768/782] G loss: -0.438 \t D loss: -0.159 \t D(x) = 0.707 \t D(G(z)) = 0.500 \t grad_pen = 0.048 \t t = 21.604 \t\n",
      "[49/10000][0/782] G loss: -0.477 \t D loss: -0.120 \t D(x) = 0.781 \t D(G(z)) = 0.559 \t grad_pen = 0.102 \t t = 1.142 \t\n",
      "[49/10000][256/782] G loss: -0.510 \t D loss: -0.194 \t D(x) = 0.786 \t D(G(z)) = 0.547 \t grad_pen = 0.045 \t t = 21.690 \t\n",
      "[49/10000][512/782] G loss: -0.553 \t D loss: -0.149 \t D(x) = 0.730 \t D(G(z)) = 0.519 \t grad_pen = 0.062 \t t = 21.608 \t\n",
      "[49/10000][768/782] G loss: -0.508 \t D loss: -0.146 \t D(x) = 0.686 \t D(G(z)) = 0.500 \t grad_pen = 0.040 \t t = 21.674 \t\n",
      "[50/10000][0/782] G loss: -0.440 \t D loss: -0.177 \t D(x) = 0.641 \t D(G(z)) = 0.440 \t grad_pen = 0.023 \t t = 1.135 \t\n",
      "[50/10000][256/782] G loss: -0.495 \t D loss: -0.165 \t D(x) = 0.706 \t D(G(z)) = 0.498 \t grad_pen = 0.043 \t t = 21.682 \t\n",
      "[50/10000][512/782] G loss: -0.451 \t D loss: -0.197 \t D(x) = 0.705 \t D(G(z)) = 0.469 \t grad_pen = 0.039 \t t = 21.718 \t\n",
      "[50/10000][768/782] G loss: -0.496 \t D loss: -0.174 \t D(x) = 0.674 \t D(G(z)) = 0.473 \t grad_pen = 0.027 \t t = 21.652 \t\n",
      "[51/10000][0/782] G loss: -0.533 \t D loss: -0.165 \t D(x) = 0.689 \t D(G(z)) = 0.498 \t grad_pen = 0.026 \t t = 1.136 \t\n",
      "[51/10000][256/782] G loss: -0.534 \t D loss: -0.172 \t D(x) = 0.709 \t D(G(z)) = 0.492 \t grad_pen = 0.045 \t t = 21.754 \t\n",
      "[51/10000][512/782] G loss: -0.483 \t D loss: -0.146 \t D(x) = 0.788 \t D(G(z)) = 0.544 \t grad_pen = 0.098 \t t = 21.823 \t\n",
      "[51/10000][768/782] G loss: -0.548 \t D loss: -0.156 \t D(x) = 0.701 \t D(G(z)) = 0.485 \t grad_pen = 0.060 \t t = 21.748 \t\n",
      "[52/10000][0/782] G loss: -0.471 \t D loss: -0.126 \t D(x) = 0.736 \t D(G(z)) = 0.540 \t grad_pen = 0.070 \t t = 1.155 \t\n",
      "[52/10000][256/782] G loss: -0.495 \t D loss: -0.124 \t D(x) = 0.751 \t D(G(z)) = 0.517 \t grad_pen = 0.110 \t t = 21.852 \t\n",
      "[52/10000][512/782] G loss: -0.510 \t D loss: -0.108 \t D(x) = 0.769 \t D(G(z)) = 0.538 \t grad_pen = 0.123 \t t = 21.817 \t\n",
      "[52/10000][768/782] G loss: -0.463 \t D loss: -0.181 \t D(x) = 0.706 \t D(G(z)) = 0.477 \t grad_pen = 0.048 \t t = 21.686 \t\n",
      "[53/10000][0/782] G loss: -0.594 \t D loss: -0.215 \t D(x) = 0.742 \t D(G(z)) = 0.490 \t grad_pen = 0.037 \t t = 1.152 \t\n",
      "[53/10000][256/782] G loss: -0.489 \t D loss: -0.150 \t D(x) = 0.644 \t D(G(z)) = 0.468 \t grad_pen = 0.026 \t t = 21.808 \t\n",
      "[53/10000][512/782] G loss: -0.447 \t D loss: -0.189 \t D(x) = 0.711 \t D(G(z)) = 0.502 \t grad_pen = 0.020 \t t = 21.621 \t\n",
      "[53/10000][768/782] G loss: -0.462 \t D loss: -0.147 \t D(x) = 0.753 \t D(G(z)) = 0.535 \t grad_pen = 0.071 \t t = 21.800 \t\n",
      "[54/10000][0/782] G loss: -0.447 \t D loss: -0.139 \t D(x) = 0.668 \t D(G(z)) = 0.482 \t grad_pen = 0.047 \t t = 1.156 \t\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Highly adapted from: https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n",
    "\"\"\"\n",
    "\n",
    "g_iters = 1 # 5\n",
    "d_iters = 2 # 1, discriminator is called critic in WGAN paper\n",
    "\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = one * -1\n",
    "\n",
    "iters = 0\n",
    "t1 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        real = data.to(device)\n",
    "        b_size = real.size(0)\n",
    "        \n",
    "        \"\"\"\n",
    "        Train G\n",
    "        \"\"\"\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        for _ in range(g_iters):\n",
    "            netG.zero_grad()\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            noise.requires_grad_(True)\n",
    "            fake = netG(noise)\n",
    "\n",
    "            g_cost = netD(fake).mean()\n",
    "            g_cost.backward(mone)\n",
    "            g_cost = -g_cost\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        \"\"\"\n",
    "        Train D\n",
    "        \"\"\"\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "        for _ in range(d_iters):\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # generate fake data\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                noisev = noise # Freeze G, training D\n",
    "\n",
    "            fake = netG(noisev).detach()\n",
    "\n",
    "            # train with real data\n",
    "            d_real = netD(real).mean()\n",
    "\n",
    "            # train with fake data\n",
    "            d_fake = netD(fake).mean()\n",
    "\n",
    "            # train with interpolates data\n",
    "            gradient_penalty = calc_gradient_penalty(netD, real, fake, b_size)\n",
    "\n",
    "             # final disc cost\n",
    "            d_cost = d_fake - d_real + gradient_penalty\n",
    "            d_cost.backward()\n",
    "            w_dist = d_fake  - d_real # wasserstein distance\n",
    "            optimizerD.step()\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        weights_saved = False\n",
    "        if (iters % 100 == 0): # save weights every % .... iters\n",
    "            #print('weights saved')\n",
    "            if ngpu > 1:\n",
    "                torch.save(netG.module.state_dict(), 'netG_state_dict2')\n",
    "                torch.save(netD.module.state_dict(), 'netD_state_dict2')\n",
    "            else:\n",
    "                torch.save(netG.state_dict(), 'netG_state_dict2')\n",
    "                torch.save(netD.state_dict(), 'netD_state_dict2')\n",
    "            \n",
    "        \n",
    "        if i % (256) == 0:\n",
    "            t2 = time.time()\n",
    "            print('[%d/%d][%d/%d] G loss: %.3f \\t D loss: %.3f \\t D(x) = %.3f \\t D(G(z)) = %.3f \\t grad_pen = %.3f \\t t = %.3f \\t'% \n",
    "                      (epoch, num_epochs, i, len(dataloader), g_cost, d_cost, d_real, d_fake, gradient_penalty, (t2-t1)))\n",
    "            t1 = time.time()\n",
    "                \n",
    "        iters += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
