{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f04fc0b2890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import time as t\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import time as time\n",
    "\n",
    "from torch import autograd\n",
    "\n",
    "import model\n",
    "from keijzer_exogan import *\n",
    "\n",
    "# initialize random seeds\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Local variables\n",
    "\"\"\"\n",
    "workers = 0 # Number of workers for dataloader, 0 when to_vram is enabled\n",
    "batch_size = 64 # 2**11\n",
    "image_size = 32\n",
    "nz = 100 # size of latent vector\n",
    "num_epochs = 10*10**3\n",
    "torch.backends.cudnn.benchmark=True # Uses udnn auto-tuner to find the best algorithm to use for your hardware, speeds up training by almost 50%\n",
    "lr = 1e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9\n",
    "\n",
    "lambda_ = 10\n",
    "\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "selected_gpus = [2,3] # Number of GPUs available. Use 0 for CPU mode.\n",
    "\n",
    "path = '/datb/16011015/ExoGAN_data/selection//' #notice how you dont put the last folder in here...\n",
    "images = np.load(path+'first_chunks_25_percent_images.npy')\n",
    "\n",
    "swap_labels_randomly = False\n",
    "\n",
    "train_d_g_conditional = False # switch between training D and G based on set threshold\n",
    "d_g_conditional_threshold = 0.55 # D_G_z1 < threshold, train G\n",
    "\n",
    "train_d_g_conditional_per_epoch = False\n",
    "\n",
    "train_d_g_conditional_per_n_iters = False\n",
    "train_d_g_n_iters = 2 # When 2, train D 2 times before training G 1 time\n",
    "\n",
    "use_saved_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  64\n",
      "Number of GPUs used:  2\n",
      "Number of images:  50000\n",
      "Succesfully loaded saved weights.\n"
     ]
    }
   ],
   "source": [
    "print('Batch size: ', batch_size)\n",
    "ngpu = len(selected_gpus)\n",
    "print('Number of GPUs used: ', ngpu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load data and prepare DataLoader\n",
    "\"\"\"\n",
    "shuffle = True\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(images) # shuffles the images\n",
    "\n",
    "images = images[:int(len(images)*0.1)] # use only first ... percent of the data (0.05)\n",
    "print('Number of images: ', len(images))\n",
    "\n",
    "dataset = numpy_dataset(data=images, to_vram=True) # to_vram pins it to all GPU's\n",
    "#dataset = numpy_dataset(data=images, to_vram=True, transform=transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])) # to_vram pins it to all GPU's\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load and setup models\n",
    "\"\"\"\n",
    "# Initialize cuda\n",
    "device = torch.device(\"cuda:\"+str(selected_gpus[0]) if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Load models\n",
    "netG = model.Generator(ngpu).to(device)\n",
    "netD = model.Discriminator(ngpu).to(device)\n",
    "\n",
    "# Apply weights\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netG.apply(weights_init) # It's not clean/efficient to load these ones first, but it works.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "if use_saved_weights:\n",
    "    try:\n",
    "        # Load saved weights\n",
    "        netG.load_state_dict(torch.load('netG_state_dict2', map_location=device)) #net.module..load_... for parallel model , net.load_... for single gpu model\n",
    "        netD.load_state_dict(torch.load('netD_state_dict2', map_location=device))\n",
    "        print('Succesfully loaded saved weights.')\n",
    "    except:\n",
    "        print('Could not load saved weights, using new ones.')\n",
    "        pass\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, device_ids=selected_gpus, output_device=device)\n",
    "    netD = nn.DataParallel(netD, device_ids=selected_gpus, output_device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define input training stuff (fancy this up)\n",
    "\"\"\"\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2)) # should be sgd\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "switch = True # condition switch, to switch between D and G per epoch\n",
    "previous_switch = 0\n",
    "\n",
    "train_D = True\n",
    "train_G = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data, b_size):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n",
    "    \"\"\"\n",
    "    alpha = torch.rand(b_size, 1)\n",
    "    alpha = alpha.expand(b_size, int(real_data.nelement()/b_size)).contiguous()\n",
    "    alpha = alpha.view(b_size, 1, image_size, image_size)\n",
    "    alpha = alpha.to(device)\n",
    "    \n",
    "    fake_data = fake_data.view(b_size, 1, image_size, image_size)\n",
    "    interpolates = alpha * real_data.detach() + ((1 - alpha) * fake_data.detach())\n",
    "\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates.requires_grad_(True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)                              \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switched to WGAN-GP (https://arxiv.org/pdf/1704.00028) instead of DCGAN for training stability reasons.  \n",
    "Wheras DCGAN collapsed (D(x) staying 0.8-0.9 and D(G(z)) becoming 0.5000) to output black images, WGAN-GP keeps training well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000][0/782] G loss: -0.524 \t D loss: -0.234 \t D(x) = 0.764 \t D(G(z)) = 0.501 \t grad_pen = 0.029 \t t = 2.599 \t\n",
      "[0/10000][256/782] G loss: -0.496 \t D loss: -0.211 \t D(x) = 0.747 \t D(G(z)) = 0.500 \t grad_pen = 0.035 \t t = 33.187 \t\n",
      "[0/10000][512/782] G loss: -0.525 \t D loss: -0.225 \t D(x) = 0.774 \t D(G(z)) = 0.528 \t grad_pen = 0.021 \t t = 33.256 \t\n",
      "[0/10000][768/782] G loss: -0.507 \t D loss: -0.203 \t D(x) = 0.741 \t D(G(z)) = 0.510 \t grad_pen = 0.028 \t t = 33.147 \t\n",
      "[1/10000][0/782] G loss: -0.496 \t D loss: -0.250 \t D(x) = 0.758 \t D(G(z)) = 0.485 \t grad_pen = 0.023 \t t = 1.881 \t\n",
      "[1/10000][256/782] G loss: -0.518 \t D loss: -0.225 \t D(x) = 0.777 \t D(G(z)) = 0.522 \t grad_pen = 0.029 \t t = 33.197 \t\n",
      "[1/10000][512/782] G loss: -0.539 \t D loss: -0.212 \t D(x) = 0.787 \t D(G(z)) = 0.528 \t grad_pen = 0.047 \t t = 33.113 \t\n",
      "[1/10000][768/782] G loss: -0.523 \t D loss: -0.197 \t D(x) = 0.761 \t D(G(z)) = 0.521 \t grad_pen = 0.043 \t t = 33.201 \t\n",
      "[2/10000][0/782] G loss: -0.530 \t D loss: -0.205 \t D(x) = 0.766 \t D(G(z)) = 0.531 \t grad_pen = 0.031 \t t = 1.786 \t\n",
      "[2/10000][256/782] G loss: -0.502 \t D loss: -0.243 \t D(x) = 0.768 \t D(G(z)) = 0.508 \t grad_pen = 0.018 \t t = 33.191 \t\n",
      "[2/10000][512/782] G loss: -0.501 \t D loss: -0.202 \t D(x) = 0.744 \t D(G(z)) = 0.500 \t grad_pen = 0.042 \t t = 33.273 \t\n",
      "[2/10000][768/782] G loss: -0.495 \t D loss: -0.220 \t D(x) = 0.747 \t D(G(z)) = 0.499 \t grad_pen = 0.029 \t t = 33.183 \t\n",
      "[3/10000][0/782] G loss: -0.537 \t D loss: -0.169 \t D(x) = 0.785 \t D(G(z)) = 0.541 \t grad_pen = 0.074 \t t = 1.793 \t\n",
      "[3/10000][256/782] G loss: -0.516 \t D loss: -0.197 \t D(x) = 0.743 \t D(G(z)) = 0.518 \t grad_pen = 0.029 \t t = 33.162 \t\n",
      "[3/10000][512/782] G loss: -0.526 \t D loss: -0.232 \t D(x) = 0.789 \t D(G(z)) = 0.531 \t grad_pen = 0.026 \t t = 33.146 \t\n",
      "[3/10000][768/782] G loss: -0.524 \t D loss: -0.228 \t D(x) = 0.770 \t D(G(z)) = 0.516 \t grad_pen = 0.026 \t t = 33.188 \t\n",
      "[4/10000][0/782] G loss: -0.521 \t D loss: -0.236 \t D(x) = 0.768 \t D(G(z)) = 0.505 \t grad_pen = 0.026 \t t = 1.793 \t\n",
      "[4/10000][256/782] G loss: -0.503 \t D loss: -0.222 \t D(x) = 0.767 \t D(G(z)) = 0.506 \t grad_pen = 0.039 \t t = 33.273 \t\n",
      "[4/10000][512/782] G loss: -0.512 \t D loss: -0.210 \t D(x) = 0.761 \t D(G(z)) = 0.529 \t grad_pen = 0.022 \t t = 33.219 \t\n",
      "[4/10000][768/782] G loss: -0.543 \t D loss: -0.218 \t D(x) = 0.807 \t D(G(z)) = 0.555 \t grad_pen = 0.033 \t t = 33.227 \t\n",
      "[5/10000][0/782] G loss: -0.507 \t D loss: -0.205 \t D(x) = 0.746 \t D(G(z)) = 0.507 \t grad_pen = 0.034 \t t = 1.824 \t\n",
      "[5/10000][256/782] G loss: -0.498 \t D loss: -0.186 \t D(x) = 0.715 \t D(G(z)) = 0.502 \t grad_pen = 0.027 \t t = 33.190 \t\n",
      "[5/10000][512/782] G loss: -0.525 \t D loss: -0.218 \t D(x) = 0.781 \t D(G(z)) = 0.524 \t grad_pen = 0.039 \t t = 33.241 \t\n",
      "[5/10000][768/782] G loss: -0.504 \t D loss: -0.239 \t D(x) = 0.786 \t D(G(z)) = 0.515 \t grad_pen = 0.033 \t t = 33.162 \t\n",
      "[6/10000][0/782] G loss: -0.552 \t D loss: -0.200 \t D(x) = 0.796 \t D(G(z)) = 0.555 \t grad_pen = 0.041 \t t = 1.784 \t\n",
      "[6/10000][256/782] G loss: -0.508 \t D loss: -0.224 \t D(x) = 0.743 \t D(G(z)) = 0.492 \t grad_pen = 0.027 \t t = 33.174 \t\n",
      "[6/10000][512/782] G loss: -0.526 \t D loss: -0.202 \t D(x) = 0.743 \t D(G(z)) = 0.520 \t grad_pen = 0.020 \t t = 33.126 \t\n",
      "[6/10000][768/782] G loss: -0.495 \t D loss: -0.215 \t D(x) = 0.754 \t D(G(z)) = 0.515 \t grad_pen = 0.023 \t t = 33.248 \t\n",
      "[7/10000][0/782] G loss: -0.501 \t D loss: -0.210 \t D(x) = 0.746 \t D(G(z)) = 0.496 \t grad_pen = 0.040 \t t = 1.788 \t\n",
      "[7/10000][256/782] G loss: -0.529 \t D loss: -0.191 \t D(x) = 0.765 \t D(G(z)) = 0.528 \t grad_pen = 0.047 \t t = 33.218 \t\n",
      "[7/10000][512/782] G loss: -0.501 \t D loss: -0.212 \t D(x) = 0.760 \t D(G(z)) = 0.505 \t grad_pen = 0.043 \t t = 33.271 \t\n",
      "[7/10000][768/782] G loss: -0.507 \t D loss: -0.192 \t D(x) = 0.746 \t D(G(z)) = 0.519 \t grad_pen = 0.035 \t t = 33.205 \t\n",
      "[8/10000][0/782] G loss: -0.506 \t D loss: -0.196 \t D(x) = 0.746 \t D(G(z)) = 0.507 \t grad_pen = 0.043 \t t = 1.795 \t\n",
      "[8/10000][256/782] G loss: -0.535 \t D loss: -0.210 \t D(x) = 0.755 \t D(G(z)) = 0.522 \t grad_pen = 0.022 \t t = 33.172 \t\n",
      "[8/10000][512/782] G loss: -0.510 \t D loss: -0.216 \t D(x) = 0.750 \t D(G(z)) = 0.509 \t grad_pen = 0.026 \t t = 33.145 \t\n",
      "[8/10000][768/782] G loss: -0.522 \t D loss: -0.216 \t D(x) = 0.763 \t D(G(z)) = 0.523 \t grad_pen = 0.023 \t t = 33.230 \t\n",
      "[9/10000][0/782] G loss: -0.489 \t D loss: -0.225 \t D(x) = 0.730 \t D(G(z)) = 0.482 \t grad_pen = 0.023 \t t = 1.788 \t\n",
      "[9/10000][256/782] G loss: -0.485 \t D loss: -0.201 \t D(x) = 0.704 \t D(G(z)) = 0.483 \t grad_pen = 0.020 \t t = 33.177 \t\n",
      "[9/10000][512/782] G loss: -0.552 \t D loss: -0.227 \t D(x) = 0.805 \t D(G(z)) = 0.551 \t grad_pen = 0.027 \t t = 33.262 \t\n",
      "[9/10000][768/782] G loss: -0.535 \t D loss: -0.200 \t D(x) = 0.779 \t D(G(z)) = 0.547 \t grad_pen = 0.032 \t t = 33.191 \t\n",
      "[10/10000][0/782] G loss: -0.530 \t D loss: -0.194 \t D(x) = 0.761 \t D(G(z)) = 0.535 \t grad_pen = 0.032 \t t = 1.790 \t\n",
      "[10/10000][256/782] G loss: -0.496 \t D loss: -0.191 \t D(x) = 0.715 \t D(G(z)) = 0.505 \t grad_pen = 0.019 \t t = 33.197 \t\n",
      "[10/10000][512/782] G loss: -0.520 \t D loss: -0.221 \t D(x) = 0.782 \t D(G(z)) = 0.517 \t grad_pen = 0.044 \t t = 33.207 \t\n",
      "[10/10000][768/782] G loss: -0.516 \t D loss: -0.246 \t D(x) = 0.811 \t D(G(z)) = 0.513 \t grad_pen = 0.052 \t t = 33.238 \t\n",
      "[11/10000][0/782] G loss: -0.522 \t D loss: -0.220 \t D(x) = 0.787 \t D(G(z)) = 0.513 \t grad_pen = 0.054 \t t = 1.796 \t\n",
      "[11/10000][256/782] G loss: -0.530 \t D loss: -0.211 \t D(x) = 0.777 \t D(G(z)) = 0.525 \t grad_pen = 0.042 \t t = 33.173 \t\n",
      "[11/10000][512/782] G loss: -0.512 \t D loss: -0.176 \t D(x) = 0.745 \t D(G(z)) = 0.528 \t grad_pen = 0.041 \t t = 33.139 \t\n",
      "[11/10000][768/782] G loss: -0.506 \t D loss: -0.215 \t D(x) = 0.754 \t D(G(z)) = 0.510 \t grad_pen = 0.030 \t t = 33.234 \t\n",
      "[12/10000][0/782] G loss: -0.515 \t D loss: -0.221 \t D(x) = 0.755 \t D(G(z)) = 0.515 \t grad_pen = 0.020 \t t = 1.789 \t\n",
      "[12/10000][256/782] G loss: -0.533 \t D loss: -0.221 \t D(x) = 0.769 \t D(G(z)) = 0.517 \t grad_pen = 0.031 \t t = 33.114 \t\n",
      "[12/10000][512/782] G loss: -0.502 \t D loss: -0.219 \t D(x) = 0.752 \t D(G(z)) = 0.500 \t grad_pen = 0.033 \t t = 33.214 \t\n",
      "[12/10000][768/782] G loss: -0.524 \t D loss: -0.234 \t D(x) = 0.772 \t D(G(z)) = 0.518 \t grad_pen = 0.019 \t t = 33.120 \t\n",
      "[13/10000][0/782] G loss: -0.498 \t D loss: -0.189 \t D(x) = 0.716 \t D(G(z)) = 0.497 \t grad_pen = 0.029 \t t = 1.787 \t\n",
      "[13/10000][256/782] G loss: -0.518 \t D loss: -0.215 \t D(x) = 0.786 \t D(G(z)) = 0.534 \t grad_pen = 0.036 \t t = 33.200 \t\n",
      "[13/10000][512/782] G loss: -0.516 \t D loss: -0.224 \t D(x) = 0.772 \t D(G(z)) = 0.519 \t grad_pen = 0.029 \t t = 33.101 \t\n",
      "[13/10000][768/782] G loss: -0.507 \t D loss: -0.213 \t D(x) = 0.763 \t D(G(z)) = 0.515 \t grad_pen = 0.036 \t t = 33.164 \t\n",
      "[14/10000][0/782] G loss: -0.485 \t D loss: -0.238 \t D(x) = 0.746 \t D(G(z)) = 0.486 \t grad_pen = 0.022 \t t = 1.789 \t\n",
      "[14/10000][256/782] G loss: -0.513 \t D loss: -0.200 \t D(x) = 0.755 \t D(G(z)) = 0.523 \t grad_pen = 0.033 \t t = 33.231 \t\n",
      "[14/10000][512/782] G loss: -0.515 \t D loss: -0.220 \t D(x) = 0.754 \t D(G(z)) = 0.503 \t grad_pen = 0.031 \t t = 33.237 \t\n",
      "[14/10000][768/782] G loss: -0.512 \t D loss: -0.239 \t D(x) = 0.775 \t D(G(z)) = 0.507 \t grad_pen = 0.029 \t t = 33.157 \t\n",
      "[15/10000][0/782] G loss: -0.541 \t D loss: -0.220 \t D(x) = 0.828 \t D(G(z)) = 0.550 \t grad_pen = 0.058 \t t = 1.804 \t\n",
      "[15/10000][256/782] G loss: -0.529 \t D loss: -0.234 \t D(x) = 0.797 \t D(G(z)) = 0.524 \t grad_pen = 0.038 \t t = 33.222 \t\n",
      "[15/10000][512/782] G loss: -0.523 \t D loss: -0.224 \t D(x) = 0.759 \t D(G(z)) = 0.507 \t grad_pen = 0.027 \t t = 33.173 \t\n",
      "[15/10000][768/782] G loss: -0.512 \t D loss: -0.236 \t D(x) = 0.788 \t D(G(z)) = 0.519 \t grad_pen = 0.033 \t t = 33.230 \t\n",
      "[16/10000][0/782] G loss: -0.542 \t D loss: -0.209 \t D(x) = 0.792 \t D(G(z)) = 0.547 \t grad_pen = 0.036 \t t = 1.791 \t\n",
      "[16/10000][256/782] G loss: -0.492 \t D loss: -0.211 \t D(x) = 0.710 \t D(G(z)) = 0.482 \t grad_pen = 0.017 \t t = 33.143 \t\n",
      "[16/10000][512/782] G loss: -0.471 \t D loss: -0.176 \t D(x) = 0.682 \t D(G(z)) = 0.476 \t grad_pen = 0.030 \t t = 33.165 \t\n",
      "[16/10000][768/782] G loss: -0.521 \t D loss: -0.220 \t D(x) = 0.773 \t D(G(z)) = 0.521 \t grad_pen = 0.032 \t t = 33.161 \t\n",
      "[17/10000][0/782] G loss: -0.507 \t D loss: -0.227 \t D(x) = 0.776 \t D(G(z)) = 0.505 \t grad_pen = 0.043 \t t = 1.790 \t\n",
      "[17/10000][256/782] G loss: -0.511 \t D loss: -0.228 \t D(x) = 0.772 \t D(G(z)) = 0.500 \t grad_pen = 0.044 \t t = 33.123 \t\n",
      "[17/10000][512/782] G loss: -0.516 \t D loss: -0.225 \t D(x) = 0.755 \t D(G(z)) = 0.512 \t grad_pen = 0.018 \t t = 33.164 \t\n",
      "[17/10000][768/782] G loss: -0.555 \t D loss: -0.185 \t D(x) = 0.813 \t D(G(z)) = 0.551 \t grad_pen = 0.077 \t t = 33.150 \t\n",
      "[18/10000][0/782] G loss: -0.476 \t D loss: -0.185 \t D(x) = 0.701 \t D(G(z)) = 0.490 \t grad_pen = 0.026 \t t = 1.790 \t\n",
      "[18/10000][256/782] G loss: -0.551 \t D loss: -0.179 \t D(x) = 0.797 \t D(G(z)) = 0.553 \t grad_pen = 0.065 \t t = 33.217 \t\n",
      "[18/10000][512/782] G loss: -0.511 \t D loss: -0.232 \t D(x) = 0.784 \t D(G(z)) = 0.527 \t grad_pen = 0.025 \t t = 33.132 \t\n",
      "[18/10000][768/782] G loss: -0.554 \t D loss: -0.216 \t D(x) = 0.800 \t D(G(z)) = 0.547 \t grad_pen = 0.037 \t t = 33.192 \t\n",
      "[19/10000][0/782] G loss: -0.488 \t D loss: -0.212 \t D(x) = 0.716 \t D(G(z)) = 0.487 \t grad_pen = 0.016 \t t = 1.787 \t\n",
      "[19/10000][256/782] G loss: -0.548 \t D loss: -0.215 \t D(x) = 0.798 \t D(G(z)) = 0.544 \t grad_pen = 0.039 \t t = 33.155 \t\n",
      "[19/10000][512/782] G loss: -0.529 \t D loss: -0.215 \t D(x) = 0.774 \t D(G(z)) = 0.524 \t grad_pen = 0.035 \t t = 33.231 \t\n",
      "[19/10000][768/782] G loss: -0.536 \t D loss: -0.218 \t D(x) = 0.792 \t D(G(z)) = 0.531 \t grad_pen = 0.043 \t t = 33.127 \t\n",
      "[20/10000][0/782] G loss: -0.526 \t D loss: -0.211 \t D(x) = 0.753 \t D(G(z)) = 0.512 \t grad_pen = 0.030 \t t = 1.790 \t\n",
      "[20/10000][256/782] G loss: -0.515 \t D loss: -0.210 \t D(x) = 0.755 \t D(G(z)) = 0.525 \t grad_pen = 0.021 \t t = 33.262 \t\n",
      "[20/10000][512/782] G loss: -0.512 \t D loss: -0.214 \t D(x) = 0.767 \t D(G(z)) = 0.525 \t grad_pen = 0.027 \t t = 33.170 \t\n",
      "[20/10000][768/782] G loss: -0.509 \t D loss: -0.218 \t D(x) = 0.765 \t D(G(z)) = 0.508 \t grad_pen = 0.039 \t t = 33.237 \t\n",
      "[21/10000][0/782] G loss: -0.522 \t D loss: -0.210 \t D(x) = 0.757 \t D(G(z)) = 0.524 \t grad_pen = 0.023 \t t = 1.786 \t\n",
      "[21/10000][256/782] G loss: -0.508 \t D loss: -0.199 \t D(x) = 0.758 \t D(G(z)) = 0.517 \t grad_pen = 0.042 \t t = 33.113 \t\n",
      "[21/10000][512/782] G loss: -0.536 \t D loss: -0.226 \t D(x) = 0.786 \t D(G(z)) = 0.542 \t grad_pen = 0.017 \t t = 33.173 \t\n",
      "[21/10000][768/782] G loss: -0.496 \t D loss: -0.213 \t D(x) = 0.746 \t D(G(z)) = 0.500 \t grad_pen = 0.033 \t t = 33.151 \t\n",
      "[22/10000][0/782] G loss: -0.507 \t D loss: -0.205 \t D(x) = 0.755 \t D(G(z)) = 0.514 \t grad_pen = 0.036 \t t = 1.795 \t\n",
      "[22/10000][256/782] G loss: -0.500 \t D loss: -0.207 \t D(x) = 0.765 \t D(G(z)) = 0.524 \t grad_pen = 0.033 \t t = 33.186 \t\n",
      "[22/10000][512/782] G loss: -0.503 \t D loss: -0.163 \t D(x) = 0.721 \t D(G(z)) = 0.515 \t grad_pen = 0.042 \t t = 33.177 \t\n",
      "[22/10000][768/782] G loss: -0.531 \t D loss: -0.225 \t D(x) = 0.788 \t D(G(z)) = 0.523 \t grad_pen = 0.039 \t t = 33.141 \t\n",
      "[23/10000][0/782] G loss: -0.490 \t D loss: -0.197 \t D(x) = 0.710 \t D(G(z)) = 0.467 \t grad_pen = 0.045 \t t = 1.787 \t\n",
      "[23/10000][256/782] G loss: -0.523 \t D loss: -0.223 \t D(x) = 0.769 \t D(G(z)) = 0.523 \t grad_pen = 0.022 \t t = 33.160 \t\n",
      "[23/10000][512/782] G loss: -0.533 \t D loss: -0.204 \t D(x) = 0.792 \t D(G(z)) = 0.541 \t grad_pen = 0.048 \t t = 33.109 \t\n",
      "[23/10000][768/782] G loss: -0.529 \t D loss: -0.218 \t D(x) = 0.778 \t D(G(z)) = 0.526 \t grad_pen = 0.035 \t t = 33.199 \t\n",
      "[24/10000][0/782] G loss: -0.541 \t D loss: -0.219 \t D(x) = 0.797 \t D(G(z)) = 0.539 \t grad_pen = 0.040 \t t = 1.789 \t\n",
      "[24/10000][256/782] G loss: -0.490 \t D loss: -0.222 \t D(x) = 0.742 \t D(G(z)) = 0.495 \t grad_pen = 0.025 \t t = 33.177 \t\n",
      "[24/10000][512/782] G loss: -0.505 \t D loss: -0.231 \t D(x) = 0.760 \t D(G(z)) = 0.502 \t grad_pen = 0.027 \t t = 33.263 \t\n",
      "[24/10000][768/782] G loss: -0.507 \t D loss: -0.204 \t D(x) = 0.757 \t D(G(z)) = 0.523 \t grad_pen = 0.031 \t t = 33.198 \t\n",
      "[25/10000][0/782] G loss: -0.500 \t D loss: -0.177 \t D(x) = 0.692 \t D(G(z)) = 0.479 \t grad_pen = 0.036 \t t = 1.795 \t\n",
      "[25/10000][256/782] G loss: -0.530 \t D loss: -0.256 \t D(x) = 0.799 \t D(G(z)) = 0.522 \t grad_pen = 0.021 \t t = 33.231 \t\n",
      "[25/10000][512/782] G loss: -0.506 \t D loss: -0.191 \t D(x) = 0.749 \t D(G(z)) = 0.522 \t grad_pen = 0.036 \t t = 33.217 \t\n",
      "[25/10000][768/782] G loss: -0.531 \t D loss: -0.203 \t D(x) = 0.760 \t D(G(z)) = 0.533 \t grad_pen = 0.024 \t t = 33.276 \t\n",
      "[26/10000][0/782] G loss: -0.563 \t D loss: -0.188 \t D(x) = 0.814 \t D(G(z)) = 0.557 \t grad_pen = 0.070 \t t = 1.794 \t\n",
      "[26/10000][256/782] G loss: -0.482 \t D loss: -0.185 \t D(x) = 0.686 \t D(G(z)) = 0.475 \t grad_pen = 0.026 \t t = 33.658 \t\n",
      "[26/10000][512/782] G loss: -0.502 \t D loss: -0.218 \t D(x) = 0.749 \t D(G(z)) = 0.497 \t grad_pen = 0.034 \t t = 33.233 \t\n",
      "[26/10000][768/782] G loss: -0.509 \t D loss: -0.197 \t D(x) = 0.725 \t D(G(z)) = 0.500 \t grad_pen = 0.028 \t t = 33.119 \t\n",
      "[27/10000][0/782] G loss: -0.475 \t D loss: -0.190 \t D(x) = 0.723 \t D(G(z)) = 0.495 \t grad_pen = 0.038 \t t = 1.794 \t\n",
      "[27/10000][256/782] G loss: -0.525 \t D loss: -0.224 \t D(x) = 0.772 \t D(G(z)) = 0.514 \t grad_pen = 0.035 \t t = 33.183 \t\n",
      "[27/10000][512/782] G loss: -0.509 \t D loss: -0.194 \t D(x) = 0.749 \t D(G(z)) = 0.504 \t grad_pen = 0.051 \t t = 33.089 \t\n",
      "[27/10000][768/782] G loss: -0.527 \t D loss: -0.201 \t D(x) = 0.760 \t D(G(z)) = 0.518 \t grad_pen = 0.041 \t t = 33.189 \t\n",
      "[28/10000][0/782] G loss: -0.495 \t D loss: -0.207 \t D(x) = 0.712 \t D(G(z)) = 0.494 \t grad_pen = 0.011 \t t = 1.804 \t\n",
      "[28/10000][256/782] G loss: -0.528 \t D loss: -0.195 \t D(x) = 0.779 \t D(G(z)) = 0.546 \t grad_pen = 0.038 \t t = 33.185 \t\n",
      "[28/10000][512/782] G loss: -0.525 \t D loss: -0.190 \t D(x) = 0.769 \t D(G(z)) = 0.520 \t grad_pen = 0.059 \t t = 33.139 \t\n",
      "[28/10000][768/782] G loss: -0.520 \t D loss: -0.171 \t D(x) = 0.733 \t D(G(z)) = 0.517 \t grad_pen = 0.045 \t t = 33.104 \t\n",
      "[29/10000][0/782] G loss: -0.515 \t D loss: -0.188 \t D(x) = 0.762 \t D(G(z)) = 0.520 \t grad_pen = 0.054 \t t = 1.786 \t\n",
      "[29/10000][256/782] G loss: -0.510 \t D loss: -0.174 \t D(x) = 0.742 \t D(G(z)) = 0.509 \t grad_pen = 0.059 \t t = 33.157 \t\n",
      "[29/10000][512/782] G loss: -0.500 \t D loss: -0.185 \t D(x) = 0.738 \t D(G(z)) = 0.513 \t grad_pen = 0.041 \t t = 33.172 \t\n",
      "[29/10000][768/782] G loss: -0.534 \t D loss: -0.193 \t D(x) = 0.784 \t D(G(z)) = 0.537 \t grad_pen = 0.054 \t t = 33.192 \t\n",
      "[30/10000][0/782] G loss: -0.516 \t D loss: -0.218 \t D(x) = 0.769 \t D(G(z)) = 0.516 \t grad_pen = 0.035 \t t = 1.787 \t\n",
      "[30/10000][256/782] G loss: -0.485 \t D loss: -0.198 \t D(x) = 0.703 \t D(G(z)) = 0.485 \t grad_pen = 0.020 \t t = 33.186 \t\n",
      "[30/10000][512/782] G loss: -0.513 \t D loss: -0.242 \t D(x) = 0.773 \t D(G(z)) = 0.497 \t grad_pen = 0.035 \t t = 33.191 \t\n",
      "[30/10000][768/782] G loss: -0.505 \t D loss: -0.236 \t D(x) = 0.761 \t D(G(z)) = 0.504 \t grad_pen = 0.020 \t t = 33.162 \t\n",
      "[31/10000][0/782] G loss: -0.503 \t D loss: -0.181 \t D(x) = 0.742 \t D(G(z)) = 0.498 \t grad_pen = 0.063 \t t = 1.787 \t\n",
      "[31/10000][256/782] G loss: -0.513 \t D loss: -0.207 \t D(x) = 0.751 \t D(G(z)) = 0.513 \t grad_pen = 0.030 \t t = 33.167 \t\n",
      "[31/10000][512/782] G loss: -0.523 \t D loss: -0.204 \t D(x) = 0.784 \t D(G(z)) = 0.536 \t grad_pen = 0.044 \t t = 33.131 \t\n",
      "[31/10000][768/782] G loss: -0.544 \t D loss: -0.208 \t D(x) = 0.791 \t D(G(z)) = 0.543 \t grad_pen = 0.040 \t t = 33.190 \t\n",
      "[32/10000][0/782] G loss: -0.497 \t D loss: -0.207 \t D(x) = 0.742 \t D(G(z)) = 0.507 \t grad_pen = 0.027 \t t = 1.788 \t\n",
      "[32/10000][256/782] G loss: -0.499 \t D loss: -0.190 \t D(x) = 0.712 \t D(G(z)) = 0.503 \t grad_pen = 0.018 \t t = 33.240 \t\n",
      "[32/10000][512/782] G loss: -0.508 \t D loss: -0.190 \t D(x) = 0.738 \t D(G(z)) = 0.494 \t grad_pen = 0.053 \t t = 33.182 \t\n",
      "[32/10000][768/782] G loss: -0.480 \t D loss: -0.183 \t D(x) = 0.710 \t D(G(z)) = 0.482 \t grad_pen = 0.044 \t t = 33.322 \t\n",
      "[33/10000][0/782] G loss: -0.538 \t D loss: -0.222 \t D(x) = 0.792 \t D(G(z)) = 0.536 \t grad_pen = 0.033 \t t = 1.791 \t\n",
      "[33/10000][256/782] G loss: -0.501 \t D loss: -0.207 \t D(x) = 0.725 \t D(G(z)) = 0.497 \t grad_pen = 0.022 \t t = 33.066 \t\n",
      "[33/10000][512/782] G loss: -0.501 \t D loss: -0.212 \t D(x) = 0.743 \t D(G(z)) = 0.502 \t grad_pen = 0.029 \t t = 33.169 \t\n",
      "[33/10000][768/782] G loss: -0.536 \t D loss: -0.227 \t D(x) = 0.780 \t D(G(z)) = 0.522 \t grad_pen = 0.031 \t t = 33.086 \t\n",
      "[34/10000][0/782] G loss: -0.519 \t D loss: -0.222 \t D(x) = 0.762 \t D(G(z)) = 0.523 \t grad_pen = 0.017 \t t = 1.787 \t\n",
      "[34/10000][256/782] G loss: -0.530 \t D loss: -0.212 \t D(x) = 0.795 \t D(G(z)) = 0.533 \t grad_pen = 0.050 \t t = 33.164 \t\n",
      "[34/10000][512/782] G loss: -0.506 \t D loss: -0.263 \t D(x) = 0.790 \t D(G(z)) = 0.503 \t grad_pen = 0.025 \t t = 33.275 \t\n",
      "[34/10000][768/782] G loss: -0.513 \t D loss: -0.223 \t D(x) = 0.761 \t D(G(z)) = 0.510 \t grad_pen = 0.027 \t t = 33.139 \t\n",
      "[35/10000][0/782] G loss: -0.526 \t D loss: -0.203 \t D(x) = 0.772 \t D(G(z)) = 0.527 \t grad_pen = 0.041 \t t = 1.787 \t\n",
      "[35/10000][256/782] G loss: -0.502 \t D loss: -0.212 \t D(x) = 0.755 \t D(G(z)) = 0.518 \t grad_pen = 0.025 \t t = 33.213 \t\n",
      "[35/10000][512/782] G loss: -0.491 \t D loss: -0.182 \t D(x) = 0.722 \t D(G(z)) = 0.511 \t grad_pen = 0.029 \t t = 33.161 \t\n",
      "[35/10000][768/782] G loss: -0.532 \t D loss: -0.209 \t D(x) = 0.765 \t D(G(z)) = 0.526 \t grad_pen = 0.030 \t t = 33.182 \t\n",
      "[36/10000][0/782] G loss: -0.520 \t D loss: -0.202 \t D(x) = 0.751 \t D(G(z)) = 0.516 \t grad_pen = 0.033 \t t = 1.790 \t\n",
      "[36/10000][256/782] G loss: -0.517 \t D loss: -0.211 \t D(x) = 0.766 \t D(G(z)) = 0.520 \t grad_pen = 0.035 \t t = 33.134 \t\n",
      "[36/10000][512/782] G loss: -0.514 \t D loss: -0.198 \t D(x) = 0.764 \t D(G(z)) = 0.519 \t grad_pen = 0.048 \t t = 33.167 \t\n",
      "[36/10000][768/782] G loss: -0.500 \t D loss: -0.210 \t D(x) = 0.739 \t D(G(z)) = 0.505 \t grad_pen = 0.024 \t t = 33.113 \t\n",
      "[37/10000][0/782] G loss: -0.506 \t D loss: -0.214 \t D(x) = 0.741 \t D(G(z)) = 0.508 \t grad_pen = 0.020 \t t = 1.784 \t\n",
      "[37/10000][256/782] G loss: -0.541 \t D loss: -0.202 \t D(x) = 0.771 \t D(G(z)) = 0.530 \t grad_pen = 0.040 \t t = 33.166 \t\n",
      "[37/10000][512/782] G loss: -0.506 \t D loss: -0.199 \t D(x) = 0.764 \t D(G(z)) = 0.531 \t grad_pen = 0.034 \t t = 33.125 \t\n",
      "[37/10000][768/782] G loss: -0.508 \t D loss: -0.207 \t D(x) = 0.753 \t D(G(z)) = 0.516 \t grad_pen = 0.030 \t t = 33.051 \t\n",
      "[38/10000][0/782] G loss: -0.516 \t D loss: -0.216 \t D(x) = 0.737 \t D(G(z)) = 0.501 \t grad_pen = 0.020 \t t = 1.785 \t\n",
      "[38/10000][256/782] G loss: -0.492 \t D loss: -0.229 \t D(x) = 0.745 \t D(G(z)) = 0.495 \t grad_pen = 0.021 \t t = 33.250 \t\n",
      "[38/10000][512/782] G loss: -0.501 \t D loss: -0.224 \t D(x) = 0.758 \t D(G(z)) = 0.502 \t grad_pen = 0.032 \t t = 33.130 \t\n",
      "[38/10000][768/782] G loss: -0.508 \t D loss: -0.155 \t D(x) = 0.742 \t D(G(z)) = 0.518 \t grad_pen = 0.068 \t t = 33.133 \t\n",
      "[39/10000][0/782] G loss: -0.507 \t D loss: -0.188 \t D(x) = 0.705 \t D(G(z)) = 0.505 \t grad_pen = 0.013 \t t = 1.780 \t\n",
      "[39/10000][256/782] G loss: -0.506 \t D loss: -0.232 \t D(x) = 0.751 \t D(G(z)) = 0.497 \t grad_pen = 0.022 \t t = 33.185 \t\n",
      "[39/10000][512/782] G loss: -0.494 \t D loss: -0.207 \t D(x) = 0.758 \t D(G(z)) = 0.505 \t grad_pen = 0.047 \t t = 33.139 \t\n",
      "[39/10000][768/782] G loss: -0.518 \t D loss: -0.207 \t D(x) = 0.752 \t D(G(z)) = 0.505 \t grad_pen = 0.040 \t t = 33.218 \t\n",
      "[40/10000][0/782] G loss: -0.492 \t D loss: -0.219 \t D(x) = 0.744 \t D(G(z)) = 0.498 \t grad_pen = 0.028 \t t = 1.790 \t\n",
      "[40/10000][256/782] G loss: -0.526 \t D loss: -0.212 \t D(x) = 0.777 \t D(G(z)) = 0.523 \t grad_pen = 0.042 \t t = 33.158 \t\n",
      "[40/10000][512/782] G loss: -0.520 \t D loss: -0.207 \t D(x) = 0.745 \t D(G(z)) = 0.519 \t grad_pen = 0.019 \t t = 33.209 \t\n",
      "[40/10000][768/782] G loss: -0.531 \t D loss: -0.219 \t D(x) = 0.780 \t D(G(z)) = 0.525 \t grad_pen = 0.037 \t t = 33.153 \t\n",
      "[41/10000][0/782] G loss: -0.529 \t D loss: -0.200 \t D(x) = 0.800 \t D(G(z)) = 0.535 \t grad_pen = 0.064 \t t = 1.783 \t\n",
      "[41/10000][256/782] G loss: -0.529 \t D loss: -0.204 \t D(x) = 0.773 \t D(G(z)) = 0.533 \t grad_pen = 0.036 \t t = 33.144 \t\n",
      "[41/10000][512/782] G loss: -0.532 \t D loss: -0.229 \t D(x) = 0.787 \t D(G(z)) = 0.533 \t grad_pen = 0.025 \t t = 33.137 \t\n",
      "[41/10000][768/782] G loss: -0.514 \t D loss: -0.215 \t D(x) = 0.762 \t D(G(z)) = 0.513 \t grad_pen = 0.034 \t t = 33.150 \t\n",
      "[42/10000][0/782] G loss: -0.532 \t D loss: -0.210 \t D(x) = 0.769 \t D(G(z)) = 0.521 \t grad_pen = 0.039 \t t = 1.794 \t\n",
      "[42/10000][256/782] G loss: -0.509 \t D loss: -0.227 \t D(x) = 0.761 \t D(G(z)) = 0.511 \t grad_pen = 0.023 \t t = 33.168 \t\n",
      "[42/10000][512/782] G loss: -0.546 \t D loss: -0.226 \t D(x) = 0.807 \t D(G(z)) = 0.546 \t grad_pen = 0.035 \t t = 33.183 \t\n",
      "[42/10000][768/782] G loss: -0.509 \t D loss: -0.225 \t D(x) = 0.775 \t D(G(z)) = 0.521 \t grad_pen = 0.030 \t t = 33.160 \t\n",
      "[43/10000][0/782] G loss: -0.530 \t D loss: -0.215 \t D(x) = 0.768 \t D(G(z)) = 0.519 \t grad_pen = 0.034 \t t = 1.783 \t\n",
      "[43/10000][256/782] G loss: -0.504 \t D loss: -0.188 \t D(x) = 0.715 \t D(G(z)) = 0.492 \t grad_pen = 0.035 \t t = 33.150 \t\n",
      "[43/10000][512/782] G loss: -0.504 \t D loss: -0.224 \t D(x) = 0.769 \t D(G(z)) = 0.509 \t grad_pen = 0.035 \t t = 33.116 \t\n",
      "[43/10000][768/782] G loss: -0.529 \t D loss: -0.193 \t D(x) = 0.787 \t D(G(z)) = 0.539 \t grad_pen = 0.055 \t t = 33.175 \t\n",
      "[44/10000][0/782] G loss: -0.490 \t D loss: -0.206 \t D(x) = 0.725 \t D(G(z)) = 0.488 \t grad_pen = 0.031 \t t = 1.789 \t\n",
      "[44/10000][256/782] G loss: -0.524 \t D loss: -0.231 \t D(x) = 0.780 \t D(G(z)) = 0.512 \t grad_pen = 0.038 \t t = 33.150 \t\n",
      "[44/10000][512/782] G loss: -0.514 \t D loss: -0.217 \t D(x) = 0.761 \t D(G(z)) = 0.516 \t grad_pen = 0.028 \t t = 33.198 \t\n",
      "[44/10000][768/782] G loss: -0.493 \t D loss: -0.211 \t D(x) = 0.749 \t D(G(z)) = 0.506 \t grad_pen = 0.032 \t t = 33.143 \t\n",
      "[45/10000][0/782] G loss: -0.521 \t D loss: -0.183 \t D(x) = 0.772 \t D(G(z)) = 0.526 \t grad_pen = 0.063 \t t = 1.791 \t\n",
      "[45/10000][256/782] G loss: -0.498 \t D loss: -0.213 \t D(x) = 0.736 \t D(G(z)) = 0.503 \t grad_pen = 0.019 \t t = 33.206 \t\n",
      "[45/10000][512/782] G loss: -0.512 \t D loss: -0.195 \t D(x) = 0.740 \t D(G(z)) = 0.513 \t grad_pen = 0.032 \t t = 33.102 \t\n",
      "[45/10000][768/782] G loss: -0.505 \t D loss: -0.185 \t D(x) = 0.740 \t D(G(z)) = 0.504 \t grad_pen = 0.051 \t t = 33.253 \t\n",
      "[46/10000][0/782] G loss: -0.510 \t D loss: -0.177 \t D(x) = 0.742 \t D(G(z)) = 0.521 \t grad_pen = 0.044 \t t = 1.805 \t\n",
      "[46/10000][256/782] G loss: -0.499 \t D loss: -0.199 \t D(x) = 0.742 \t D(G(z)) = 0.504 \t grad_pen = 0.038 \t t = 33.111 \t\n",
      "[46/10000][512/782] G loss: -0.500 \t D loss: -0.220 \t D(x) = 0.767 \t D(G(z)) = 0.516 \t grad_pen = 0.032 \t t = 33.211 \t\n",
      "[46/10000][768/782] G loss: -0.523 \t D loss: -0.233 \t D(x) = 0.771 \t D(G(z)) = 0.516 \t grad_pen = 0.022 \t t = 33.128 \t\n",
      "[47/10000][0/782] G loss: -0.487 \t D loss: -0.202 \t D(x) = 0.741 \t D(G(z)) = 0.506 \t grad_pen = 0.034 \t t = 1.794 \t\n",
      "[47/10000][256/782] G loss: -0.537 \t D loss: -0.208 \t D(x) = 0.793 \t D(G(z)) = 0.540 \t grad_pen = 0.046 \t t = 33.183 \t\n",
      "[47/10000][512/782] G loss: -0.494 \t D loss: -0.195 \t D(x) = 0.720 \t D(G(z)) = 0.496 \t grad_pen = 0.029 \t t = 33.086 \t\n",
      "[47/10000][768/782] G loss: -0.509 \t D loss: -0.203 \t D(x) = 0.776 \t D(G(z)) = 0.517 \t grad_pen = 0.056 \t t = 33.132 \t\n",
      "[48/10000][0/782] G loss: -0.506 \t D loss: -0.204 \t D(x) = 0.743 \t D(G(z)) = 0.518 \t grad_pen = 0.022 \t t = 1.784 \t\n",
      "[48/10000][256/782] G loss: -0.528 \t D loss: -0.209 \t D(x) = 0.777 \t D(G(z)) = 0.531 \t grad_pen = 0.037 \t t = 33.185 \t\n",
      "[48/10000][512/782] G loss: -0.510 \t D loss: -0.224 \t D(x) = 0.760 \t D(G(z)) = 0.521 \t grad_pen = 0.015 \t t = 33.165 \t\n",
      "[48/10000][768/782] G loss: -0.565 \t D loss: -0.236 \t D(x) = 0.804 \t D(G(z)) = 0.550 \t grad_pen = 0.018 \t t = 33.106 \t\n",
      "[49/10000][0/782] G loss: -0.497 \t D loss: -0.202 \t D(x) = 0.744 \t D(G(z)) = 0.509 \t grad_pen = 0.032 \t t = 1.789 \t\n",
      "[49/10000][256/782] G loss: -0.512 \t D loss: -0.207 \t D(x) = 0.754 \t D(G(z)) = 0.515 \t grad_pen = 0.032 \t t = 33.155 \t\n",
      "[49/10000][512/782] G loss: -0.522 \t D loss: -0.177 \t D(x) = 0.779 \t D(G(z)) = 0.535 \t grad_pen = 0.068 \t t = 33.197 \t\n",
      "[49/10000][768/782] G loss: -0.537 \t D loss: -0.174 \t D(x) = 0.777 \t D(G(z)) = 0.525 \t grad_pen = 0.078 \t t = 33.151 \t\n",
      "[50/10000][0/782] G loss: -0.500 \t D loss: -0.210 \t D(x) = 0.747 \t D(G(z)) = 0.515 \t grad_pen = 0.022 \t t = 1.785 \t\n",
      "[50/10000][256/782] G loss: -0.528 \t D loss: -0.215 \t D(x) = 0.787 \t D(G(z)) = 0.535 \t grad_pen = 0.038 \t t = 33.221 \t\n",
      "[50/10000][512/782] G loss: -0.506 \t D loss: -0.213 \t D(x) = 0.732 \t D(G(z)) = 0.495 \t grad_pen = 0.024 \t t = 33.128 \t\n",
      "[50/10000][768/782] G loss: -0.520 \t D loss: -0.185 \t D(x) = 0.785 \t D(G(z)) = 0.528 \t grad_pen = 0.071 \t t = 33.196 \t\n",
      "[51/10000][0/782] G loss: -0.554 \t D loss: -0.167 \t D(x) = 0.803 \t D(G(z)) = 0.564 \t grad_pen = 0.072 \t t = 1.786 \t\n",
      "[51/10000][256/782] G loss: -0.540 \t D loss: -0.227 \t D(x) = 0.767 \t D(G(z)) = 0.511 \t grad_pen = 0.029 \t t = 33.116 \t\n",
      "[51/10000][512/782] G loss: -0.516 \t D loss: -0.212 \t D(x) = 0.771 \t D(G(z)) = 0.514 \t grad_pen = 0.044 \t t = 33.196 \t\n",
      "[51/10000][768/782] G loss: -0.531 \t D loss: -0.204 \t D(x) = 0.779 \t D(G(z)) = 0.538 \t grad_pen = 0.037 \t t = 33.078 \t\n",
      "[52/10000][0/782] G loss: -0.518 \t D loss: -0.218 \t D(x) = 0.761 \t D(G(z)) = 0.525 \t grad_pen = 0.018 \t t = 1.785 \t\n",
      "[52/10000][256/782] G loss: -0.515 \t D loss: -0.197 \t D(x) = 0.760 \t D(G(z)) = 0.522 \t grad_pen = 0.041 \t t = 33.224 \t\n",
      "[52/10000][512/782] G loss: -0.500 \t D loss: -0.199 \t D(x) = 0.739 \t D(G(z)) = 0.500 \t grad_pen = 0.040 \t t = 33.123 \t\n",
      "[52/10000][768/782] G loss: -0.495 \t D loss: -0.199 \t D(x) = 0.751 \t D(G(z)) = 0.506 \t grad_pen = 0.046 \t t = 33.174 \t\n",
      "[53/10000][0/782] G loss: -0.551 \t D loss: -0.196 \t D(x) = 0.791 \t D(G(z)) = 0.559 \t grad_pen = 0.035 \t t = 1.784 \t\n",
      "[53/10000][256/782] G loss: -0.504 \t D loss: -0.214 \t D(x) = 0.746 \t D(G(z)) = 0.515 \t grad_pen = 0.017 \t t = 33.080 \t\n",
      "[53/10000][512/782] G loss: -0.528 \t D loss: -0.197 \t D(x) = 0.792 \t D(G(z)) = 0.547 \t grad_pen = 0.048 \t t = 33.208 \t\n",
      "[53/10000][768/782] G loss: -0.515 \t D loss: -0.207 \t D(x) = 0.744 \t D(G(z)) = 0.512 \t grad_pen = 0.025 \t t = 33.096 \t\n",
      "[54/10000][0/782] G loss: -0.513 \t D loss: -0.181 \t D(x) = 0.763 \t D(G(z)) = 0.519 \t grad_pen = 0.063 \t t = 1.797 \t\n",
      "[54/10000][256/782] G loss: -0.487 \t D loss: -0.203 \t D(x) = 0.700 \t D(G(z)) = 0.478 \t grad_pen = 0.019 \t t = 33.224 \t\n",
      "[54/10000][512/782] G loss: -0.500 \t D loss: -0.207 \t D(x) = 0.715 \t D(G(z)) = 0.487 \t grad_pen = 0.022 \t t = 33.161 \t\n",
      "[54/10000][768/782] G loss: -0.510 \t D loss: -0.213 \t D(x) = 0.742 \t D(G(z)) = 0.500 \t grad_pen = 0.030 \t t = 33.201 \t\n",
      "[55/10000][0/782] G loss: -0.541 \t D loss: -0.230 \t D(x) = 0.803 \t D(G(z)) = 0.550 \t grad_pen = 0.024 \t t = 1.787 \t\n",
      "[55/10000][256/782] G loss: -0.520 \t D loss: -0.230 \t D(x) = 0.804 \t D(G(z)) = 0.544 \t grad_pen = 0.030 \t t = 33.135 \t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-33d8d2ecd5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mg_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mreplica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mreplica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mreplica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mreplica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mmodule_copies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplica\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    535\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Highly adapted from: https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n",
    "\"\"\"\n",
    "\n",
    "g_iters = 5 # 5\n",
    "d_iters = 1 # 1, discriminator is called critic in WGAN paper\n",
    "\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = one * -1\n",
    "\n",
    "iters = 0\n",
    "t1 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        real = data.to(device)\n",
    "        b_size = real.size(0)\n",
    "        \n",
    "        \"\"\"\n",
    "        Train G\n",
    "        \"\"\"\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        for _ in range(g_iters):\n",
    "            netG.zero_grad()\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            noise.requires_grad_(True)\n",
    "            fake = netG(noise)\n",
    "\n",
    "            g_cost = netD(fake).mean()\n",
    "            g_cost.backward(mone)\n",
    "            g_cost = -g_cost\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        \"\"\"\n",
    "        Train D\n",
    "        \"\"\"\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "        for _ in range(d_iters):\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # generate fake data\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                noisev = noise # Freeze G, training D\n",
    "\n",
    "            fake = netG(noisev).detach()\n",
    "\n",
    "            # train with real data\n",
    "            d_real = netD(real).mean()\n",
    "\n",
    "            # train with fake data\n",
    "            d_fake = netD(fake).mean()\n",
    "\n",
    "            # train with interpolates data\n",
    "            gradient_penalty = calc_gradient_penalty(netD, real, fake, b_size)\n",
    "\n",
    "             # final disc cost\n",
    "            d_cost = d_fake - d_real + gradient_penalty\n",
    "            d_cost.backward()\n",
    "            w_dist = d_fake  - d_real # wasserstein distance\n",
    "            optimizerD.step()\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        weights_saved = False\n",
    "        if (iters % 100 == 0): # save weights every % .... iters\n",
    "            #print('weights saved')\n",
    "            if ngpu > 1:\n",
    "                torch.save(netG.module.state_dict(), 'netG_state_dict')\n",
    "                torch.save(netD.module.state_dict(), 'netD_state_dict')\n",
    "            else:\n",
    "                torch.save(netG.state_dict(), 'netG_state_dict')\n",
    "                torch.save(netD.state_dict(), 'netD_state_dict')\n",
    "            \n",
    "        \n",
    "        if i % (256) == 0:\n",
    "            t2 = time.time()\n",
    "            print('[%d/%d][%d/%d] G loss: %.3f \\t D loss: %.3f \\t D(x) = %.3f \\t D(G(z)) = %.3f \\t grad_pen = %.3f \\t t = %.3f \\t'% \n",
    "                      (epoch, num_epochs, i, len(dataloader), g_cost, d_cost, d_real, d_fake, gradient_penalty, (t2-t1)))\n",
    "            t1 = time.time()\n",
    "                \n",
    "        iters += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
