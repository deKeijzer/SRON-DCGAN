{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2d641338f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import time as t\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import time as time\n",
    "\n",
    "from torch import autograd\n",
    "\n",
    "import model\n",
    "from keijzer_exogan import *\n",
    "\n",
    "# initialize random seeds\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Local variables\n",
    "\"\"\"\n",
    "workers = 0 # Number of workers for dataloader, 0 when to_vram is enabled\n",
    "batch_size = 64 # 2**11\n",
    "image_size = 32\n",
    "nz = 100 # size of latent vector\n",
    "num_epochs = 10*10**3\n",
    "torch.backends.cudnn.benchmark=True # Uses udnn auto-tuner to find the best algorithm to use for your hardware, speeds up training by almost 50%\n",
    "lr = 1e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9\n",
    "\n",
    "lambda_ = 10\n",
    "\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "selected_gpus = [2,3] # Number of GPUs available. Use 0 for CPU mode.\n",
    "\n",
    "path = '/datb/16011015/ExoGAN_data/selection//' #notice how you dont put the last folder in here...\n",
    "images = np.load(path+'first_chunks_25_percent_images.npy')\n",
    "\n",
    "swap_labels_randomly = False\n",
    "\n",
    "train_d_g_conditional = False # switch between training D and G based on set threshold\n",
    "d_g_conditional_threshold = 0.55 # D_G_z1 < threshold, train G\n",
    "\n",
    "train_d_g_conditional_per_epoch = False\n",
    "\n",
    "train_d_g_conditional_per_n_iters = False\n",
    "train_d_g_n_iters = 2 # When 2, train D 2 times before training G 1 time\n",
    "\n",
    "use_saved_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  64\n",
      "Number of GPUs used:  2\n",
      "Number of images:  50000\n"
     ]
    }
   ],
   "source": [
    "print('Batch size: ', batch_size)\n",
    "ngpu = len(selected_gpus)\n",
    "print('Number of GPUs used: ', ngpu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load data and prepare DataLoader\n",
    "\"\"\"\n",
    "shuffle = True\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(images) # shuffles the images\n",
    "\n",
    "images = images[:int(len(images)*0.1)] # use only first ... percent of the data (0.05)\n",
    "print('Number of images: ', len(images))\n",
    "\n",
    "dataset = numpy_dataset(data=images, to_vram=True) # to_vram pins it to all GPU's\n",
    "#dataset = numpy_dataset(data=images, to_vram=True, transform=transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])) # to_vram pins it to all GPU's\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load and setup models\n",
    "\"\"\"\n",
    "# Initialize cuda\n",
    "device = torch.device(\"cuda:\"+str(selected_gpus[0]) if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Load models\n",
    "netG = model.Generator(ngpu).to(device)\n",
    "netD = model.Discriminator(ngpu).to(device)\n",
    "\n",
    "# Apply weights\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netG.apply(weights_init) # It's not clean/efficient to load these ones first, but it works.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "if use_saved_weights:\n",
    "    try:\n",
    "        # Load saved weights\n",
    "        netG.load_state_dict(torch.load('netG_state_dict2', map_location=device)) #net.module..load_... for parallel model , net.load_... for single gpu model\n",
    "        netD.load_state_dict(torch.load('netD_state_dict2', map_location=device))\n",
    "        print('Succesfully loaded saved weights.')\n",
    "    except:\n",
    "        print('Could not load saved weights, using new ones.')\n",
    "        pass\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, device_ids=selected_gpus, output_device=device)\n",
    "    netD = nn.DataParallel(netD, device_ids=selected_gpus, output_device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define input training stuff (fancy this up)\n",
    "\"\"\"\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2)) # should be sgd\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "switch = True # condition switch, to switch between D and G per epoch\n",
    "previous_switch = 0\n",
    "\n",
    "train_D = True\n",
    "train_G = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data, b_size):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n",
    "    \"\"\"\n",
    "    alpha = torch.rand(b_size, 1)\n",
    "    alpha = alpha.expand(b_size, int(real_data.nelement()/b_size)).contiguous()\n",
    "    alpha = alpha.view(b_size, 1, image_size, image_size)\n",
    "    alpha = alpha.to(device)\n",
    "    \n",
    "    fake_data = fake_data.view(b_size, 1, image_size, image_size)\n",
    "    interpolates = alpha * real_data.detach() + ((1 - alpha) * fake_data.detach())\n",
    "\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates.requires_grad_(True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)                              \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][0/782] G loss: -0.517 \t D loss: 0.507 \t D(x) = 0.415 \t D(G(z)) = 0.516 \t grad_pen = 0.406 \t t = 0.105 \t\n",
      "[0/10][16/782] G loss: -0.474 \t D loss: 0.466 \t D(x) = 0.344 \t D(G(z)) = 0.485 \t grad_pen = 0.325 \t t = 1.386 \t\n",
      "[0/10][32/782] G loss: -0.509 \t D loss: 0.290 \t D(x) = 0.487 \t D(G(z)) = 0.533 \t grad_pen = 0.244 \t t = 1.370 \t\n",
      "[0/10][48/782] G loss: -0.514 \t D loss: 0.331 \t D(x) = 0.434 \t D(G(z)) = 0.501 \t grad_pen = 0.265 \t t = 1.352 \t\n",
      "[0/10][64/782] G loss: -0.528 \t D loss: 0.342 \t D(x) = 0.498 \t D(G(z)) = 0.548 \t grad_pen = 0.292 \t t = 1.358 \t\n",
      "[0/10][80/782] G loss: -0.552 \t D loss: 0.258 \t D(x) = 0.519 \t D(G(z)) = 0.576 \t grad_pen = 0.202 \t t = 1.346 \t\n",
      "[0/10][96/782] G loss: -0.495 \t D loss: 0.141 \t D(x) = 0.473 \t D(G(z)) = 0.435 \t grad_pen = 0.178 \t t = 1.345 \t\n",
      "[0/10][112/782] G loss: -0.446 \t D loss: 0.135 \t D(x) = 0.490 \t D(G(z)) = 0.424 \t grad_pen = 0.200 \t t = 1.349 \t\n",
      "[0/10][128/782] G loss: -0.505 \t D loss: 0.277 \t D(x) = 0.537 \t D(G(z)) = 0.523 \t grad_pen = 0.291 \t t = 1.344 \t\n",
      "[0/10][144/782] G loss: -0.513 \t D loss: 0.243 \t D(x) = 0.434 \t D(G(z)) = 0.452 \t grad_pen = 0.224 \t t = 1.353 \t\n",
      "[0/10][160/782] G loss: -0.544 \t D loss: 0.340 \t D(x) = 0.464 \t D(G(z)) = 0.505 \t grad_pen = 0.298 \t t = 1.368 \t\n",
      "[0/10][176/782] G loss: -0.564 \t D loss: 0.202 \t D(x) = 0.484 \t D(G(z)) = 0.522 \t grad_pen = 0.164 \t t = 1.370 \t\n",
      "[0/10][192/782] G loss: -0.552 \t D loss: 0.235 \t D(x) = 0.551 \t D(G(z)) = 0.558 \t grad_pen = 0.228 \t t = 1.355 \t\n",
      "[0/10][208/782] G loss: -0.532 \t D loss: 0.148 \t D(x) = 0.581 \t D(G(z)) = 0.532 \t grad_pen = 0.197 \t t = 1.373 \t\n",
      "[0/10][224/782] G loss: -0.503 \t D loss: 0.155 \t D(x) = 0.624 \t D(G(z)) = 0.542 \t grad_pen = 0.237 \t t = 1.357 \t\n",
      "[0/10][240/782] G loss: -0.474 \t D loss: -0.020 \t D(x) = 0.673 \t D(G(z)) = 0.504 \t grad_pen = 0.150 \t t = 1.359 \t\n",
      "[0/10][256/782] G loss: -0.514 \t D loss: -0.009 \t D(x) = 0.674 \t D(G(z)) = 0.528 \t grad_pen = 0.136 \t t = 1.340 \t\n",
      "[0/10][272/782] G loss: -0.495 \t D loss: 0.027 \t D(x) = 0.655 \t D(G(z)) = 0.531 \t grad_pen = 0.151 \t t = 1.334 \t\n",
      "[0/10][288/782] G loss: -0.489 \t D loss: 0.083 \t D(x) = 0.587 \t D(G(z)) = 0.500 \t grad_pen = 0.170 \t t = 1.335 \t\n",
      "[0/10][304/782] G loss: -0.522 \t D loss: 0.099 \t D(x) = 0.711 \t D(G(z)) = 0.583 \t grad_pen = 0.227 \t t = 1.353 \t\n",
      "[0/10][320/782] G loss: -0.923 \t D loss: 0.703 \t D(x) = 0.607 \t D(G(z)) = 0.929 \t grad_pen = 0.381 \t t = 1.353 \t\n",
      "[0/10][336/782] G loss: -0.863 \t D loss: 0.535 \t D(x) = 0.596 \t D(G(z)) = 0.853 \t grad_pen = 0.278 \t t = 1.362 \t\n",
      "[0/10][352/782] G loss: -0.729 \t D loss: 0.213 \t D(x) = 0.655 \t D(G(z)) = 0.701 \t grad_pen = 0.166 \t t = 1.345 \t\n",
      "[0/10][368/782] G loss: -0.369 \t D loss: -0.133 \t D(x) = 0.646 \t D(G(z)) = 0.355 \t grad_pen = 0.158 \t t = 1.334 \t\n",
      "[0/10][384/782] G loss: -0.454 \t D loss: -0.045 \t D(x) = 0.640 \t D(G(z)) = 0.449 \t grad_pen = 0.146 \t t = 1.351 \t\n",
      "[0/10][400/782] G loss: -0.515 \t D loss: 0.063 \t D(x) = 0.736 \t D(G(z)) = 0.551 \t grad_pen = 0.248 \t t = 1.358 \t\n",
      "[0/10][416/782] G loss: -0.570 \t D loss: -0.012 \t D(x) = 0.679 \t D(G(z)) = 0.557 \t grad_pen = 0.110 \t t = 1.341 \t\n",
      "[0/10][432/782] G loss: -0.587 \t D loss: 0.014 \t D(x) = 0.641 \t D(G(z)) = 0.561 \t grad_pen = 0.095 \t t = 1.352 \t\n",
      "[0/10][448/782] G loss: -0.518 \t D loss: 0.017 \t D(x) = 0.698 \t D(G(z)) = 0.573 \t grad_pen = 0.143 \t t = 1.370 \t\n",
      "[0/10][464/782] G loss: -0.531 \t D loss: -0.041 \t D(x) = 0.640 \t D(G(z)) = 0.520 \t grad_pen = 0.079 \t t = 1.352 \t\n",
      "[0/10][480/782] G loss: -0.487 \t D loss: -0.041 \t D(x) = 0.704 \t D(G(z)) = 0.514 \t grad_pen = 0.149 \t t = 1.357 \t\n",
      "[0/10][496/782] G loss: -0.513 \t D loss: 0.006 \t D(x) = 0.653 \t D(G(z)) = 0.516 \t grad_pen = 0.143 \t t = 1.350 \t\n",
      "[0/10][512/782] G loss: -0.537 \t D loss: -0.099 \t D(x) = 0.697 \t D(G(z)) = 0.528 \t grad_pen = 0.070 \t t = 1.354 \t\n",
      "[0/10][528/782] G loss: -0.475 \t D loss: -0.057 \t D(x) = 0.680 \t D(G(z)) = 0.467 \t grad_pen = 0.156 \t t = 1.338 \t\n",
      "[0/10][544/782] G loss: -0.518 \t D loss: -0.005 \t D(x) = 0.659 \t D(G(z)) = 0.490 \t grad_pen = 0.165 \t t = 1.338 \t\n",
      "[0/10][560/782] G loss: -1.000 \t D loss: 6.149 \t D(x) = 0.934 \t D(G(z)) = 1.000 \t grad_pen = 6.082 \t t = 1.331 \t\n",
      "[0/10][576/782] G loss: -0.998 \t D loss: 3.783 \t D(x) = 0.894 \t D(G(z)) = 1.000 \t grad_pen = 3.677 \t t = 1.354 \t\n",
      "[0/10][592/782] G loss: -1.000 \t D loss: 2.853 \t D(x) = 0.847 \t D(G(z)) = 1.000 \t grad_pen = 2.701 \t t = 1.335 \t\n",
      "[0/10][608/782] G loss: -1.000 \t D loss: 2.526 \t D(x) = 0.807 \t D(G(z)) = 0.993 \t grad_pen = 2.340 \t t = 1.370 \t\n",
      "[0/10][624/782] G loss: -0.996 \t D loss: 2.581 \t D(x) = 0.772 \t D(G(z)) = 0.993 \t grad_pen = 2.360 \t t = 1.350 \t\n",
      "[0/10][640/782] G loss: -0.999 \t D loss: 3.777 \t D(x) = 0.856 \t D(G(z)) = 1.000 \t grad_pen = 3.633 \t t = 1.341 \t\n",
      "[0/10][656/782] G loss: -1.000 \t D loss: 3.043 \t D(x) = 0.744 \t D(G(z)) = 1.000 \t grad_pen = 2.787 \t t = 1.341 \t\n",
      "[0/10][672/782] G loss: -0.999 \t D loss: 3.241 \t D(x) = 0.660 \t D(G(z)) = 1.000 \t grad_pen = 2.902 \t t = 1.337 \t\n",
      "[0/10][688/782] G loss: -0.977 \t D loss: 1.350 \t D(x) = 0.668 \t D(G(z)) = 0.982 \t grad_pen = 1.036 \t t = 1.352 \t\n",
      "[0/10][704/782] G loss: -0.898 \t D loss: 0.827 \t D(x) = 0.509 \t D(G(z)) = 0.850 \t grad_pen = 0.487 \t t = 1.361 \t\n",
      "[0/10][720/782] G loss: -0.462 \t D loss: 0.222 \t D(x) = 0.536 \t D(G(z)) = 0.482 \t grad_pen = 0.276 \t t = 1.364 \t\n",
      "[0/10][736/782] G loss: -0.102 \t D loss: 2.105 \t D(x) = 0.068 \t D(G(z)) = 0.087 \t grad_pen = 2.087 \t t = 1.353 \t\n",
      "[0/10][752/782] G loss: -0.097 \t D loss: 1.875 \t D(x) = 0.029 \t D(G(z)) = 0.072 \t grad_pen = 1.832 \t t = 1.351 \t\n",
      "[0/10][768/782] G loss: -0.103 \t D loss: 3.413 \t D(x) = 0.024 \t D(G(z)) = 0.123 \t grad_pen = 3.314 \t t = 1.348 \t\n",
      "[1/10][0/782] G loss: -0.120 \t D loss: 5.482 \t D(x) = 0.001 \t D(G(z)) = 0.131 \t grad_pen = 5.352 \t t = 1.151 \t\n",
      "[1/10][16/782] G loss: -0.213 \t D loss: 6.252 \t D(x) = 0.002 \t D(G(z)) = 0.181 \t grad_pen = 6.073 \t t = 1.339 \t\n",
      "[1/10][32/782] G loss: -0.219 \t D loss: 7.461 \t D(x) = 0.008 \t D(G(z)) = 0.246 \t grad_pen = 7.223 \t t = 1.338 \t\n",
      "[1/10][48/782] G loss: -0.271 \t D loss: 7.916 \t D(x) = 0.000 \t D(G(z)) = 0.285 \t grad_pen = 7.632 \t t = 1.346 \t\n",
      "[1/10][64/782] G loss: -0.364 \t D loss: 9.248 \t D(x) = 0.000 \t D(G(z)) = 0.365 \t grad_pen = 8.883 \t t = 1.353 \t\n",
      "[1/10][80/782] G loss: -0.482 \t D loss: 2.367 \t D(x) = 0.324 \t D(G(z)) = 0.487 \t grad_pen = 2.205 \t t = 1.346 \t\n",
      "[1/10][96/782] G loss: -0.499 \t D loss: 1.752 \t D(x) = 0.431 \t D(G(z)) = 0.504 \t grad_pen = 1.679 \t t = 1.357 \t\n",
      "[1/10][112/782] G loss: -0.553 \t D loss: 1.075 \t D(x) = 0.427 \t D(G(z)) = 0.560 \t grad_pen = 0.942 \t t = 1.352 \t\n",
      "[1/10][128/782] G loss: -0.589 \t D loss: 0.560 \t D(x) = 0.649 \t D(G(z)) = 0.573 \t grad_pen = 0.635 \t t = 1.338 \t\n",
      "[1/10][144/782] G loss: -0.597 \t D loss: 0.018 \t D(x) = 0.781 \t D(G(z)) = 0.604 \t grad_pen = 0.195 \t t = 1.336 \t\n",
      "[1/10][160/782] G loss: -0.560 \t D loss: -0.055 \t D(x) = 0.748 \t D(G(z)) = 0.559 \t grad_pen = 0.134 \t t = 1.336 \t\n",
      "[1/10][176/782] G loss: -0.569 \t D loss: -0.069 \t D(x) = 0.742 \t D(G(z)) = 0.549 \t grad_pen = 0.123 \t t = 1.349 \t\n",
      "[1/10][192/782] G loss: -0.560 \t D loss: -0.006 \t D(x) = 0.719 \t D(G(z)) = 0.574 \t grad_pen = 0.139 \t t = 1.342 \t\n",
      "[1/10][208/782] G loss: -0.595 \t D loss: 0.092 \t D(x) = 0.666 \t D(G(z)) = 0.569 \t grad_pen = 0.189 \t t = 1.349 \t\n",
      "[1/10][224/782] G loss: -0.559 \t D loss: 0.180 \t D(x) = 0.605 \t D(G(z)) = 0.555 \t grad_pen = 0.230 \t t = 1.351 \t\n",
      "[1/10][240/782] G loss: -0.554 \t D loss: 0.073 \t D(x) = 0.635 \t D(G(z)) = 0.521 \t grad_pen = 0.186 \t t = 1.359 \t\n",
      "[1/10][256/782] G loss: -0.561 \t D loss: 0.094 \t D(x) = 0.670 \t D(G(z)) = 0.539 \t grad_pen = 0.224 \t t = 1.347 \t\n",
      "[1/10][272/782] G loss: -0.469 \t D loss: -0.058 \t D(x) = 0.674 \t D(G(z)) = 0.523 \t grad_pen = 0.093 \t t = 1.344 \t\n",
      "[1/10][288/782] G loss: -0.516 \t D loss: 0.018 \t D(x) = 0.697 \t D(G(z)) = 0.552 \t grad_pen = 0.163 \t t = 1.337 \t\n",
      "[1/10][304/782] G loss: -0.520 \t D loss: 0.028 \t D(x) = 0.648 \t D(G(z)) = 0.539 \t grad_pen = 0.137 \t t = 1.339 \t\n",
      "[1/10][320/782] G loss: -0.531 \t D loss: -0.000 \t D(x) = 0.651 \t D(G(z)) = 0.527 \t grad_pen = 0.124 \t t = 1.334 \t\n",
      "[1/10][336/782] G loss: -0.516 \t D loss: -0.042 \t D(x) = 0.653 \t D(G(z)) = 0.490 \t grad_pen = 0.121 \t t = 1.335 \t\n",
      "[1/10][352/782] G loss: -0.511 \t D loss: -0.009 \t D(x) = 0.692 \t D(G(z)) = 0.501 \t grad_pen = 0.183 \t t = 1.333 \t\n",
      "[1/10][368/782] G loss: -0.536 \t D loss: -0.079 \t D(x) = 0.743 \t D(G(z)) = 0.532 \t grad_pen = 0.132 \t t = 1.332 \t\n",
      "[1/10][384/782] G loss: -0.544 \t D loss: -0.061 \t D(x) = 0.670 \t D(G(z)) = 0.526 \t grad_pen = 0.084 \t t = 1.337 \t\n",
      "[1/10][400/782] G loss: -0.517 \t D loss: 0.040 \t D(x) = 0.750 \t D(G(z)) = 0.561 \t grad_pen = 0.228 \t t = 1.338 \t\n",
      "[1/10][416/782] G loss: -0.577 \t D loss: 0.014 \t D(x) = 0.713 \t D(G(z)) = 0.580 \t grad_pen = 0.148 \t t = 1.342 \t\n",
      "[1/10][432/782] G loss: -0.585 \t D loss: 0.000 \t D(x) = 0.705 \t D(G(z)) = 0.585 \t grad_pen = 0.121 \t t = 1.355 \t\n",
      "[1/10][448/782] G loss: -0.515 \t D loss: 0.107 \t D(x) = 0.534 \t D(G(z)) = 0.516 \t grad_pen = 0.125 \t t = 1.363 \t\n",
      "[1/10][464/782] G loss: -0.481 \t D loss: -0.053 \t D(x) = 0.669 \t D(G(z)) = 0.496 \t grad_pen = 0.120 \t t = 1.358 \t\n",
      "[1/10][480/782] G loss: -0.506 \t D loss: -0.045 \t D(x) = 0.689 \t D(G(z)) = 0.507 \t grad_pen = 0.137 \t t = 1.352 \t\n",
      "[1/10][496/782] G loss: -0.537 \t D loss: -0.040 \t D(x) = 0.738 \t D(G(z)) = 0.552 \t grad_pen = 0.146 \t t = 1.347 \t\n",
      "[1/10][512/782] G loss: -0.027 \t D loss: 4.556 \t D(x) = 0.033 \t D(G(z)) = 0.033 \t grad_pen = 4.555 \t t = 1.339 \t\n",
      "[1/10][528/782] G loss: -0.154 \t D loss: 5.143 \t D(x) = 0.007 \t D(G(z)) = 0.152 \t grad_pen = 4.999 \t t = 1.339 \t\n",
      "[1/10][544/782] G loss: -0.206 \t D loss: 9.461 \t D(x) = 0.000 \t D(G(z)) = 0.213 \t grad_pen = 9.248 \t t = 1.348 \t\n",
      "[1/10][560/782] G loss: -0.214 \t D loss: 9.499 \t D(x) = 0.000 \t D(G(z)) = 0.209 \t grad_pen = 9.291 \t t = 1.354 \t\n",
      "[1/10][576/782] G loss: -0.172 \t D loss: 10.182 \t D(x) = 0.000 \t D(G(z)) = 0.183 \t grad_pen = 9.999 \t t = 1.361 \t\n",
      "[1/10][592/782] G loss: -0.202 \t D loss: 16.298 \t D(x) = 0.000 \t D(G(z)) = 0.222 \t grad_pen = 16.076 \t t = 1.350 \t\n",
      "[1/10][608/782] G loss: -0.246 \t D loss: 10.248 \t D(x) = 0.000 \t D(G(z)) = 0.248 \t grad_pen = 10.000 \t t = 1.353 \t\n",
      "[1/10][624/782] G loss: -0.280 \t D loss: 10.264 \t D(x) = 0.000 \t D(G(z)) = 0.288 \t grad_pen = 9.976 \t t = 1.363 \t\n",
      "[1/10][640/782] G loss: -0.357 \t D loss: 10.347 \t D(x) = 0.000 \t D(G(z)) = 0.347 \t grad_pen = 10.000 \t t = 1.362 \t\n",
      "[1/10][656/782] G loss: -0.392 \t D loss: 10.378 \t D(x) = 0.000 \t D(G(z)) = 0.400 \t grad_pen = 9.979 \t t = 1.372 \t\n",
      "[1/10][672/782] G loss: -0.415 \t D loss: 10.404 \t D(x) = 0.000 \t D(G(z)) = 0.413 \t grad_pen = 9.991 \t t = 1.374 \t\n",
      "[1/10][688/782] G loss: -0.436 \t D loss: 11.256 \t D(x) = 0.000 \t D(G(z)) = 0.435 \t grad_pen = 10.820 \t t = 1.356 \t\n",
      "[1/10][704/782] G loss: -0.453 \t D loss: 10.322 \t D(x) = 0.000 \t D(G(z)) = 0.456 \t grad_pen = 9.866 \t t = 1.353 \t\n",
      "[1/10][720/782] G loss: -0.462 \t D loss: 10.420 \t D(x) = 0.000 \t D(G(z)) = 0.465 \t grad_pen = 9.955 \t t = 1.352 \t\n",
      "[1/10][736/782] G loss: -0.472 \t D loss: 10.440 \t D(x) = 0.000 \t D(G(z)) = 0.471 \t grad_pen = 9.969 \t t = 1.348 \t\n",
      "[1/10][752/782] G loss: -0.478 \t D loss: 12.742 \t D(x) = 0.000 \t D(G(z)) = 0.479 \t grad_pen = 12.263 \t t = 1.350 \t\n",
      "[1/10][768/782] G loss: -0.487 \t D loss: 9.868 \t D(x) = 0.000 \t D(G(z)) = 0.487 \t grad_pen = 9.381 \t t = 1.363 \t\n",
      "[2/10][0/782] G loss: -0.497 \t D loss: 6.386 \t D(x) = 0.033 \t D(G(z)) = 0.498 \t grad_pen = 5.920 \t t = 1.163 \t\n",
      "[2/10][16/782] G loss: -0.996 \t D loss: 9.975 \t D(x) = 1.000 \t D(G(z)) = 0.994 \t grad_pen = 9.981 \t t = 1.356 \t\n",
      "[2/10][32/782] G loss: -0.999 \t D loss: 9.985 \t D(x) = 1.000 \t D(G(z)) = 0.999 \t grad_pen = 9.986 \t t = 1.348 \t\n",
      "[2/10][48/782] G loss: -0.934 \t D loss: 6.766 \t D(x) = 0.819 \t D(G(z)) = 0.963 \t grad_pen = 6.622 \t t = 1.348 \t\n",
      "[2/10][64/782] G loss: -0.912 \t D loss: 2.457 \t D(x) = 0.367 \t D(G(z)) = 0.820 \t grad_pen = 2.004 \t t = 1.347 \t\n",
      "[2/10][80/782] G loss: -0.645 \t D loss: 1.348 \t D(x) = 0.309 \t D(G(z)) = 0.643 \t grad_pen = 1.013 \t t = 1.363 \t\n",
      "[2/10][96/782] G loss: -0.287 \t D loss: 4.302 \t D(x) = 0.037 \t D(G(z)) = 0.043 \t grad_pen = 4.297 \t t = 1.349 \t\n",
      "[2/10][112/782] G loss: -0.183 \t D loss: 2.638 \t D(x) = 0.117 \t D(G(z)) = 0.166 \t grad_pen = 2.589 \t t = 1.352 \t\n",
      "[2/10][128/782] G loss: -0.456 \t D loss: 1.585 \t D(x) = 0.545 \t D(G(z)) = 0.462 \t grad_pen = 1.668 \t t = 1.349 \t\n",
      "[2/10][144/782] G loss: -0.504 \t D loss: -0.152 \t D(x) = 0.726 \t D(G(z)) = 0.508 \t grad_pen = 0.067 \t t = 1.356 \t\n",
      "[2/10][160/782] G loss: -0.530 \t D loss: 0.550 \t D(x) = 0.802 \t D(G(z)) = 0.541 \t grad_pen = 0.811 \t t = 1.360 \t\n",
      "[2/10][176/782] G loss: -0.933 \t D loss: 1.045 \t D(x) = 0.749 \t D(G(z)) = 0.949 \t grad_pen = 0.844 \t t = 1.362 \t\n",
      "[2/10][192/782] G loss: -0.740 \t D loss: 0.642 \t D(x) = 0.509 \t D(G(z)) = 0.783 \t grad_pen = 0.368 \t t = 1.354 \t\n",
      "[2/10][208/782] G loss: -0.580 \t D loss: 0.212 \t D(x) = 0.467 \t D(G(z)) = 0.573 \t grad_pen = 0.106 \t t = 1.348 \t\n",
      "[2/10][224/782] G loss: -0.041 \t D loss: 3.024 \t D(x) = 0.098 \t D(G(z)) = 0.059 \t grad_pen = 3.063 \t t = 1.344 \t\n",
      "[2/10][240/782] G loss: -0.115 \t D loss: 1.496 \t D(x) = 0.141 \t D(G(z)) = 0.137 \t grad_pen = 1.499 \t t = 1.357 \t\n",
      "[2/10][256/782] G loss: -0.365 \t D loss: 2.207 \t D(x) = 0.211 \t D(G(z)) = 0.375 \t grad_pen = 2.042 \t t = 1.345 \t\n",
      "[2/10][272/782] G loss: -0.452 \t D loss: 1.497 \t D(x) = 0.168 \t D(G(z)) = 0.455 \t grad_pen = 1.210 \t t = 1.348 \t\n",
      "[2/10][288/782] G loss: -0.513 \t D loss: 0.106 \t D(x) = 0.736 \t D(G(z)) = 0.514 \t grad_pen = 0.328 \t t = 1.340 \t\n",
      "[2/10][304/782] G loss: -0.541 \t D loss: -0.008 \t D(x) = 0.853 \t D(G(z)) = 0.553 \t grad_pen = 0.291 \t t = 1.342 \t\n",
      "[2/10][320/782] G loss: -0.562 \t D loss: 0.099 \t D(x) = 0.658 \t D(G(z)) = 0.567 \t grad_pen = 0.190 \t t = 1.342 \t\n",
      "[2/10][336/782] G loss: -0.963 \t D loss: 1.913 \t D(x) = 0.578 \t D(G(z)) = 0.960 \t grad_pen = 1.531 \t t = 1.335 \t\n",
      "[2/10][352/782] G loss: -0.945 \t D loss: 1.151 \t D(x) = 0.467 \t D(G(z)) = 0.930 \t grad_pen = 0.688 \t t = 1.332 \t\n",
      "[2/10][368/782] G loss: -0.846 \t D loss: 0.615 \t D(x) = 0.465 \t D(G(z)) = 0.821 \t grad_pen = 0.259 \t t = 1.340 \t\n",
      "[2/10][384/782] G loss: -0.137 \t D loss: 4.022 \t D(x) = 0.120 \t D(G(z)) = 0.063 \t grad_pen = 4.079 \t t = 1.333 \t\n",
      "[2/10][400/782] G loss: -0.299 \t D loss: 8.424 \t D(x) = 0.001 \t D(G(z)) = 0.315 \t grad_pen = 8.110 \t t = 1.337 \t\n",
      "[2/10][416/782] G loss: -0.453 \t D loss: 2.127 \t D(x) = 0.139 \t D(G(z)) = 0.456 \t grad_pen = 1.810 \t t = 1.330 \t\n",
      "[2/10][432/782] G loss: -0.490 \t D loss: 1.389 \t D(x) = 0.419 \t D(G(z)) = 0.495 \t grad_pen = 1.314 \t t = 1.332 \t\n",
      "[2/10][448/782] G loss: -0.501 \t D loss: 1.209 \t D(x) = 0.435 \t D(G(z)) = 0.500 \t grad_pen = 1.143 \t t = 1.333 \t\n",
      "[2/10][464/782] G loss: -0.504 \t D loss: 1.224 \t D(x) = 0.526 \t D(G(z)) = 0.505 \t grad_pen = 1.246 \t t = 1.339 \t\n",
      "[2/10][480/782] G loss: -0.594 \t D loss: -0.016 \t D(x) = 0.749 \t D(G(z)) = 0.590 \t grad_pen = 0.144 \t t = 1.332 \t\n",
      "[2/10][496/782] G loss: -0.490 \t D loss: 0.655 \t D(x) = 0.249 \t D(G(z)) = 0.487 \t grad_pen = 0.417 \t t = 1.331 \t\n",
      "[2/10][512/782] G loss: -0.498 \t D loss: 1.038 \t D(x) = 0.393 \t D(G(z)) = 0.496 \t grad_pen = 0.935 \t t = 1.344 \t\n",
      "[2/10][528/782] G loss: -0.544 \t D loss: 0.075 \t D(x) = 0.840 \t D(G(z)) = 0.556 \t grad_pen = 0.360 \t t = 1.361 \t\n",
      "[2/10][544/782] G loss: -0.609 \t D loss: 0.076 \t D(x) = 0.727 \t D(G(z)) = 0.624 \t grad_pen = 0.179 \t t = 1.356 \t\n",
      "[2/10][560/782] G loss: -0.493 \t D loss: 0.757 \t D(x) = 0.209 \t D(G(z)) = 0.487 \t grad_pen = 0.479 \t t = 1.346 \t\n",
      "[2/10][576/782] G loss: -0.495 \t D loss: 0.911 \t D(x) = 0.332 \t D(G(z)) = 0.502 \t grad_pen = 0.741 \t t = 1.344 \t\n",
      "[2/10][592/782] G loss: -0.513 \t D loss: 1.339 \t D(x) = 0.129 \t D(G(z)) = 0.510 \t grad_pen = 0.958 \t t = 1.338 \t\n",
      "[2/10][608/782] G loss: -0.508 \t D loss: 1.135 \t D(x) = 0.298 \t D(G(z)) = 0.533 \t grad_pen = 0.900 \t t = 1.347 \t\n",
      "[2/10][624/782] G loss: -0.532 \t D loss: 0.935 \t D(x) = 0.224 \t D(G(z)) = 0.556 \t grad_pen = 0.602 \t t = 1.347 \t\n",
      "[2/10][640/782] G loss: -0.559 \t D loss: 0.838 \t D(x) = 0.226 \t D(G(z)) = 0.557 \t grad_pen = 0.507 \t t = 1.348 \t\n",
      "[2/10][656/782] G loss: -0.573 \t D loss: 0.814 \t D(x) = 0.268 \t D(G(z)) = 0.599 \t grad_pen = 0.482 \t t = 1.346 \t\n",
      "[2/10][672/782] G loss: -0.632 \t D loss: 0.587 \t D(x) = 0.370 \t D(G(z)) = 0.626 \t grad_pen = 0.330 \t t = 1.353 \t\n",
      "[2/10][688/782] G loss: -0.540 \t D loss: 0.576 \t D(x) = 0.314 \t D(G(z)) = 0.549 \t grad_pen = 0.341 \t t = 1.357 \t\n",
      "[2/10][704/782] G loss: -0.406 \t D loss: 0.551 \t D(x) = 0.161 \t D(G(z)) = 0.456 \t grad_pen = 0.255 \t t = 1.354 \t\n",
      "[2/10][720/782] G loss: -0.482 \t D loss: 0.432 \t D(x) = 0.303 \t D(G(z)) = 0.500 \t grad_pen = 0.235 \t t = 1.338 \t\n",
      "[2/10][736/782] G loss: -0.567 \t D loss: 0.362 \t D(x) = 0.467 \t D(G(z)) = 0.561 \t grad_pen = 0.268 \t t = 1.352 \t\n",
      "[2/10][752/782] G loss: -0.548 \t D loss: 0.441 \t D(x) = 0.572 \t D(G(z)) = 0.563 \t grad_pen = 0.451 \t t = 1.351 \t\n",
      "[2/10][768/782] G loss: -0.506 \t D loss: -0.025 \t D(x) = 0.661 \t D(G(z)) = 0.513 \t grad_pen = 0.123 \t t = 1.352 \t\n",
      "[3/10][0/782] G loss: -0.473 \t D loss: 0.028 \t D(x) = 0.658 \t D(G(z)) = 0.489 \t grad_pen = 0.197 \t t = 1.146 \t\n",
      "[3/10][16/782] G loss: -0.351 \t D loss: 0.522 \t D(x) = 0.282 \t D(G(z)) = 0.355 \t grad_pen = 0.449 \t t = 1.336 \t\n",
      "[3/10][32/782] G loss: -0.282 \t D loss: 5.398 \t D(x) = 0.013 \t D(G(z)) = 0.296 \t grad_pen = 5.116 \t t = 1.336 \t\n",
      "[3/10][48/782] G loss: -0.425 \t D loss: 5.983 \t D(x) = 0.066 \t D(G(z)) = 0.436 \t grad_pen = 5.613 \t t = 1.341 \t\n",
      "[3/10][64/782] G loss: -0.511 \t D loss: 0.912 \t D(x) = 0.858 \t D(G(z)) = 0.515 \t grad_pen = 1.255 \t t = 1.337 \t\n",
      "[3/10][80/782] G loss: -0.590 \t D loss: 0.851 \t D(x) = 0.728 \t D(G(z)) = 0.624 \t grad_pen = 0.955 \t t = 1.344 \t\n",
      "[3/10][96/782] G loss: -0.864 \t D loss: 0.611 \t D(x) = 0.612 \t D(G(z)) = 0.855 \t grad_pen = 0.369 \t t = 1.339 \t\n",
      "[3/10][112/782] G loss: -0.513 \t D loss: -0.035 \t D(x) = 0.694 \t D(G(z)) = 0.507 \t grad_pen = 0.152 \t t = 1.344 \t\n",
      "[3/10][128/782] G loss: -0.571 \t D loss: 0.031 \t D(x) = 0.762 \t D(G(z)) = 0.519 \t grad_pen = 0.273 \t t = 1.332 \t\n",
      "[3/10][144/782] G loss: -0.600 \t D loss: -0.031 \t D(x) = 0.767 \t D(G(z)) = 0.627 \t grad_pen = 0.109 \t t = 1.366 \t\n",
      "[3/10][160/782] G loss: -0.460 \t D loss: -0.102 \t D(x) = 0.660 \t D(G(z)) = 0.455 \t grad_pen = 0.103 \t t = 1.340 \t\n",
      "[3/10][176/782] G loss: -0.242 \t D loss: 4.547 \t D(x) = 0.070 \t D(G(z)) = 0.247 \t grad_pen = 4.370 \t t = 1.331 \t\n",
      "[3/10][192/782] G loss: -0.363 \t D loss: 7.575 \t D(x) = 0.005 \t D(G(z)) = 0.363 \t grad_pen = 7.217 \t t = 1.331 \t\n",
      "[3/10][208/782] G loss: -0.489 \t D loss: 1.557 \t D(x) = 0.350 \t D(G(z)) = 0.490 \t grad_pen = 1.418 \t t = 1.333 \t\n",
      "[3/10][224/782] G loss: -0.479 \t D loss: 5.801 \t D(x) = 0.034 \t D(G(z)) = 0.455 \t grad_pen = 5.380 \t t = 1.337 \t\n",
      "[3/10][240/782] G loss: -0.559 \t D loss: 1.898 \t D(x) = 0.824 \t D(G(z)) = 0.570 \t grad_pen = 2.152 \t t = 1.352 \t\n",
      "[3/10][256/782] G loss: -0.750 \t D loss: 0.889 \t D(x) = 0.782 \t D(G(z)) = 0.770 \t grad_pen = 0.900 \t t = 1.336 \t\n",
      "[3/10][272/782] G loss: -0.794 \t D loss: 0.576 \t D(x) = 0.673 \t D(G(z)) = 0.837 \t grad_pen = 0.412 \t t = 1.334 \t\n",
      "[3/10][288/782] G loss: -0.565 \t D loss: -0.032 \t D(x) = 0.766 \t D(G(z)) = 0.550 \t grad_pen = 0.184 \t t = 1.336 \t\n",
      "[3/10][304/782] G loss: -0.586 \t D loss: 0.085 \t D(x) = 0.655 \t D(G(z)) = 0.578 \t grad_pen = 0.161 \t t = 1.333 \t\n",
      "[3/10][320/782] G loss: -0.441 \t D loss: -0.082 \t D(x) = 0.578 \t D(G(z)) = 0.422 \t grad_pen = 0.075 \t t = 1.335 \t\n",
      "[3/10][336/782] G loss: -0.218 \t D loss: 5.564 \t D(x) = 0.006 \t D(G(z)) = 0.187 \t grad_pen = 5.383 \t t = 1.334 \t\n",
      "[3/10][352/782] G loss: -0.363 \t D loss: 6.920 \t D(x) = 0.008 \t D(G(z)) = 0.363 \t grad_pen = 6.565 \t t = 1.339 \t\n",
      "[3/10][368/782] G loss: -0.467 \t D loss: 3.581 \t D(x) = 0.057 \t D(G(z)) = 0.464 \t grad_pen = 3.174 \t t = 1.347 \t\n",
      "[3/10][384/782] G loss: -0.523 \t D loss: 0.630 \t D(x) = 0.490 \t D(G(z)) = 0.524 \t grad_pen = 0.596 \t t = 1.340 \t\n",
      "[3/10][400/782] G loss: -0.787 \t D loss: 0.974 \t D(x) = 0.623 \t D(G(z)) = 0.816 \t grad_pen = 0.781 \t t = 1.349 \t\n",
      "[3/10][416/782] G loss: -0.868 \t D loss: 0.390 \t D(x) = 0.751 \t D(G(z)) = 0.849 \t grad_pen = 0.292 \t t = 1.361 \t\n",
      "[3/10][432/782] G loss: -0.700 \t D loss: 0.674 \t D(x) = 0.588 \t D(G(z)) = 0.734 \t grad_pen = 0.528 \t t = 1.340 \t\n",
      "[3/10][448/782] G loss: -0.192 \t D loss: 1.500 \t D(x) = 0.173 \t D(G(z)) = 0.210 \t grad_pen = 1.463 \t t = 1.353 \t\n",
      "[3/10][464/782] G loss: -0.337 \t D loss: 1.770 \t D(x) = 0.096 \t D(G(z)) = 0.329 \t grad_pen = 1.538 \t t = 1.353 \t\n",
      "[3/10][480/782] G loss: -0.503 \t D loss: 0.308 \t D(x) = 0.610 \t D(G(z)) = 0.506 \t grad_pen = 0.412 \t t = 1.347 \t\n",
      "[3/10][496/782] G loss: -0.588 \t D loss: 0.593 \t D(x) = 0.766 \t D(G(z)) = 0.658 \t grad_pen = 0.701 \t t = 1.341 \t\n",
      "[3/10][512/782] G loss: -0.828 \t D loss: 0.566 \t D(x) = 0.596 \t D(G(z)) = 0.869 \t grad_pen = 0.293 \t t = 1.340 \t\n",
      "[3/10][528/782] G loss: -0.502 \t D loss: 0.844 \t D(x) = 0.480 \t D(G(z)) = 0.508 \t grad_pen = 0.816 \t t = 1.341 \t\n",
      "[3/10][544/782] G loss: -0.429 \t D loss: 0.565 \t D(x) = 0.120 \t D(G(z)) = 0.409 \t grad_pen = 0.276 \t t = 1.344 \t\n",
      "[3/10][560/782] G loss: -0.463 \t D loss: 0.613 \t D(x) = 0.261 \t D(G(z)) = 0.470 \t grad_pen = 0.404 \t t = 1.341 \t\n",
      "[3/10][576/782] G loss: -0.519 \t D loss: 0.455 \t D(x) = 0.582 \t D(G(z)) = 0.526 \t grad_pen = 0.511 \t t = 1.334 \t\n",
      "[3/10][592/782] G loss: -0.697 \t D loss: 0.381 \t D(x) = 0.677 \t D(G(z)) = 0.674 \t grad_pen = 0.384 \t t = 1.348 \t\n",
      "[3/10][608/782] G loss: -0.481 \t D loss: 0.544 \t D(x) = 0.183 \t D(G(z)) = 0.489 \t grad_pen = 0.238 \t t = 1.341 \t\n",
      "[3/10][624/782] G loss: -0.522 \t D loss: 0.925 \t D(x) = 0.502 \t D(G(z)) = 0.544 \t grad_pen = 0.883 \t t = 1.343 \t\n",
      "[3/10][640/782] G loss: -0.502 \t D loss: 0.606 \t D(x) = 0.189 \t D(G(z)) = 0.506 \t grad_pen = 0.289 \t t = 1.349 \t\n",
      "[3/10][656/782] G loss: -0.573 \t D loss: 0.813 \t D(x) = 0.230 \t D(G(z)) = 0.558 \t grad_pen = 0.486 \t t = 1.363 \t\n",
      "[3/10][672/782] G loss: -0.570 \t D loss: 0.393 \t D(x) = 0.458 \t D(G(z)) = 0.575 \t grad_pen = 0.276 \t t = 1.350 \t\n",
      "[3/10][688/782] G loss: -0.252 \t D loss: 1.613 \t D(x) = 0.145 \t D(G(z)) = 0.288 \t grad_pen = 1.470 \t t = 1.347 \t\n",
      "[3/10][704/782] G loss: -0.440 \t D loss: 0.411 \t D(x) = 0.290 \t D(G(z)) = 0.448 \t grad_pen = 0.252 \t t = 1.347 \t\n",
      "[3/10][720/782] G loss: -0.513 \t D loss: 0.521 \t D(x) = 0.441 \t D(G(z)) = 0.520 \t grad_pen = 0.442 \t t = 1.349 \t\n",
      "[3/10][736/782] G loss: -0.529 \t D loss: 0.314 \t D(x) = 0.564 \t D(G(z)) = 0.536 \t grad_pen = 0.342 \t t = 1.349 \t\n",
      "[3/10][752/782] G loss: -0.782 \t D loss: 0.315 \t D(x) = 0.805 \t D(G(z)) = 0.738 \t grad_pen = 0.382 \t t = 1.351 \t\n",
      "[3/10][768/782] G loss: -0.495 \t D loss: 0.005 \t D(x) = 0.604 \t D(G(z)) = 0.505 \t grad_pen = 0.104 \t t = 1.344 \t\n",
      "[4/10][0/782] G loss: -0.476 \t D loss: 0.148 \t D(x) = 0.499 \t D(G(z)) = 0.479 \t grad_pen = 0.168 \t t = 1.155 \t\n",
      "[4/10][16/782] G loss: -0.484 \t D loss: 0.137 \t D(x) = 0.619 \t D(G(z)) = 0.490 \t grad_pen = 0.266 \t t = 1.359 \t\n",
      "[4/10][32/782] G loss: -0.504 \t D loss: 0.048 \t D(x) = 0.639 \t D(G(z)) = 0.510 \t grad_pen = 0.176 \t t = 1.356 \t\n",
      "[4/10][48/782] G loss: -0.551 \t D loss: 0.153 \t D(x) = 0.595 \t D(G(z)) = 0.549 \t grad_pen = 0.199 \t t = 1.350 \t\n",
      "[4/10][64/782] G loss: -0.533 \t D loss: 0.260 \t D(x) = 0.466 \t D(G(z)) = 0.486 \t grad_pen = 0.241 \t t = 1.377 \t\n",
      "[4/10][80/782] G loss: -0.436 \t D loss: 0.140 \t D(x) = 0.469 \t D(G(z)) = 0.425 \t grad_pen = 0.184 \t t = 1.358 \t\n",
      "[4/10][96/782] G loss: -0.473 \t D loss: -0.019 \t D(x) = 0.656 \t D(G(z)) = 0.496 \t grad_pen = 0.142 \t t = 1.368 \t\n",
      "[4/10][112/782] G loss: -0.542 \t D loss: -0.034 \t D(x) = 0.645 \t D(G(z)) = 0.500 \t grad_pen = 0.111 \t t = 1.367 \t\n",
      "[4/10][128/782] G loss: -0.547 \t D loss: -0.079 \t D(x) = 0.706 \t D(G(z)) = 0.540 \t grad_pen = 0.087 \t t = 1.352 \t\n",
      "[4/10][144/782] G loss: -0.526 \t D loss: -0.026 \t D(x) = 0.683 \t D(G(z)) = 0.506 \t grad_pen = 0.150 \t t = 1.354 \t\n",
      "[4/10][160/782] G loss: -0.531 \t D loss: -0.095 \t D(x) = 0.740 \t D(G(z)) = 0.564 \t grad_pen = 0.080 \t t = 1.349 \t\n",
      "[4/10][176/782] G loss: -0.517 \t D loss: -0.055 \t D(x) = 0.753 \t D(G(z)) = 0.547 \t grad_pen = 0.150 \t t = 1.340 \t\n",
      "[4/10][192/782] G loss: -0.469 \t D loss: -0.019 \t D(x) = 0.666 \t D(G(z)) = 0.470 \t grad_pen = 0.177 \t t = 1.341 \t\n",
      "[4/10][208/782] G loss: -0.446 \t D loss: -0.070 \t D(x) = 0.671 \t D(G(z)) = 0.466 \t grad_pen = 0.135 \t t = 1.344 \t\n",
      "[4/10][224/782] G loss: -0.513 \t D loss: 0.054 \t D(x) = 0.603 \t D(G(z)) = 0.462 \t grad_pen = 0.196 \t t = 1.348 \t\n",
      "[4/10][240/782] G loss: -0.545 \t D loss: 0.042 \t D(x) = 0.597 \t D(G(z)) = 0.474 \t grad_pen = 0.165 \t t = 1.340 \t\n",
      "[4/10][256/782] G loss: -0.467 \t D loss: -0.046 \t D(x) = 0.670 \t D(G(z)) = 0.503 \t grad_pen = 0.121 \t t = 1.343 \t\n",
      "[4/10][272/782] G loss: -0.499 \t D loss: -0.080 \t D(x) = 0.647 \t D(G(z)) = 0.475 \t grad_pen = 0.091 \t t = 1.349 \t\n",
      "[4/10][288/782] G loss: -0.519 \t D loss: -0.120 \t D(x) = 0.738 \t D(G(z)) = 0.526 \t grad_pen = 0.092 \t t = 1.350 \t\n",
      "[4/10][304/782] G loss: -0.512 \t D loss: -0.078 \t D(x) = 0.697 \t D(G(z)) = 0.511 \t grad_pen = 0.109 \t t = 1.362 \t\n",
      "[4/10][320/782] G loss: -0.511 \t D loss: -0.113 \t D(x) = 0.710 \t D(G(z)) = 0.513 \t grad_pen = 0.084 \t t = 1.363 \t\n",
      "[4/10][336/782] G loss: -0.535 \t D loss: -0.112 \t D(x) = 0.696 \t D(G(z)) = 0.510 \t grad_pen = 0.075 \t t = 1.351 \t\n",
      "[4/10][352/782] G loss: -0.487 \t D loss: -0.078 \t D(x) = 0.695 \t D(G(z)) = 0.515 \t grad_pen = 0.102 \t t = 1.358 \t\n",
      "[4/10][368/782] G loss: -0.477 \t D loss: -0.122 \t D(x) = 0.673 \t D(G(z)) = 0.486 \t grad_pen = 0.065 \t t = 1.354 \t\n",
      "[4/10][384/782] G loss: -0.519 \t D loss: -0.042 \t D(x) = 0.705 \t D(G(z)) = 0.507 \t grad_pen = 0.155 \t t = 1.353 \t\n",
      "[4/10][400/782] G loss: -0.552 \t D loss: -0.021 \t D(x) = 0.697 \t D(G(z)) = 0.540 \t grad_pen = 0.136 \t t = 1.336 \t\n",
      "[4/10][416/782] G loss: -0.541 \t D loss: 0.005 \t D(x) = 0.711 \t D(G(z)) = 0.562 \t grad_pen = 0.154 \t t = 1.338 \t\n",
      "[4/10][432/782] G loss: -0.551 \t D loss: -0.040 \t D(x) = 0.630 \t D(G(z)) = 0.474 \t grad_pen = 0.116 \t t = 1.333 \t\n",
      "[4/10][448/782] G loss: -0.564 \t D loss: 0.008 \t D(x) = 0.634 \t D(G(z)) = 0.478 \t grad_pen = 0.164 \t t = 1.334 \t\n",
      "[4/10][464/782] G loss: -0.545 \t D loss: -0.069 \t D(x) = 0.674 \t D(G(z)) = 0.499 \t grad_pen = 0.107 \t t = 1.356 \t\n",
      "[4/10][480/782] G loss: -0.551 \t D loss: 0.069 \t D(x) = 0.589 \t D(G(z)) = 0.465 \t grad_pen = 0.193 \t t = 1.345 \t\n",
      "[4/10][496/782] G loss: -0.524 \t D loss: -0.102 \t D(x) = 0.701 \t D(G(z)) = 0.521 \t grad_pen = 0.077 \t t = 1.359 \t\n",
      "[4/10][512/782] G loss: -0.472 \t D loss: 0.014 \t D(x) = 0.618 \t D(G(z)) = 0.495 \t grad_pen = 0.137 \t t = 1.361 \t\n",
      "[4/10][528/782] G loss: -0.488 \t D loss: -0.153 \t D(x) = 0.746 \t D(G(z)) = 0.509 \t grad_pen = 0.084 \t t = 1.336 \t\n",
      "[4/10][544/782] G loss: -0.518 \t D loss: -0.066 \t D(x) = 0.674 \t D(G(z)) = 0.489 \t grad_pen = 0.119 \t t = 1.346 \t\n",
      "[4/10][560/782] G loss: -0.518 \t D loss: -0.102 \t D(x) = 0.654 \t D(G(z)) = 0.497 \t grad_pen = 0.055 \t t = 1.340 \t\n",
      "[4/10][576/782] G loss: -0.482 \t D loss: -0.122 \t D(x) = 0.698 \t D(G(z)) = 0.513 \t grad_pen = 0.062 \t t = 1.347 \t\n",
      "[4/10][592/782] G loss: -0.476 \t D loss: -0.081 \t D(x) = 0.646 \t D(G(z)) = 0.472 \t grad_pen = 0.092 \t t = 1.350 \t\n",
      "[4/10][608/782] G loss: -0.498 \t D loss: -0.129 \t D(x) = 0.796 \t D(G(z)) = 0.531 \t grad_pen = 0.136 \t t = 1.342 \t\n",
      "[4/10][624/782] G loss: -0.456 \t D loss: -0.096 \t D(x) = 0.651 \t D(G(z)) = 0.462 \t grad_pen = 0.094 \t t = 1.359 \t\n",
      "[4/10][640/782] G loss: -0.500 \t D loss: -0.097 \t D(x) = 0.736 \t D(G(z)) = 0.517 \t grad_pen = 0.121 \t t = 1.349 \t\n",
      "[4/10][656/782] G loss: -0.493 \t D loss: -0.148 \t D(x) = 0.744 \t D(G(z)) = 0.513 \t grad_pen = 0.082 \t t = 1.341 \t\n",
      "[4/10][672/782] G loss: -0.458 \t D loss: -0.137 \t D(x) = 0.743 \t D(G(z)) = 0.515 \t grad_pen = 0.091 \t t = 1.361 \t\n",
      "[4/10][688/782] G loss: -0.551 \t D loss: -0.085 \t D(x) = 0.652 \t D(G(z)) = 0.504 \t grad_pen = 0.063 \t t = 1.345 \t\n",
      "[4/10][704/782] G loss: -0.471 \t D loss: -0.120 \t D(x) = 0.694 \t D(G(z)) = 0.479 \t grad_pen = 0.096 \t t = 1.354 \t\n",
      "[4/10][720/782] G loss: -0.542 \t D loss: -0.064 \t D(x) = 0.643 \t D(G(z)) = 0.490 \t grad_pen = 0.089 \t t = 1.354 \t\n",
      "[4/10][736/782] G loss: -0.529 \t D loss: -0.113 \t D(x) = 0.729 \t D(G(z)) = 0.536 \t grad_pen = 0.080 \t t = 1.343 \t\n",
      "[4/10][752/782] G loss: -0.508 \t D loss: -0.119 \t D(x) = 0.723 \t D(G(z)) = 0.520 \t grad_pen = 0.085 \t t = 1.353 \t\n",
      "[4/10][768/782] G loss: -0.506 \t D loss: -0.184 \t D(x) = 0.761 \t D(G(z)) = 0.512 \t grad_pen = 0.064 \t t = 1.340 \t\n",
      "[5/10][0/782] G loss: -0.475 \t D loss: -0.143 \t D(x) = 0.772 \t D(G(z)) = 0.516 \t grad_pen = 0.113 \t t = 1.141 \t\n",
      "[5/10][16/782] G loss: -0.488 \t D loss: -0.109 \t D(x) = 0.734 \t D(G(z)) = 0.518 \t grad_pen = 0.107 \t t = 1.350 \t\n",
      "[5/10][32/782] G loss: -0.452 \t D loss: -0.126 \t D(x) = 0.675 \t D(G(z)) = 0.472 \t grad_pen = 0.078 \t t = 1.342 \t\n",
      "[5/10][48/782] G loss: -0.493 \t D loss: -0.153 \t D(x) = 0.762 \t D(G(z)) = 0.520 \t grad_pen = 0.089 \t t = 1.338 \t\n",
      "[5/10][64/782] G loss: -0.528 \t D loss: -0.147 \t D(x) = 0.744 \t D(G(z)) = 0.518 \t grad_pen = 0.079 \t t = 1.337 \t\n",
      "[5/10][80/782] G loss: -0.487 \t D loss: -0.148 \t D(x) = 0.750 \t D(G(z)) = 0.480 \t grad_pen = 0.122 \t t = 1.342 \t\n",
      "[5/10][96/782] G loss: -0.461 \t D loss: -0.092 \t D(x) = 0.721 \t D(G(z)) = 0.483 \t grad_pen = 0.146 \t t = 1.338 \t\n",
      "[5/10][112/782] G loss: -0.509 \t D loss: -0.090 \t D(x) = 0.663 \t D(G(z)) = 0.484 \t grad_pen = 0.089 \t t = 1.335 \t\n",
      "[5/10][128/782] G loss: -0.936 \t D loss: 0.882 \t D(x) = 0.830 \t D(G(z)) = 0.885 \t grad_pen = 0.828 \t t = 1.335 \t\n",
      "[5/10][144/782] G loss: -0.899 \t D loss: 0.798 \t D(x) = 0.787 \t D(G(z)) = 0.916 \t grad_pen = 0.669 \t t = 1.358 \t\n",
      "[5/10][160/782] G loss: -0.913 \t D loss: 0.684 \t D(x) = 0.788 \t D(G(z)) = 0.888 \t grad_pen = 0.585 \t t = 1.353 \t\n",
      "[5/10][176/782] G loss: -0.945 \t D loss: 0.579 \t D(x) = 0.756 \t D(G(z)) = 0.940 \t grad_pen = 0.395 \t t = 1.351 \t\n",
      "[5/10][192/782] G loss: -0.942 \t D loss: 0.426 \t D(x) = 0.753 \t D(G(z)) = 0.945 \t grad_pen = 0.235 \t t = 1.354 \t\n",
      "[5/10][208/782] G loss: -0.900 \t D loss: 0.410 \t D(x) = 0.777 \t D(G(z)) = 0.878 \t grad_pen = 0.309 \t t = 1.351 \t\n",
      "[5/10][224/782] G loss: -0.831 \t D loss: 0.315 \t D(x) = 0.751 \t D(G(z)) = 0.844 \t grad_pen = 0.223 \t t = 1.365 \t\n",
      "[5/10][240/782] G loss: -0.905 \t D loss: 0.388 \t D(x) = 0.830 \t D(G(z)) = 0.939 \t grad_pen = 0.279 \t t = 1.360 \t\n",
      "[5/10][256/782] G loss: -0.905 \t D loss: 0.290 \t D(x) = 0.785 \t D(G(z)) = 0.909 \t grad_pen = 0.166 \t t = 1.357 \t\n",
      "[5/10][272/782] G loss: -0.940 \t D loss: 0.331 \t D(x) = 0.765 \t D(G(z)) = 0.934 \t grad_pen = 0.162 \t t = 1.354 \t\n",
      "[5/10][288/782] G loss: -0.901 \t D loss: 0.318 \t D(x) = 0.801 \t D(G(z)) = 0.906 \t grad_pen = 0.213 \t t = 1.335 \t\n",
      "[5/10][304/782] G loss: -0.918 \t D loss: 0.323 \t D(x) = 0.786 \t D(G(z)) = 0.905 \t grad_pen = 0.204 \t t = 1.342 \t\n",
      "[5/10][320/782] G loss: -0.902 \t D loss: 0.343 \t D(x) = 0.763 \t D(G(z)) = 0.884 \t grad_pen = 0.223 \t t = 1.339 \t\n",
      "[5/10][336/782] G loss: -0.891 \t D loss: 0.261 \t D(x) = 0.819 \t D(G(z)) = 0.898 \t grad_pen = 0.182 \t t = 1.351 \t\n",
      "[5/10][352/782] G loss: -0.905 \t D loss: 0.256 \t D(x) = 0.770 \t D(G(z)) = 0.884 \t grad_pen = 0.142 \t t = 1.339 \t\n",
      "[5/10][368/782] G loss: -0.868 \t D loss: 0.222 \t D(x) = 0.756 \t D(G(z)) = 0.876 \t grad_pen = 0.103 \t t = 1.342 \t\n",
      "[5/10][384/782] G loss: -0.828 \t D loss: 0.177 \t D(x) = 0.769 \t D(G(z)) = 0.831 \t grad_pen = 0.115 \t t = 1.349 \t\n",
      "[5/10][400/782] G loss: -0.620 \t D loss: 0.018 \t D(x) = 0.749 \t D(G(z)) = 0.625 \t grad_pen = 0.142 \t t = 1.358 \t\n",
      "[5/10][416/782] G loss: -0.427 \t D loss: -0.168 \t D(x) = 0.720 \t D(G(z)) = 0.442 \t grad_pen = 0.110 \t t = 1.363 \t\n",
      "[5/10][432/782] G loss: -0.031 \t D loss: 0.230 \t D(x) = 0.248 \t D(G(z)) = 0.041 \t grad_pen = 0.436 \t t = 1.364 \t\n",
      "[5/10][448/782] G loss: -0.085 \t D loss: 1.407 \t D(x) = 0.083 \t D(G(z)) = 0.051 \t grad_pen = 1.439 \t t = 1.354 \t\n",
      "[5/10][464/782] G loss: -0.124 \t D loss: 1.714 \t D(x) = 0.070 \t D(G(z)) = 0.117 \t grad_pen = 1.667 \t t = 1.333 \t\n",
      "[5/10][480/782] G loss: -0.226 \t D loss: 3.683 \t D(x) = 0.057 \t D(G(z)) = 0.271 \t grad_pen = 3.469 \t t = 1.341 \t\n",
      "[5/10][496/782] G loss: -0.473 \t D loss: 0.602 \t D(x) = 0.494 \t D(G(z)) = 0.477 \t grad_pen = 0.619 \t t = 1.340 \t\n",
      "[5/10][512/782] G loss: -0.505 \t D loss: 0.115 \t D(x) = 0.661 \t D(G(z)) = 0.513 \t grad_pen = 0.263 \t t = 1.341 \t\n",
      "[5/10][528/782] G loss: -0.677 \t D loss: 0.421 \t D(x) = 0.874 \t D(G(z)) = 0.745 \t grad_pen = 0.550 \t t = 1.338 \t\n",
      "[5/10][544/782] G loss: -0.516 \t D loss: 0.012 \t D(x) = 0.631 \t D(G(z)) = 0.517 \t grad_pen = 0.126 \t t = 1.350 \t\n",
      "[5/10][560/782] G loss: -0.504 \t D loss: -0.158 \t D(x) = 0.785 \t D(G(z)) = 0.516 \t grad_pen = 0.110 \t t = 1.347 \t\n",
      "[5/10][576/782] G loss: -0.515 \t D loss: -0.087 \t D(x) = 0.799 \t D(G(z)) = 0.548 \t grad_pen = 0.165 \t t = 1.347 \t\n",
      "[5/10][592/782] G loss: -0.499 \t D loss: -0.132 \t D(x) = 0.692 \t D(G(z)) = 0.486 \t grad_pen = 0.073 \t t = 1.354 \t\n",
      "[5/10][608/782] G loss: -0.468 \t D loss: -0.162 \t D(x) = 0.737 \t D(G(z)) = 0.490 \t grad_pen = 0.085 \t t = 1.354 \t\n",
      "[5/10][624/782] G loss: -0.514 \t D loss: -0.084 \t D(x) = 0.748 \t D(G(z)) = 0.536 \t grad_pen = 0.128 \t t = 1.353 \t\n",
      "[5/10][640/782] G loss: -0.497 \t D loss: -0.121 \t D(x) = 0.720 \t D(G(z)) = 0.506 \t grad_pen = 0.093 \t t = 1.347 \t\n",
      "[5/10][656/782] G loss: -0.482 \t D loss: -0.135 \t D(x) = 0.713 \t D(G(z)) = 0.491 \t grad_pen = 0.087 \t t = 1.347 \t\n",
      "[5/10][672/782] G loss: -0.516 \t D loss: -0.065 \t D(x) = 0.658 \t D(G(z)) = 0.471 \t grad_pen = 0.121 \t t = 1.355 \t\n",
      "[5/10][688/782] G loss: -0.507 \t D loss: -0.158 \t D(x) = 0.754 \t D(G(z)) = 0.504 \t grad_pen = 0.093 \t t = 1.342 \t\n",
      "[5/10][704/782] G loss: -0.492 \t D loss: -0.091 \t D(x) = 0.732 \t D(G(z)) = 0.524 \t grad_pen = 0.118 \t t = 1.344 \t\n",
      "[5/10][720/782] G loss: -0.491 \t D loss: 0.039 \t D(x) = 0.549 \t D(G(z)) = 0.407 \t grad_pen = 0.181 \t t = 1.339 \t\n",
      "[5/10][736/782] G loss: -0.482 \t D loss: -0.116 \t D(x) = 0.695 \t D(G(z)) = 0.466 \t grad_pen = 0.113 \t t = 1.346 \t\n",
      "[5/10][752/782] G loss: -0.495 \t D loss: -0.161 \t D(x) = 0.722 \t D(G(z)) = 0.486 \t grad_pen = 0.075 \t t = 1.350 \t\n",
      "[5/10][768/782] G loss: -0.474 \t D loss: -0.178 \t D(x) = 0.776 \t D(G(z)) = 0.515 \t grad_pen = 0.083 \t t = 1.341 \t\n",
      "[6/10][0/782] G loss: -0.429 \t D loss: -0.167 \t D(x) = 0.768 \t D(G(z)) = 0.474 \t grad_pen = 0.127 \t t = 1.147 \t\n",
      "[6/10][16/782] G loss: -0.485 \t D loss: -0.114 \t D(x) = 0.693 \t D(G(z)) = 0.466 \t grad_pen = 0.113 \t t = 1.348 \t\n",
      "[6/10][32/782] G loss: -0.475 \t D loss: -0.150 \t D(x) = 0.728 \t D(G(z)) = 0.504 \t grad_pen = 0.075 \t t = 1.337 \t\n",
      "[6/10][48/782] G loss: -0.511 \t D loss: -0.170 \t D(x) = 0.774 \t D(G(z)) = 0.525 \t grad_pen = 0.078 \t t = 1.336 \t\n",
      "[6/10][64/782] G loss: -0.525 \t D loss: -0.067 \t D(x) = 0.665 \t D(G(z)) = 0.489 \t grad_pen = 0.110 \t t = 1.337 \t\n",
      "[6/10][80/782] G loss: -0.484 \t D loss: -0.163 \t D(x) = 0.766 \t D(G(z)) = 0.510 \t grad_pen = 0.093 \t t = 1.342 \t\n",
      "[6/10][96/782] G loss: -0.534 \t D loss: -0.142 \t D(x) = 0.723 \t D(G(z)) = 0.524 \t grad_pen = 0.057 \t t = 1.338 \t\n",
      "[6/10][112/782] G loss: -0.505 \t D loss: -0.145 \t D(x) = 0.725 \t D(G(z)) = 0.523 \t grad_pen = 0.057 \t t = 1.352 \t\n",
      "[6/10][128/782] G loss: -0.504 \t D loss: -0.126 \t D(x) = 0.697 \t D(G(z)) = 0.491 \t grad_pen = 0.080 \t t = 1.355 \t\n",
      "[6/10][144/782] G loss: -0.460 \t D loss: -0.186 \t D(x) = 0.746 \t D(G(z)) = 0.511 \t grad_pen = 0.049 \t t = 1.356 \t\n",
      "[6/10][160/782] G loss: -0.509 \t D loss: -0.090 \t D(x) = 0.762 \t D(G(z)) = 0.537 \t grad_pen = 0.135 \t t = 1.341 \t\n",
      "[6/10][176/782] G loss: -0.468 \t D loss: -0.120 \t D(x) = 0.708 \t D(G(z)) = 0.492 \t grad_pen = 0.096 \t t = 1.349 \t\n",
      "[6/10][192/782] G loss: -0.499 \t D loss: -0.111 \t D(x) = 0.709 \t D(G(z)) = 0.506 \t grad_pen = 0.092 \t t = 1.348 \t\n",
      "[6/10][208/782] G loss: -0.476 \t D loss: -0.141 \t D(x) = 0.762 \t D(G(z)) = 0.513 \t grad_pen = 0.107 \t t = 1.363 \t\n",
      "[6/10][224/782] G loss: -0.539 \t D loss: -0.074 \t D(x) = 0.616 \t D(G(z)) = 0.456 \t grad_pen = 0.086 \t t = 1.355 \t\n",
      "[6/10][240/782] G loss: -0.488 \t D loss: -0.158 \t D(x) = 0.750 \t D(G(z)) = 0.522 \t grad_pen = 0.070 \t t = 1.350 \t\n",
      "[6/10][256/782] G loss: -0.515 \t D loss: -0.138 \t D(x) = 0.718 \t D(G(z)) = 0.508 \t grad_pen = 0.073 \t t = 1.343 \t\n",
      "[6/10][272/782] G loss: -0.473 \t D loss: -0.177 \t D(x) = 0.742 \t D(G(z)) = 0.499 \t grad_pen = 0.065 \t t = 1.351 \t\n",
      "[6/10][288/782] G loss: -0.496 \t D loss: -0.076 \t D(x) = 0.793 \t D(G(z)) = 0.556 \t grad_pen = 0.161 \t t = 1.348 \t\n",
      "[6/10][304/782] G loss: -0.462 \t D loss: -0.140 \t D(x) = 0.780 \t D(G(z)) = 0.518 \t grad_pen = 0.122 \t t = 1.351 \t\n",
      "[6/10][320/782] G loss: -0.482 \t D loss: -0.174 \t D(x) = 0.704 \t D(G(z)) = 0.484 \t grad_pen = 0.046 \t t = 1.345 \t\n",
      "[6/10][336/782] G loss: -0.478 \t D loss: -0.150 \t D(x) = 0.736 \t D(G(z)) = 0.496 \t grad_pen = 0.090 \t t = 1.338 \t\n",
      "[6/10][352/782] G loss: -0.507 \t D loss: -0.084 \t D(x) = 0.801 \t D(G(z)) = 0.584 \t grad_pen = 0.133 \t t = 1.338 \t\n",
      "[6/10][368/782] G loss: -0.522 \t D loss: -0.079 \t D(x) = 0.638 \t D(G(z)) = 0.476 \t grad_pen = 0.084 \t t = 1.356 \t\n",
      "[6/10][384/782] G loss: -0.534 \t D loss: -0.152 \t D(x) = 0.744 \t D(G(z)) = 0.504 \t grad_pen = 0.089 \t t = 1.363 \t\n",
      "[6/10][400/782] G loss: -0.508 \t D loss: -0.202 \t D(x) = 0.769 \t D(G(z)) = 0.501 \t grad_pen = 0.066 \t t = 1.364 \t\n",
      "[6/10][416/782] G loss: -0.481 \t D loss: -0.149 \t D(x) = 0.680 \t D(G(z)) = 0.466 \t grad_pen = 0.065 \t t = 1.360 \t\n",
      "[6/10][432/782] G loss: -0.499 \t D loss: -0.094 \t D(x) = 0.666 \t D(G(z)) = 0.473 \t grad_pen = 0.098 \t t = 1.363 \t\n",
      "[6/10][448/782] G loss: -0.515 \t D loss: -0.100 \t D(x) = 0.632 \t D(G(z)) = 0.438 \t grad_pen = 0.093 \t t = 1.363 \t\n",
      "[6/10][464/782] G loss: -0.454 \t D loss: -0.205 \t D(x) = 0.757 \t D(G(z)) = 0.463 \t grad_pen = 0.089 \t t = 1.360 \t\n",
      "[6/10][480/782] G loss: -0.482 \t D loss: -0.221 \t D(x) = 0.737 \t D(G(z)) = 0.444 \t grad_pen = 0.073 \t t = 1.362 \t\n",
      "[6/10][496/782] G loss: -0.267 \t D loss: 4.654 \t D(x) = 0.039 \t D(G(z)) = 0.297 \t grad_pen = 4.395 \t t = 1.348 \t\n",
      "[6/10][512/782] G loss: -0.392 \t D loss: 7.630 \t D(x) = 0.031 \t D(G(z)) = 0.393 \t grad_pen = 7.269 \t t = 1.359 \t\n",
      "[6/10][528/782] G loss: -0.506 \t D loss: 0.083 \t D(x) = 0.668 \t D(G(z)) = 0.507 \t grad_pen = 0.244 \t t = 1.355 \t\n",
      "[6/10][544/782] G loss: -0.937 \t D loss: 4.260 \t D(x) = 0.991 \t D(G(z)) = 0.909 \t grad_pen = 4.343 \t t = 1.352 \t\n",
      "[6/10][560/782] G loss: -0.853 \t D loss: 0.489 \t D(x) = 0.753 \t D(G(z)) = 0.769 \t grad_pen = 0.473 \t t = 1.350 \t\n",
      "[6/10][576/782] G loss: -0.430 \t D loss: -0.110 \t D(x) = 0.634 \t D(G(z)) = 0.442 \t grad_pen = 0.082 \t t = 1.350 \t\n",
      "[6/10][592/782] G loss: -0.239 \t D loss: 2.155 \t D(x) = 0.119 \t D(G(z)) = 0.217 \t grad_pen = 2.057 \t t = 1.356 \t\n",
      "[6/10][608/782] G loss: -0.486 \t D loss: 0.762 \t D(x) = 0.596 \t D(G(z)) = 0.490 \t grad_pen = 0.869 \t t = 1.346 \t\n",
      "[6/10][624/782] G loss: -0.857 \t D loss: 3.354 \t D(x) = 0.946 \t D(G(z)) = 0.889 \t grad_pen = 3.410 \t t = 1.352 \t\n",
      "[6/10][640/782] G loss: -0.508 \t D loss: 0.266 \t D(x) = 0.448 \t D(G(z)) = 0.466 \t grad_pen = 0.249 \t t = 1.350 \t\n",
      "[6/10][656/782] G loss: -0.245 \t D loss: 2.552 \t D(x) = 0.234 \t D(G(z)) = 0.216 \t grad_pen = 2.570 \t t = 1.348 \t\n",
      "[6/10][672/782] G loss: -0.505 \t D loss: 0.134 \t D(x) = 0.570 \t D(G(z)) = 0.509 \t grad_pen = 0.194 \t t = 1.346 \t\n",
      "[6/10][688/782] G loss: -0.570 \t D loss: -0.041 \t D(x) = 0.691 \t D(G(z)) = 0.563 \t grad_pen = 0.088 \t t = 1.344 \t\n",
      "[6/10][704/782] G loss: -0.451 \t D loss: -0.063 \t D(x) = 0.640 \t D(G(z)) = 0.495 \t grad_pen = 0.083 \t t = 1.347 \t\n",
      "[6/10][720/782] G loss: -0.508 \t D loss: -0.113 \t D(x) = 0.748 \t D(G(z)) = 0.531 \t grad_pen = 0.104 \t t = 1.350 \t\n",
      "[6/10][736/782] G loss: -0.541 \t D loss: -0.136 \t D(x) = 0.734 \t D(G(z)) = 0.527 \t grad_pen = 0.071 \t t = 1.336 \t\n",
      "[6/10][752/782] G loss: -0.524 \t D loss: -0.081 \t D(x) = 0.698 \t D(G(z)) = 0.529 \t grad_pen = 0.088 \t t = 1.345 \t\n",
      "[6/10][768/782] G loss: -0.518 \t D loss: -0.125 \t D(x) = 0.646 \t D(G(z)) = 0.459 \t grad_pen = 0.062 \t t = 1.352 \t\n",
      "[7/10][0/782] G loss: -0.442 \t D loss: -0.054 \t D(x) = 0.640 \t D(G(z)) = 0.489 \t grad_pen = 0.098 \t t = 1.164 \t\n",
      "[7/10][16/782] G loss: -0.497 \t D loss: -0.136 \t D(x) = 0.711 \t D(G(z)) = 0.501 \t grad_pen = 0.074 \t t = 1.358 \t\n",
      "[7/10][32/782] G loss: -0.440 \t D loss: -0.154 \t D(x) = 0.755 \t D(G(z)) = 0.487 \t grad_pen = 0.114 \t t = 1.339 \t\n",
      "[7/10][48/782] G loss: -0.441 \t D loss: -0.109 \t D(x) = 0.778 \t D(G(z)) = 0.502 \t grad_pen = 0.167 \t t = 1.365 \t\n",
      "[7/10][64/782] G loss: -0.480 \t D loss: -0.175 \t D(x) = 0.742 \t D(G(z)) = 0.485 \t grad_pen = 0.081 \t t = 1.340 \t\n",
      "[7/10][80/782] G loss: -0.499 \t D loss: -0.177 \t D(x) = 0.741 \t D(G(z)) = 0.480 \t grad_pen = 0.083 \t t = 1.358 \t\n",
      "[7/10][96/782] G loss: -0.458 \t D loss: -0.046 \t D(x) = 0.586 \t D(G(z)) = 0.422 \t grad_pen = 0.118 \t t = 1.350 \t\n",
      "[7/10][112/782] G loss: -0.165 \t D loss: 3.652 \t D(x) = 0.058 \t D(G(z)) = 0.194 \t grad_pen = 3.516 \t t = 1.338 \t\n",
      "[7/10][128/782] G loss: -0.323 \t D loss: 5.075 \t D(x) = 0.061 \t D(G(z)) = 0.356 \t grad_pen = 4.780 \t t = 1.356 \t\n",
      "[7/10][144/782] G loss: -0.474 \t D loss: 0.469 \t D(x) = 0.598 \t D(G(z)) = 0.475 \t grad_pen = 0.592 \t t = 1.339 \t\n",
      "[7/10][160/782] G loss: -0.487 \t D loss: 0.499 \t D(x) = 0.533 \t D(G(z)) = 0.489 \t grad_pen = 0.543 \t t = 1.349 \t\n",
      "[7/10][176/782] G loss: -0.862 \t D loss: 9.479 \t D(x) = 0.996 \t D(G(z)) = 0.926 \t grad_pen = 9.549 \t t = 1.346 \t\n",
      "[7/10][192/782] G loss: -0.939 \t D loss: 0.970 \t D(x) = 0.758 \t D(G(z)) = 0.946 \t grad_pen = 0.781 \t t = 1.338 \t\n",
      "[7/10][208/782] G loss: -0.215 \t D loss: 3.957 \t D(x) = 0.022 \t D(G(z)) = 0.086 \t grad_pen = 3.894 \t t = 1.354 \t\n",
      "[7/10][224/782] G loss: -0.294 \t D loss: 1.050 \t D(x) = 0.254 \t D(G(z)) = 0.306 \t grad_pen = 0.997 \t t = 1.343 \t\n",
      "[7/10][240/782] G loss: -0.464 \t D loss: 0.391 \t D(x) = 0.569 \t D(G(z)) = 0.466 \t grad_pen = 0.494 \t t = 1.376 \t\n",
      "[7/10][256/782] G loss: -0.868 \t D loss: 4.202 \t D(x) = 0.971 \t D(G(z)) = 0.927 \t grad_pen = 4.247 \t t = 1.354 \t\n",
      "[7/10][272/782] G loss: -0.876 \t D loss: 0.583 \t D(x) = 0.749 \t D(G(z)) = 0.892 \t grad_pen = 0.440 \t t = 1.337 \t\n",
      "[7/10][288/782] G loss: -0.007 \t D loss: 7.190 \t D(x) = 0.024 \t D(G(z)) = 0.021 \t grad_pen = 7.193 \t t = 1.366 \t\n",
      "[7/10][304/782] G loss: -0.241 \t D loss: 1.087 \t D(x) = 0.106 \t D(G(z)) = 0.246 \t grad_pen = 0.947 \t t = 1.351 \t\n",
      "[7/10][320/782] G loss: -0.365 \t D loss: 1.311 \t D(x) = 0.103 \t D(G(z)) = 0.394 \t grad_pen = 1.020 \t t = 1.345 \t\n",
      "[7/10][336/782] G loss: -0.506 \t D loss: 0.293 \t D(x) = 0.602 \t D(G(z)) = 0.506 \t grad_pen = 0.389 \t t = 1.343 \t\n",
      "[7/10][352/782] G loss: -0.731 \t D loss: 0.277 \t D(x) = 0.849 \t D(G(z)) = 0.761 \t grad_pen = 0.364 \t t = 1.337 \t\n",
      "[7/10][368/782] G loss: -0.345 \t D loss: 3.216 \t D(x) = 0.038 \t D(G(z)) = 0.352 \t grad_pen = 2.902 \t t = 1.349 \t\n",
      "[7/10][384/782] G loss: -0.504 \t D loss: 1.194 \t D(x) = 0.521 \t D(G(z)) = 0.505 \t grad_pen = 1.210 \t t = 1.334 \t\n",
      "[7/10][400/782] G loss: -0.793 \t D loss: 1.722 \t D(x) = 0.919 \t D(G(z)) = 0.781 \t grad_pen = 1.860 \t t = 1.340 \t\n",
      "[7/10][416/782] G loss: -0.192 \t D loss: 1.776 \t D(x) = 0.081 \t D(G(z)) = 0.183 \t grad_pen = 1.673 \t t = 1.349 \t\n",
      "[7/10][432/782] G loss: -0.469 \t D loss: 1.071 \t D(x) = 0.449 \t D(G(z)) = 0.486 \t grad_pen = 1.035 \t t = 1.347 \t\n",
      "[7/10][448/782] G loss: -0.557 \t D loss: 0.376 \t D(x) = 0.570 \t D(G(z)) = 0.551 \t grad_pen = 0.395 \t t = 1.354 \t\n",
      "[7/10][464/782] G loss: -0.411 \t D loss: 0.803 \t D(x) = 0.158 \t D(G(z)) = 0.428 \t grad_pen = 0.532 \t t = 1.333 \t\n",
      "[7/10][480/782] G loss: -0.511 \t D loss: 0.745 \t D(x) = 0.571 \t D(G(z)) = 0.513 \t grad_pen = 0.803 \t t = 1.343 \t\n",
      "[7/10][496/782] G loss: -0.886 \t D loss: 0.566 \t D(x) = 0.866 \t D(G(z)) = 0.892 \t grad_pen = 0.540 \t t = 1.348 \t\n",
      "[7/10][512/782] G loss: -0.380 \t D loss: 0.335 \t D(x) = 0.208 \t D(G(z)) = 0.399 \t grad_pen = 0.144 \t t = 1.344 \t\n",
      "[7/10][528/782] G loss: -0.530 \t D loss: 0.059 \t D(x) = 0.631 \t D(G(z)) = 0.530 \t grad_pen = 0.160 \t t = 1.365 \t\n",
      "[7/10][544/782] G loss: -0.226 \t D loss: 2.850 \t D(x) = 0.069 \t D(G(z)) = 0.277 \t grad_pen = 2.641 \t t = 1.345 \t\n",
      "[7/10][560/782] G loss: -0.448 \t D loss: 0.524 \t D(x) = 0.189 \t D(G(z)) = 0.459 \t grad_pen = 0.254 \t t = 1.351 \t\n",
      "[7/10][576/782] G loss: -0.741 \t D loss: 1.465 \t D(x) = 0.932 \t D(G(z)) = 0.799 \t grad_pen = 1.598 \t t = 1.358 \t\n",
      "[7/10][592/782] G loss: -0.324 \t D loss: 0.275 \t D(x) = 0.225 \t D(G(z)) = 0.352 \t grad_pen = 0.149 \t t = 1.350 \t\n",
      "[7/10][608/782] G loss: -0.505 \t D loss: 0.544 \t D(x) = 0.558 \t D(G(z)) = 0.506 \t grad_pen = 0.596 \t t = 1.360 \t\n",
      "[7/10][624/782] G loss: -0.477 \t D loss: 1.477 \t D(x) = 0.239 \t D(G(z)) = 0.356 \t grad_pen = 1.360 \t t = 1.336 \t\n",
      "[7/10][640/782] G loss: -0.472 \t D loss: 0.196 \t D(x) = 0.534 \t D(G(z)) = 0.488 \t grad_pen = 0.242 \t t = 1.344 \t\n",
      "[7/10][656/782] G loss: -0.492 \t D loss: 0.182 \t D(x) = 0.516 \t D(G(z)) = 0.492 \t grad_pen = 0.206 \t t = 1.344 \t\n",
      "[7/10][672/782] G loss: -0.422 \t D loss: 0.063 \t D(x) = 0.511 \t D(G(z)) = 0.420 \t grad_pen = 0.153 \t t = 1.350 \t\n",
      "[7/10][688/782] G loss: -0.446 \t D loss: 0.012 \t D(x) = 0.599 \t D(G(z)) = 0.468 \t grad_pen = 0.143 \t t = 1.362 \t\n",
      "[7/10][704/782] G loss: -0.493 \t D loss: 0.077 \t D(x) = 0.666 \t D(G(z)) = 0.496 \t grad_pen = 0.247 \t t = 1.336 \t\n",
      "[7/10][720/782] G loss: -0.884 \t D loss: 0.873 \t D(x) = 0.839 \t D(G(z)) = 0.860 \t grad_pen = 0.852 \t t = 1.344 \t\n",
      "[7/10][736/782] G loss: -0.498 \t D loss: 0.236 \t D(x) = 0.553 \t D(G(z)) = 0.492 \t grad_pen = 0.298 \t t = 1.344 \t\n",
      "[7/10][752/782] G loss: -0.439 \t D loss: 0.075 \t D(x) = 0.551 \t D(G(z)) = 0.446 \t grad_pen = 0.181 \t t = 1.336 \t\n",
      "[7/10][768/782] G loss: -0.464 \t D loss: 0.128 \t D(x) = 0.614 \t D(G(z)) = 0.475 \t grad_pen = 0.268 \t t = 1.359 \t\n",
      "[8/10][0/782] G loss: -0.480 \t D loss: -0.026 \t D(x) = 0.672 \t D(G(z)) = 0.465 \t grad_pen = 0.181 \t t = 1.143 \t\n",
      "[8/10][16/782] G loss: -0.481 \t D loss: 0.073 \t D(x) = 0.666 \t D(G(z)) = 0.484 \t grad_pen = 0.255 \t t = 1.344 \t\n",
      "[8/10][32/782] G loss: -0.528 \t D loss: -0.061 \t D(x) = 0.690 \t D(G(z)) = 0.512 \t grad_pen = 0.117 \t t = 1.346 \t\n",
      "[8/10][48/782] G loss: -0.607 \t D loss: 0.020 \t D(x) = 0.684 \t D(G(z)) = 0.591 \t grad_pen = 0.113 \t t = 1.333 \t\n",
      "[8/10][64/782] G loss: -0.343 \t D loss: -0.043 \t D(x) = 0.588 \t D(G(z)) = 0.359 \t grad_pen = 0.186 \t t = 1.332 \t\n",
      "[8/10][80/782] G loss: -0.447 \t D loss: 0.043 \t D(x) = 0.670 \t D(G(z)) = 0.459 \t grad_pen = 0.255 \t t = 1.340 \t\n",
      "[8/10][96/782] G loss: -0.787 \t D loss: 1.474 \t D(x) = 0.923 \t D(G(z)) = 0.796 \t grad_pen = 1.601 \t t = 1.343 \t\n",
      "[8/10][112/782] G loss: -0.340 \t D loss: 1.089 \t D(x) = 0.125 \t D(G(z)) = 0.175 \t grad_pen = 1.040 \t t = 1.341 \t\n",
      "[8/10][128/782] G loss: -0.448 \t D loss: 0.079 \t D(x) = 0.516 \t D(G(z)) = 0.436 \t grad_pen = 0.158 \t t = 1.331 \t\n",
      "[8/10][144/782] G loss: -0.419 \t D loss: 0.030 \t D(x) = 0.562 \t D(G(z)) = 0.420 \t grad_pen = 0.172 \t t = 1.342 \t\n",
      "[8/10][160/782] G loss: -0.499 \t D loss: 0.081 \t D(x) = 0.635 \t D(G(z)) = 0.496 \t grad_pen = 0.220 \t t = 1.350 \t\n",
      "[8/10][176/782] G loss: -0.802 \t D loss: 0.203 \t D(x) = 0.832 \t D(G(z)) = 0.780 \t grad_pen = 0.255 \t t = 1.338 \t\n",
      "[8/10][192/782] G loss: -0.483 \t D loss: 0.142 \t D(x) = 0.437 \t D(G(z)) = 0.498 \t grad_pen = 0.081 \t t = 1.337 \t\n",
      "[8/10][208/782] G loss: -0.434 \t D loss: -0.014 \t D(x) = 0.574 \t D(G(z)) = 0.447 \t grad_pen = 0.113 \t t = 1.344 \t\n",
      "[8/10][224/782] G loss: -0.435 \t D loss: -0.034 \t D(x) = 0.621 \t D(G(z)) = 0.449 \t grad_pen = 0.138 \t t = 1.343 \t\n",
      "[8/10][240/782] G loss: -0.479 \t D loss: -0.073 \t D(x) = 0.665 \t D(G(z)) = 0.485 \t grad_pen = 0.107 \t t = 1.348 \t\n",
      "[8/10][256/782] G loss: -0.538 \t D loss: 0.190 \t D(x) = 0.874 \t D(G(z)) = 0.565 \t grad_pen = 0.499 \t t = 1.349 \t\n",
      "[8/10][272/782] G loss: -0.473 \t D loss: 0.669 \t D(x) = 0.329 \t D(G(z)) = 0.501 \t grad_pen = 0.497 \t t = 1.354 \t\n",
      "[8/10][288/782] G loss: -0.410 \t D loss: -0.012 \t D(x) = 0.534 \t D(G(z)) = 0.419 \t grad_pen = 0.103 \t t = 1.351 \t\n",
      "[8/10][304/782] G loss: -0.461 \t D loss: 0.074 \t D(x) = 0.587 \t D(G(z)) = 0.447 \t grad_pen = 0.214 \t t = 1.353 \t\n",
      "[8/10][320/782] G loss: -0.463 \t D loss: 0.289 \t D(x) = 0.383 \t D(G(z)) = 0.428 \t grad_pen = 0.243 \t t = 1.342 \t\n",
      "[8/10][336/782] G loss: -0.613 \t D loss: 1.942 \t D(x) = 0.956 \t D(G(z)) = 0.721 \t grad_pen = 2.178 \t t = 1.334 \t\n",
      "[8/10][352/782] G loss: -0.455 \t D loss: 0.022 \t D(x) = 0.534 \t D(G(z)) = 0.453 \t grad_pen = 0.102 \t t = 1.344 \t\n",
      "[8/10][368/782] G loss: -0.489 \t D loss: -0.004 \t D(x) = 0.691 \t D(G(z)) = 0.491 \t grad_pen = 0.196 \t t = 1.343 \t\n",
      "[8/10][384/782] G loss: -0.458 \t D loss: -0.045 \t D(x) = 0.635 \t D(G(z)) = 0.468 \t grad_pen = 0.122 \t t = 1.350 \t\n",
      "[8/10][400/782] G loss: -0.435 \t D loss: 0.002 \t D(x) = 0.631 \t D(G(z)) = 0.471 \t grad_pen = 0.162 \t t = 1.343 \t\n",
      "[8/10][416/782] G loss: -0.431 \t D loss: -0.064 \t D(x) = 0.620 \t D(G(z)) = 0.447 \t grad_pen = 0.109 \t t = 1.339 \t\n",
      "[8/10][432/782] G loss: -0.504 \t D loss: 0.104 \t D(x) = 0.529 \t D(G(z)) = 0.469 \t grad_pen = 0.164 \t t = 1.339 \t\n",
      "[8/10][448/782] G loss: -0.837 \t D loss: 0.431 \t D(x) = 0.859 \t D(G(z)) = 0.829 \t grad_pen = 0.460 \t t = 1.338 \t\n",
      "[8/10][464/782] G loss: -0.444 \t D loss: 0.183 \t D(x) = 0.371 \t D(G(z)) = 0.470 \t grad_pen = 0.084 \t t = 1.340 \t\n",
      "[8/10][480/782] G loss: -0.404 \t D loss: -0.003 \t D(x) = 0.637 \t D(G(z)) = 0.453 \t grad_pen = 0.181 \t t = 1.341 \t\n",
      "[8/10][496/782] G loss: -0.476 \t D loss: 0.005 \t D(x) = 0.607 \t D(G(z)) = 0.452 \t grad_pen = 0.160 \t t = 1.358 \t\n",
      "[8/10][512/782] G loss: -0.486 \t D loss: -0.086 \t D(x) = 0.733 \t D(G(z)) = 0.497 \t grad_pen = 0.150 \t t = 1.359 \t\n",
      "[8/10][528/782] G loss: -0.506 \t D loss: -0.130 \t D(x) = 0.705 \t D(G(z)) = 0.506 \t grad_pen = 0.069 \t t = 1.348 \t\n",
      "[8/10][544/782] G loss: -0.505 \t D loss: 0.037 \t D(x) = 0.703 \t D(G(z)) = 0.480 \t grad_pen = 0.260 \t t = 1.343 \t\n",
      "[8/10][560/782] G loss: -0.734 \t D loss: 0.332 \t D(x) = 0.867 \t D(G(z)) = 0.774 \t grad_pen = 0.424 \t t = 1.358 \t\n",
      "[8/10][576/782] G loss: -0.462 \t D loss: 0.060 \t D(x) = 0.574 \t D(G(z)) = 0.463 \t grad_pen = 0.172 \t t = 1.358 \t\n",
      "[8/10][592/782] G loss: -0.464 \t D loss: -0.001 \t D(x) = 0.681 \t D(G(z)) = 0.493 \t grad_pen = 0.188 \t t = 1.354 \t\n",
      "[8/10][608/782] G loss: -0.518 \t D loss: 0.113 \t D(x) = 0.570 \t D(G(z)) = 0.528 \t grad_pen = 0.155 \t t = 1.348 \t\n",
      "[8/10][624/782] G loss: -0.725 \t D loss: 0.096 \t D(x) = 0.801 \t D(G(z)) = 0.721 \t grad_pen = 0.176 \t t = 1.338 \t\n",
      "[8/10][640/782] G loss: -0.453 \t D loss: -0.059 \t D(x) = 0.627 \t D(G(z)) = 0.441 \t grad_pen = 0.126 \t t = 1.334 \t\n",
      "[8/10][656/782] G loss: -0.405 \t D loss: -0.084 \t D(x) = 0.579 \t D(G(z)) = 0.410 \t grad_pen = 0.084 \t t = 1.335 \t\n",
      "[8/10][672/782] G loss: -0.484 \t D loss: 0.033 \t D(x) = 0.565 \t D(G(z)) = 0.448 \t grad_pen = 0.150 \t t = 1.334 \t\n",
      "[8/10][688/782] G loss: -0.444 \t D loss: 0.042 \t D(x) = 0.560 \t D(G(z)) = 0.438 \t grad_pen = 0.164 \t t = 1.349 \t\n",
      "[8/10][704/782] G loss: -0.500 \t D loss: 0.069 \t D(x) = 0.777 \t D(G(z)) = 0.508 \t grad_pen = 0.339 \t t = 1.350 \t\n",
      "[8/10][720/782] G loss: -0.491 \t D loss: -0.072 \t D(x) = 0.755 \t D(G(z)) = 0.519 \t grad_pen = 0.164 \t t = 1.351 \t\n",
      "[8/10][736/782] G loss: -0.767 \t D loss: 0.656 \t D(x) = 0.814 \t D(G(z)) = 0.739 \t grad_pen = 0.730 \t t = 1.339 \t\n",
      "[8/10][752/782] G loss: -0.459 \t D loss: 0.152 \t D(x) = 0.492 \t D(G(z)) = 0.464 \t grad_pen = 0.180 \t t = 1.341 \t\n",
      "[8/10][768/782] G loss: -0.426 \t D loss: 0.053 \t D(x) = 0.513 \t D(G(z)) = 0.419 \t grad_pen = 0.147 \t t = 1.352 \t\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Highly adapted from: https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n",
    "\"\"\"\n",
    "\n",
    "g_iters = 1 # 5\n",
    "d_iters = 2 # 1, discriminator is called critic in WGAN paper\n",
    "\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = one * -1\n",
    "\n",
    "iters = 0\n",
    "t1 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        real = data.to(device)\n",
    "        b_size = real.size(0)\n",
    "        \n",
    "        \"\"\"\n",
    "        Train G\n",
    "        \"\"\"\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        for _ in range(g_iters):\n",
    "            netG.zero_grad()\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            noise.requires_grad_(True)\n",
    "            fake = netG(noise)\n",
    "\n",
    "            g_cost = netD(fake).mean()\n",
    "            g_cost.backward(mone)\n",
    "            g_cost = -g_cost\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        \"\"\"\n",
    "        Train D\n",
    "        \"\"\"\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "        for _ in range(d_iters):\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # generate fake data\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                noisev = noise # Freeze G, training D\n",
    "\n",
    "            fake = netG(noisev).detach()\n",
    "\n",
    "            # train with real data\n",
    "            d_real = netD(real).mean()\n",
    "\n",
    "            # train with fake data\n",
    "            d_fake = netD(fake).mean()\n",
    "\n",
    "            # train with interpolates data\n",
    "            gradient_penalty = calc_gradient_penalty(netD, real, fake, b_size)\n",
    "\n",
    "             # final disc cost\n",
    "            d_cost = d_fake - d_real + gradient_penalty\n",
    "            d_cost.backward()\n",
    "            w_dist = d_fake  - d_real # wasserstein distance\n",
    "            optimizerD.step()\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        weights_saved = False\n",
    "        if (iters % 100 == 0): # save weights every % .... iters\n",
    "            #print('weights saved')\n",
    "            if ngpu > 1:\n",
    "                torch.save(netG.module.state_dict(), 'netG_state_dict2')\n",
    "                torch.save(netD.module.state_dict(), 'netD_state_dict2')\n",
    "            else:\n",
    "                torch.save(netG.state_dict(), 'netG_state_dict2')\n",
    "                torch.save(netD.state_dict(), 'netD_state_dict2')\n",
    "            \n",
    "        \n",
    "        if i % (16) == 0:\n",
    "            t2 = time.time()\n",
    "            print('[%d/%d][%d/%d] G loss: %.3f \\t D loss: %.3f \\t D(x) = %.3f \\t D(G(z)) = %.3f \\t grad_pen = %.3f \\t t = %.3f \\t'% \n",
    "                      (epoch, num_epochs, i, len(dataloader), g_cost, d_cost, d_real, d_fake, gradient_penalty, (t2-t1)))\n",
    "            t1 = time.time()\n",
    "                \n",
    "        iters += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
