{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  1\n",
      "Number of GPUs used:  1\n",
      "Number of images:  1\n",
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import time as t\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import model\n",
    "from keijzer_exogan import *\n",
    "\n",
    "# initialize random seeds\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Local variables\n",
    "\"\"\"\n",
    "workers = 0 # Number of workers for dataloader, 0 when to_vram is enabled\n",
    "batch_size = 1 # using one image ofcourse\n",
    "image_size = 32\n",
    "nz = 100 # size of latent vector\n",
    "n_iters = 5*10**3 # number of iterations to do for inpainting\n",
    "torch.backends.cudnn.benchmark=True # Uses udnn auto-tuner to find the best algorithm to use for your hardware, speeds up training by almost 50%\n",
    "lr = 1e-1\n",
    "lr_G = 2e-4\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "selected_gpus = [0] # Number of GPUs available. Use 0 for CPU mode.\n",
    "\n",
    "path = '/datb/16011015/ExoGAN_data/selection//' #notice how you dont put the last folder in here...\n",
    "images = np.load(path+'last_chunks_25_percent_images.npy')\n",
    "\n",
    "print('Batch size: ', batch_size)\n",
    "\n",
    "\n",
    "# Number of training epochs\n",
    "\n",
    "# Learning rate for optimizers\n",
    "ngpu = len(selected_gpus)\n",
    "print('Number of GPUs used: ', ngpu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load data and prepare DataLoader\n",
    "\"\"\"\n",
    "shuffle = True\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(images) # shuffles the images\n",
    "\n",
    "images = images[:int(len(images)*0.0000025)] # use only first ... percent of the data (0.05)\n",
    "print('Number of images: ', len(images))\n",
    "\n",
    "dataset = numpy_dataset(data=images, to_vram=True) # to_vram pins it to all GPU's\n",
    "#dataset = numpy_dataset(data=images, to_vram=True, transform=transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])) # to_vram pins it to all GPU's\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)\n",
    "\n",
    "\"\"\"\n",
    "Load and setup models\n",
    "\"\"\"\n",
    "# Initialize cuda\n",
    "device = torch.device(\"cuda:\"+str(selected_gpus[0]) if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Load models, set to evaluation mode since training is not needed (this also allows batchsize 1 to work with batchnorm2d layers)\n",
    "netG = model.Generator(ngpu).eval().to(device)\n",
    "netD = model.Discriminator(ngpu).eval().to(device)\n",
    "\n",
    "# Apply weights\n",
    "print('Loading weights...')\n",
    "try:\n",
    "    # Load saved weights\n",
    "    netG.load_state_dict(torch.load('netG_state_dict', map_location=device)) #net.module..load_... for parallel model , net.load_... for single gpu model\n",
    "    netD.load_state_dict(torch.load('netD_state_dict', map_location=device))\n",
    "except:\n",
    "    print('Could not load saved weights.')\n",
    "    sys.exit()\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, device_ids=selected_gpus, output_device=device)\n",
    "    netD = nn.DataParallel(netD, device_ids=selected_gpus, output_device=device)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define input training stuff (fancy this up)\n",
    "\"\"\"\n",
    "G = netG\n",
    "D = netD\n",
    "z = torch.randn(1, nz, 1, 1, requires_grad=True, device=device)\n",
    "\n",
    "lamb = 3e-1\n",
    "\n",
    "criteria = nn.BCELoss()\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) # should be sgd\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show a generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f78853e2668>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z2 = torch.randn(1, nz, 1, 1, device=device)\n",
    "t = G(z2).detach().cpu()[0, 0, :, :]\n",
    "# TODO: why does this already look real?\n",
    "plt.imshow(t, cmap='gray') # plot the masked image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting\n",
    "The corrupted image $y$ is mapped to the closest $z$ in the latent representation space, this mapping is denoted as $\\hat{z}$.\n",
    "    \n",
    "$\\hat{z} = \\operatorname{arg\\,min}_z \\{ \\mathcal{L}_c(z |y, M) + \\mathcal{L}_p (z) \\}$\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathcal{L}_c(z |y, M) = || M \\bigodot G(z) - M \\bigodot y||_1 = || M \\bigodot (G(z)-y) ||_1 $\n",
    "\n",
    "with $\\mathcal{L}_c$ being contextual loss and $M$ being a binary mask with the same size as $y$,\n",
    "\n",
    "$\\mathcal{L}_p (z) = \\lambda \\operatorname{log}(1-D(G(z)))$\n",
    "\n",
    "with $\\mathcal{L}_p$ being perceptual loss and $D$ being the discriminator.\n",
    "  \n",
    "Once $G(\\hat{z})$ is generated, the final solution $\\hat{x}$ is calculated as\n",
    "\n",
    "$\\hat{x} = \\operatorname{arg\\, min}_x ||\\nabla x - \\nabla G(\\hat{z}) ||^2_2$  \n",
    "\n",
    "(substitute $x_i = y_i$ for $M_i = 1$).\n",
    "\n",
    "-----\n",
    "\n",
    "$|| ... ||$ is done by `torch.norm()`.  \n",
    "$... \\bigodot ...$ is done by `torch.mul()`.  \n",
    "-----\n",
    "TODO: Implement $\\hat{x} = \\operatorname{arg\\, min}_x ||\\nabla x - \\nabla G(\\hat{z}) ||^2_2$    \n",
    "Currently $\\hat{x} = G(\\hat{z}) \\bigodot (1 -M)+y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "real_images = []\n",
    "masked_images= []\n",
    "inpainted_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape:  torch.Size([1, 1, 32, 32])\n",
      "image.shape:  torch.Size([1, 1, 32, 32])\n",
      "mask.shape torch.Size([1, 1, 32, 32])\n",
      "masked image shape torch.Size([1, 1, 32, 32])\n",
      " iteration : 4608 , context_loss: 30.7214, perceptual_loss: -2.249909, total_loss: 30.046434"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Inpainting\n",
    "\"\"\"\n",
    "for i, data in enumerate(dataloader, 0): # batches per epoch\n",
    "    real_cpu = data.to(device)\n",
    "    b_size = real_cpu.size(0) # this is one ofc, it's one image we're trying to inpaint\n",
    "\n",
    "    print(\"data.shape: \", data.shape)\n",
    "    image = data.to(device) # select the image (Channel, Height, Width), this is the original unmasked input image\n",
    "    real_images.append(image)\n",
    "    print(\"image.shape: \", image.shape)\n",
    "\n",
    "    \"\"\"Mask the images manually, for testing pruposes\"\"\"\n",
    "    mask = torch.ones(size=image.shape).to(device) # create mask with 1's in the shape of image\n",
    "    \n",
    "    print(\"mask.shape\", mask.shape)\n",
    "\n",
    "    # use a random 'easy' mask\n",
    "    mask[:, :, :30, 30:] = 0\n",
    "    mask[:, :, 30:, :30] = 0\n",
    "\n",
    "    masked_image = torch.mul(image, mask).to(device) #image bigodot mask\n",
    "    masked_images.append(masked_image)\n",
    "    print('masked image shape', masked_image.shape)\n",
    "    plt.imshow(masked_image.detach().cpu()[0, 0, :, :], cmap='gray') # plot the masked image\n",
    "\n",
    "    opt = optim.Adam([z], lr=lr)\n",
    "\n",
    "    # what's v and m?\n",
    "    v = torch.tensor(0, dtype=torch.float32, device=device)\n",
    "    m = torch.tensor(0, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "    \"\"\"Start the inpainting process\"\"\"\n",
    "    for iteration in range(n_iters):\n",
    "        if z.grad is not None:\n",
    "            z.grad.data.zero_()\n",
    "\n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "\n",
    "\n",
    "        image_generated = G(z) # generated image G(z)\n",
    "        image_generated_masked = torch.mul(image_generated, mask) # G(z) bigodot M\n",
    "        image_generated_inpainted = torch.mul(image_generated, (1-mask))+masked_image\n",
    "        \n",
    "        if (iteration % 100 == 0):\n",
    "            inpainted_images.append(image_generated_inpainted)\n",
    "\n",
    "        #print(\"image_generated_inpainted.shape : \",image_generated_inpainted.shape)\n",
    "\n",
    "        t = image_generated_inpainted.detach().cpu()[0, 0, :, :]\n",
    "\n",
    "        # TODO: why does this already look real?\n",
    "        plt.imshow(t, cmap='gray') # plot the masked image \n",
    "\n",
    "        \"\"\"Calculate losses\"\"\"\n",
    "        loss_context = torch.norm(image_generated_masked-masked_image, p=1) #what's p=1?\n",
    "        discriminator_output = netD(image_generated_inpainted)\n",
    "        #print(\"Discriminator output: \", discriminator_output)\n",
    "\n",
    "        labels = torch.full((b_size,), 1, device=device)\n",
    "        loss_perceptual = torch.log(1-discriminator_output)\n",
    "        \n",
    "        #print(loss_perceptual.data.cpu().numpy().flatten()[0])\n",
    "\n",
    "        total_loss = loss_context + lamb*loss_perceptual\n",
    "        \n",
    "        # grab the values from losses for printing\n",
    "        loss_perceptual = loss_perceptual.data.cpu().numpy().flatten()[0]\n",
    "        loss_context = loss_context.data.cpu().numpy().flatten()[0]\n",
    "\n",
    "\n",
    "\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss = total_loss.data.cpu().numpy().flatten()[0]\n",
    "        \n",
    "        print(\"\\r iteration : {:4} , context_loss: {:.4f}, perceptual_loss: {:4f}, total_loss: {:4f}\".format(iteration,loss_context,loss_perceptual, total_loss),end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = real_images[0].detach().cpu()[0, 0, :, :]\n",
    "real_masked = masked_images[0].detach().cpu()[0, 0, :, :]\n",
    "\n",
    "#first_generated =inpainted_images[1].detach().cpu()[0, 0, :, :]\n",
    "last_generated = inpainted_images[-1].detach().cpu()[0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(real, cmap='gray')\n",
    "plt.title('Ground truth')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(real_masked, cmap='gray')\n",
    "plt.title('Masked input')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(last_generated, cmap='gray')\n",
    "plt.title('Inpainted')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('plots/inpainting.png', dpi=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = real - last_generated\n",
    "sns.heatmap(difference, cmap='gray')\n",
    "plt.title('Ground truth - Inpainted ')\n",
    "\n",
    "difference  = difference.cpu().detach().numpy().flatten()\n",
    "\n",
    "mean, sigma, min_, max_ = difference.mean().item(), difference.std(), difference.min(), difference.max()\n",
    "print('mean: %.4f \\n std: %.4f \\n min: %.4f \\n max: %.4f' % (mean, sigma, min_, max_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['animation.embed_limit'] = 20**128\n",
    "\n",
    "#%%capture\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(i.detach().cpu()[0, 0, :, :], animated=True, cmap='gray')] for i in inpainted_images]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=1000, blit=True)\n",
    "\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
