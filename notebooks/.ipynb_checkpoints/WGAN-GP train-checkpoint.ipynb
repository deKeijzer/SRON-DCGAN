{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fab841198f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import time as t\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import time as time\n",
    "\n",
    "from torch import autograd\n",
    "\n",
    "import model\n",
    "from keijzer_exogan import *\n",
    "\n",
    "# initialize random seeds\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Local variables\n",
    "\"\"\"\n",
    "workers = 0 # Number of workers for dataloader, 0 when to_vram is enabled\n",
    "batch_size = 64 # 2**11\n",
    "image_size = 32\n",
    "nz = 100 # size of latent vector\n",
    "num_epochs = 10*10**3\n",
    "torch.backends.cudnn.benchmark=True # Uses udnn auto-tuner to find the best algorithm to use for your hardware, speeds up training by almost 50%\n",
    "lr = 1e-4\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9\n",
    "\n",
    "lambda_ = 10\n",
    "\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "selected_gpus = [2,3] # Number of GPUs available. Use 0 for CPU mode.\n",
    "\n",
    "path = '/datb/16011015/ExoGAN_data/selection//' #notice how you dont put the last folder in here...\n",
    "images = np.load(path+'first_chunks_25_percent_images.npy')\n",
    "\n",
    "swap_labels_randomly = False\n",
    "\n",
    "train_d_g_conditional = False # switch between training D and G based on set threshold\n",
    "d_g_conditional_threshold = 0.55 # D_G_z1 < threshold, train G\n",
    "\n",
    "train_d_g_conditional_per_epoch = False\n",
    "\n",
    "train_d_g_conditional_per_n_iters = False\n",
    "train_d_g_n_iters = 2 # When 2, train D 2 times before training G 1 time\n",
    "\n",
    "use_saved_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  64\n",
      "Number of GPUs used:  2\n",
      "Number of images:  50000\n"
     ]
    }
   ],
   "source": [
    "print('Batch size: ', batch_size)\n",
    "ngpu = len(selected_gpus)\n",
    "print('Number of GPUs used: ', ngpu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load data and prepare DataLoader\n",
    "\"\"\"\n",
    "shuffle = True\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(images) # shuffles the images\n",
    "\n",
    "images = images[:int(len(images)*0.1)] # use only first ... percent of the data (0.05)\n",
    "print('Number of images: ', len(images))\n",
    "\n",
    "dataset = numpy_dataset(data=images, to_vram=True) # to_vram pins it to all GPU's\n",
    "#dataset = numpy_dataset(data=images, to_vram=True, transform=transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])) # to_vram pins it to all GPU's\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load and setup models\n",
    "\"\"\"\n",
    "# Initialize cuda\n",
    "device = torch.device(\"cuda:\"+str(selected_gpus[0]) if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Load models\n",
    "netG = model.Generator(ngpu).to(device)\n",
    "netD = model.Discriminator(ngpu).to(device)\n",
    "\n",
    "# Apply weights\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netG.apply(weights_init) # It's not clean/efficient to load these ones first, but it works.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "if use_saved_weights:\n",
    "    try:\n",
    "        # Load saved weights\n",
    "        netG.load_state_dict(torch.load('netG_state_dict2', map_location=device)) #net.module..load_... for parallel model , net.load_... for single gpu model\n",
    "        netD.load_state_dict(torch.load('netD_state_dict2', map_location=device))\n",
    "        print('Succesfully loaded saved weights.')\n",
    "    except:\n",
    "        print('Could not load saved weights, using new ones.')\n",
    "        pass\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, device_ids=selected_gpus, output_device=device)\n",
    "    netD = nn.DataParallel(netD, device_ids=selected_gpus, output_device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define input training stuff (fancy this up)\n",
    "\"\"\"\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2)) # should be sgd\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "switch = True # condition switch, to switch between D and G per epoch\n",
    "previous_switch = 0\n",
    "\n",
    "train_D = True\n",
    "train_G = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data, b_size):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n",
    "    \"\"\"\n",
    "    alpha = torch.rand(b_size, 1)\n",
    "    alpha = alpha.expand(b_size, int(real_data.nelement()/b_size)).contiguous()\n",
    "    alpha = alpha.view(b_size, 1, image_size, image_size)\n",
    "    alpha = alpha.to(device)\n",
    "    \n",
    "    fake_data = fake_data.view(b_size, 1, image_size, image_size)\n",
    "    interpolates = alpha * real_data.detach() + ((1 - alpha) * fake_data.detach())\n",
    "\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates.requires_grad_(True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)                              \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switched to WGAN-GP (https://arxiv.org/pdf/1704.00028) instead of DCGAN for training stability reasons.  \n",
    "Wheras DCGAN collapsed (D(x) staying 0.8-0.9 and D(G(z)) becoming 0.5000) to output black images, WGAN-GP keeps training well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000][0/782] G loss: -0.500 \t D loss: 9.999 \t D(x) = 0.500 \t D(G(z)) = 0.500 \t grad_pen = 9.999 \t t = 2.526 \t\n",
      "[0/10000][256/782] G loss: -0.836 \t D loss: 2.697 \t D(x) = 0.509 \t D(G(z)) = 0.848 \t grad_pen = 2.358 \t t = 33.189 \t\n",
      "[0/10000][512/782] G loss: -0.578 \t D loss: 5.436 \t D(x) = 0.217 \t D(G(z)) = 0.592 \t grad_pen = 5.061 \t t = 33.165 \t\n",
      "[0/10000][768/782] G loss: -0.540 \t D loss: 1.998 \t D(x) = 0.128 \t D(G(z)) = 0.568 \t grad_pen = 1.559 \t t = 33.140 \t\n",
      "[1/10000][0/782] G loss: -0.443 \t D loss: 1.393 \t D(x) = 0.083 \t D(G(z)) = 0.420 \t grad_pen = 1.057 \t t = 1.921 \t\n",
      "[1/10000][256/782] G loss: -0.397 \t D loss: 3.120 \t D(x) = 0.019 \t D(G(z)) = 0.420 \t grad_pen = 2.718 \t t = 33.151 \t\n",
      "[1/10000][512/782] G loss: -0.612 \t D loss: 1.399 \t D(x) = 0.067 \t D(G(z)) = 0.614 \t grad_pen = 0.852 \t t = 33.214 \t\n",
      "[1/10000][768/782] G loss: -0.641 \t D loss: 1.327 \t D(x) = 0.121 \t D(G(z)) = 0.647 \t grad_pen = 0.801 \t t = 33.088 \t\n",
      "[2/10000][0/782] G loss: -0.625 \t D loss: 1.264 \t D(x) = 0.137 \t D(G(z)) = 0.627 \t grad_pen = 0.775 \t t = 1.788 \t\n",
      "[2/10000][256/782] G loss: -0.507 \t D loss: 0.942 \t D(x) = 0.085 \t D(G(z)) = 0.518 \t grad_pen = 0.510 \t t = 33.131 \t\n",
      "[2/10000][512/782] G loss: -0.495 \t D loss: 1.986 \t D(x) = 0.081 \t D(G(z)) = 0.495 \t grad_pen = 1.572 \t t = 33.312 \t\n",
      "[2/10000][768/782] G loss: -0.445 \t D loss: 10.448 \t D(x) = 0.000 \t D(G(z)) = 0.448 \t grad_pen = 10.000 \t t = 33.199 \t\n",
      "[3/10000][0/782] G loss: -0.446 \t D loss: 23.908 \t D(x) = 0.000 \t D(G(z)) = 0.445 \t grad_pen = 23.463 \t t = 1.788 \t\n",
      "[3/10000][256/782] G loss: -0.487 \t D loss: 10.491 \t D(x) = 0.000 \t D(G(z)) = 0.491 \t grad_pen = 10.000 \t t = 33.216 \t\n",
      "[3/10000][512/782] G loss: -0.509 \t D loss: 0.721 \t D(x) = 0.504 \t D(G(z)) = 0.512 \t grad_pen = 0.713 \t t = 33.151 \t\n",
      "[3/10000][768/782] G loss: -0.529 \t D loss: 0.375 \t D(x) = 0.627 \t D(G(z)) = 0.534 \t grad_pen = 0.467 \t t = 33.208 \t\n",
      "[4/10000][0/782] G loss: -0.540 \t D loss: 0.510 \t D(x) = 0.607 \t D(G(z)) = 0.546 \t grad_pen = 0.571 \t t = 1.781 \t\n",
      "[4/10000][256/782] G loss: -0.517 \t D loss: 1.433 \t D(x) = 0.060 \t D(G(z)) = 0.519 \t grad_pen = 0.974 \t t = 33.176 \t\n",
      "[4/10000][512/782] G loss: -0.524 \t D loss: 0.902 \t D(x) = 0.153 \t D(G(z)) = 0.527 \t grad_pen = 0.529 \t t = 33.192 \t\n",
      "[4/10000][768/782] G loss: -0.470 \t D loss: 0.697 \t D(x) = 0.153 \t D(G(z)) = 0.476 \t grad_pen = 0.374 \t t = 33.252 \t\n",
      "[5/10000][0/782] G loss: -0.581 \t D loss: 0.661 \t D(x) = 0.288 \t D(G(z)) = 0.580 \t grad_pen = 0.369 \t t = 1.795 \t\n",
      "[5/10000][256/782] G loss: -0.786 \t D loss: 1.064 \t D(x) = 0.498 \t D(G(z)) = 0.803 \t grad_pen = 0.759 \t t = 33.240 \t\n",
      "[5/10000][512/782] G loss: -0.452 \t D loss: 0.473 \t D(x) = 0.259 \t D(G(z)) = 0.456 \t grad_pen = 0.276 \t t = 33.222 \t\n",
      "[5/10000][768/782] G loss: -0.486 \t D loss: 0.308 \t D(x) = 0.366 \t D(G(z)) = 0.494 \t grad_pen = 0.180 \t t = 33.188 \t\n",
      "[6/10000][0/782] G loss: -0.554 \t D loss: 0.227 \t D(x) = 0.470 \t D(G(z)) = 0.555 \t grad_pen = 0.142 \t t = 1.789 \t\n",
      "[6/10000][256/782] G loss: -0.506 \t D loss: 0.102 \t D(x) = 0.488 \t D(G(z)) = 0.510 \t grad_pen = 0.079 \t t = 33.113 \t\n",
      "[6/10000][512/782] G loss: -0.812 \t D loss: 0.489 \t D(x) = 0.657 \t D(G(z)) = 0.878 \t grad_pen = 0.269 \t t = 33.133 \t\n",
      "[6/10000][768/782] G loss: -0.515 \t D loss: 0.133 \t D(x) = 0.505 \t D(G(z)) = 0.529 \t grad_pen = 0.109 \t t = 33.180 \t\n",
      "[7/10000][0/782] G loss: -0.580 \t D loss: 0.147 \t D(x) = 0.531 \t D(G(z)) = 0.607 \t grad_pen = 0.071 \t t = 1.786 \t\n",
      "[7/10000][256/782] G loss: -0.530 \t D loss: 0.052 \t D(x) = 0.541 \t D(G(z)) = 0.547 \t grad_pen = 0.046 \t t = 33.277 \t\n",
      "[7/10000][512/782] G loss: -0.476 \t D loss: 0.032 \t D(x) = 0.507 \t D(G(z)) = 0.483 \t grad_pen = 0.056 \t t = 33.266 \t\n",
      "[7/10000][768/782] G loss: -0.489 \t D loss: -0.015 \t D(x) = 0.568 \t D(G(z)) = 0.506 \t grad_pen = 0.048 \t t = 33.332 \t\n",
      "[8/10000][0/782] G loss: -0.501 \t D loss: 0.023 \t D(x) = 0.567 \t D(G(z)) = 0.506 \t grad_pen = 0.085 \t t = 1.784 \t\n",
      "[8/10000][256/782] G loss: -0.490 \t D loss: 0.018 \t D(x) = 0.512 \t D(G(z)) = 0.493 \t grad_pen = 0.036 \t t = 33.048 \t\n",
      "[8/10000][512/782] G loss: -0.584 \t D loss: 0.018 \t D(x) = 0.647 \t D(G(z)) = 0.587 \t grad_pen = 0.077 \t t = 33.174 \t\n",
      "[8/10000][768/782] G loss: -0.516 \t D loss: 0.011 \t D(x) = 0.544 \t D(G(z)) = 0.519 \t grad_pen = 0.036 \t t = 33.166 \t\n",
      "[9/10000][0/782] G loss: -0.487 \t D loss: 0.024 \t D(x) = 0.500 \t D(G(z)) = 0.487 \t grad_pen = 0.037 \t t = 1.808 \t\n",
      "[9/10000][256/782] G loss: -0.502 \t D loss: -0.007 \t D(x) = 0.536 \t D(G(z)) = 0.499 \t grad_pen = 0.030 \t t = 33.203 \t\n",
      "[9/10000][512/782] G loss: -0.625 \t D loss: 0.105 \t D(x) = 0.678 \t D(G(z)) = 0.642 \t grad_pen = 0.141 \t t = 33.205 \t\n",
      "[9/10000][768/782] G loss: -0.482 \t D loss: -0.009 \t D(x) = 0.531 \t D(G(z)) = 0.495 \t grad_pen = 0.027 \t t = 33.175 \t\n",
      "[10/10000][0/782] G loss: -0.543 \t D loss: 0.010 \t D(x) = 0.563 \t D(G(z)) = 0.546 \t grad_pen = 0.027 \t t = 1.782 \t\n",
      "[10/10000][256/782] G loss: -0.528 \t D loss: -0.030 \t D(x) = 0.575 \t D(G(z)) = 0.524 \t grad_pen = 0.022 \t t = 33.130 \t\n",
      "[10/10000][512/782] G loss: -0.430 \t D loss: 0.053 \t D(x) = 0.458 \t D(G(z)) = 0.447 \t grad_pen = 0.064 \t t = 33.308 \t\n",
      "[10/10000][768/782] G loss: -0.559 \t D loss: -0.007 \t D(x) = 0.611 \t D(G(z)) = 0.563 \t grad_pen = 0.040 \t t = 33.131 \t\n",
      "[11/10000][0/782] G loss: -0.476 \t D loss: -0.015 \t D(x) = 0.526 \t D(G(z)) = 0.480 \t grad_pen = 0.031 \t t = 1.790 \t\n",
      "[11/10000][256/782] G loss: -0.486 \t D loss: -0.021 \t D(x) = 0.529 \t D(G(z)) = 0.486 \t grad_pen = 0.023 \t t = 33.166 \t\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Highly adapted from: https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n",
    "\"\"\"\n",
    "\n",
    "g_iters = 1 # 5\n",
    "d_iters = 2 # 1, discriminator is called critic in WGAN paper\n",
    "\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = one * -1\n",
    "\n",
    "iters = 0\n",
    "t1 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        real = data.to(device)\n",
    "        b_size = real.size(0)\n",
    "        \n",
    "        \"\"\"\n",
    "        Train G\n",
    "        \"\"\"\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        for _ in range(g_iters):\n",
    "            netG.zero_grad()\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            noise.requires_grad_(True)\n",
    "            fake = netG(noise)\n",
    "\n",
    "            g_cost = netD(fake).mean()\n",
    "            g_cost.backward(mone)\n",
    "            g_cost = -g_cost\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        \"\"\"\n",
    "        Train D\n",
    "        \"\"\"\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "        for _ in range(d_iters):\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # generate fake data\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                noisev = noise # Freeze G, training D\n",
    "\n",
    "            fake = netG(noisev).detach()\n",
    "\n",
    "            # train with real data\n",
    "            d_real = netD(real).mean()\n",
    "\n",
    "            # train with fake data\n",
    "            d_fake = netD(fake).mean()\n",
    "\n",
    "            # train with interpolates data\n",
    "            gradient_penalty = calc_gradient_penalty(netD, real, fake, b_size)\n",
    "\n",
    "             # final disc cost\n",
    "            d_cost = d_fake - d_real + gradient_penalty\n",
    "            d_cost.backward()\n",
    "            w_dist = d_fake  - d_real # wasserstein distance\n",
    "            optimizerD.step()\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        weights_saved = False\n",
    "        if (iters % 100 == 0): # save weights every % .... iters\n",
    "            #print('weights saved')\n",
    "            if ngpu > 1:\n",
    "                torch.save(netG.module.state_dict(), 'netG_state_dict2')\n",
    "                torch.save(netD.module.state_dict(), 'netD_state_dict2')\n",
    "            else:\n",
    "                torch.save(netG.state_dict(), 'netG_state_dict2')\n",
    "                torch.save(netD.state_dict(), 'netD_state_dict2')\n",
    "            \n",
    "        \n",
    "        if i % (256) == 0:\n",
    "            t2 = time.time()\n",
    "            print('[%d/%d][%d/%d] G loss: %.3f \\t D loss: %.3f \\t D(x) = %.3f \\t D(G(z)) = %.3f \\t grad_pen = %.3f \\t t = %.3f \\t'% \n",
    "                      (epoch, num_epochs, i, len(dataloader), g_cost, d_cost, d_real, d_fake, gradient_penalty, (t2-t1)))\n",
    "            t1 = time.time()\n",
    "                \n",
    "        iters += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
