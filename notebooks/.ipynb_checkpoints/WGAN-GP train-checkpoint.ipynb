{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fac580cc950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import time as t\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import time as time\n",
    "\n",
    "from torch import autograd\n",
    "\n",
    "import model\n",
    "from keijzer_exogan import *\n",
    "\n",
    "# initialize random seeds\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Local variables\n",
    "\"\"\"\n",
    "workers = 0 # Number of workers for dataloader, 0 when to_vram is enabled\n",
    "batch_size = 64 # 2**11\n",
    "image_size = 32\n",
    "nz = 100 # size of latent vector\n",
    "num_epochs = 10\n",
    "torch.backends.cudnn.benchmark=True # Uses udnn auto-tuner to find the best algorithm to use for your hardware, speeds up training by almost 50%\n",
    "lr = 1e-4\n",
    "lr_G = 1e-4\n",
    "lambda_ = 10\n",
    "\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "selected_gpus = [2,3] # Number of GPUs available. Use 0 for CPU mode.\n",
    "\n",
    "path = '/datb/16011015/ExoGAN_data/selection//' #notice how you dont put the last folder in here...\n",
    "images = np.load(path+'first_chunks_25_percent_images.npy')\n",
    "\n",
    "swap_labels_randomly = False\n",
    "\n",
    "train_d_g_conditional = False # switch between training D and G based on set threshold\n",
    "d_g_conditional_threshold = 0.55 # D_G_z1 < threshold, train G\n",
    "\n",
    "train_d_g_conditional_per_epoch = False\n",
    "\n",
    "train_d_g_conditional_per_n_iters = False\n",
    "train_d_g_n_iters = 2 # When 2, train D 2 times before training G 1 time\n",
    "\n",
    "use_saved_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  64\n",
      "Number of GPUs used:  2\n",
      "Number of images:  50000\n"
     ]
    }
   ],
   "source": [
    "print('Batch size: ', batch_size)\n",
    "ngpu = len(selected_gpus)\n",
    "print('Number of GPUs used: ', ngpu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load data and prepare DataLoader\n",
    "\"\"\"\n",
    "shuffle = True\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(images) # shuffles the images\n",
    "\n",
    "images = images[:int(len(images)*0.1)] # use only first ... percent of the data (0.05)\n",
    "print('Number of images: ', len(images))\n",
    "\n",
    "dataset = numpy_dataset(data=images, to_vram=True) # to_vram pins it to all GPU's\n",
    "#dataset = numpy_dataset(data=images, to_vram=True, transform=transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])) # to_vram pins it to all GPU's\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load and setup models\n",
    "\"\"\"\n",
    "# Initialize cuda\n",
    "device = torch.device(\"cuda:\"+str(selected_gpus[0]) if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Load models\n",
    "netG = model.Generator(ngpu).to(device)\n",
    "netD = model.Discriminator(ngpu).to(device)\n",
    "\n",
    "# Apply weights\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netG.apply(weights_init) # It's not clean/efficient to load these ones first, but it works.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "if use_saved_weights:\n",
    "    try:\n",
    "        # Load saved weights\n",
    "        netG.load_state_dict(torch.load('netG_state_dict2', map_location=device)) #net.module..load_... for parallel model , net.load_... for single gpu model\n",
    "        netD.load_state_dict(torch.load('netD_state_dict2', map_location=device))\n",
    "        print('Succesfully loaded saved weights.')\n",
    "    except:\n",
    "        print('Could not load saved weights, using new ones.')\n",
    "        pass\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, device_ids=selected_gpus, output_device=device)\n",
    "    netD = nn.DataParallel(netD, device_ids=selected_gpus, output_device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define input training stuff (fancy this up)\n",
    "\"\"\"\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) # should be sgd\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(beta1, 0.999))\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "switch = True # condition switch, to switch between D and G per epoch\n",
    "previous_switch = 0\n",
    "\n",
    "train_D = True\n",
    "train_G = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data, b_size):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n",
    "    \"\"\"\n",
    "    alpha = torch.rand(b_size, 1)\n",
    "    alpha = alpha.expand(b_size, int(real_data.nelement()/b_size)).contiguous()\n",
    "    alpha = alpha.view(b_size, 1, image_size, image_size)\n",
    "    alpha = alpha.to(device)\n",
    "    \n",
    "    fake_data = fake_data.view(b_size, 1, image_size, image_size)\n",
    "    interpolates = alpha * real_data.detach() + ((1 - alpha) * fake_data.detach())\n",
    "\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates.requires_grad_(True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)                              \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][0/782] G loss: -0.503 \t D loss: 0.469 \t D(x) = 0.180 \t D(G(z)) = 0.459 \t w_dist = 0.278 \t t = 0.207 \t\n",
      "[0/10][16/782] G loss: -0.471 \t D loss: 0.490 \t D(x) = 0.210 \t D(G(z)) = 0.466 \t w_dist = 0.256 \t t = 2.884 \t\n",
      "[0/10][32/782] G loss: -0.451 \t D loss: 0.411 \t D(x) = 0.206 \t D(G(z)) = 0.423 \t w_dist = 0.217 \t t = 2.874 \t\n",
      "[0/10][48/782] G loss: -0.443 \t D loss: 0.431 \t D(x) = 0.247 \t D(G(z)) = 0.443 \t w_dist = 0.195 \t t = 2.864 \t\n",
      "[0/10][64/782] G loss: -0.516 \t D loss: 5.442 \t D(x) = 0.011 \t D(G(z)) = 0.116 \t w_dist = 0.105 \t t = 2.874 \t\n",
      "[0/10][80/782] G loss: -0.388 \t D loss: 1.764 \t D(x) = 0.090 \t D(G(z)) = 0.375 \t w_dist = 0.285 \t t = 2.875 \t\n",
      "[0/10][96/782] G loss: -0.362 \t D loss: 0.618 \t D(x) = 0.147 \t D(G(z)) = 0.353 \t w_dist = 0.206 \t t = 2.872 \t\n",
      "[0/10][112/782] G loss: -0.444 \t D loss: 0.493 \t D(x) = 0.252 \t D(G(z)) = 0.457 \t w_dist = 0.205 \t t = 2.877 \t\n",
      "[0/10][128/782] G loss: -0.479 \t D loss: 0.481 \t D(x) = 0.266 \t D(G(z)) = 0.462 \t w_dist = 0.197 \t t = 2.870 \t\n",
      "[0/10][144/782] G loss: -0.517 \t D loss: 0.430 \t D(x) = 0.367 \t D(G(z)) = 0.493 \t w_dist = 0.125 \t t = 2.880 \t\n",
      "[0/10][160/782] G loss: -0.488 \t D loss: 0.369 \t D(x) = 0.243 \t D(G(z)) = 0.466 \t w_dist = 0.223 \t t = 2.868 \t\n",
      "[0/10][176/782] G loss: -0.493 \t D loss: 0.398 \t D(x) = 0.389 \t D(G(z)) = 0.517 \t w_dist = 0.129 \t t = 2.880 \t\n",
      "[0/10][192/782] G loss: -0.473 \t D loss: 0.439 \t D(x) = 0.288 \t D(G(z)) = 0.494 \t w_dist = 0.206 \t t = 2.855 \t\n",
      "[0/10][208/782] G loss: -0.562 \t D loss: 0.174 \t D(x) = 0.517 \t D(G(z)) = 0.578 \t w_dist = 0.062 \t t = 2.886 \t\n",
      "[0/10][224/782] G loss: -0.571 \t D loss: 0.178 \t D(x) = 0.549 \t D(G(z)) = 0.576 \t w_dist = 0.026 \t t = 2.853 \t\n",
      "[0/10][240/782] G loss: -1.000 \t D loss: 10.000 \t D(x) = 1.000 \t D(G(z)) = 1.000 \t w_dist = 0.000 \t t = 2.874 \t\n",
      "[0/10][256/782] G loss: -1.000 \t D loss: 10.000 \t D(x) = 1.000 \t D(G(z)) = 1.000 \t w_dist = 0.000 \t t = 2.875 \t\n",
      "[0/10][272/782] G loss: -1.000 \t D loss: 10.000 \t D(x) = 1.000 \t D(G(z)) = 1.000 \t w_dist = 0.000 \t t = 2.849 \t\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Highly adapted from: https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n",
    "\"\"\"\n",
    "\n",
    "g_iters = 1 # 5\n",
    "d_iters = 5 # 1, discriminator is called critic in WGAN paper\n",
    "\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = one * -1\n",
    "\n",
    "iters = 0\n",
    "t1 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        real = data.to(device)\n",
    "        b_size = real.size(0)\n",
    "        \n",
    "        \"\"\"\n",
    "        Train G\n",
    "        \"\"\"\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        for _ in range(g_iters):\n",
    "            netG.zero_grad()\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            noise.requires_grad_(True)\n",
    "            fake = netG(noise)\n",
    "\n",
    "            g_cost = netD(fake).mean()\n",
    "            g_cost.backward(mone)\n",
    "            g_cost = -g_cost\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        \"\"\"\n",
    "        Train D\n",
    "        \"\"\"\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad_(True)\n",
    "\n",
    "        for _ in range(d_iters):\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # generate fake data\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                noisev = noise # Freeze G, training D\n",
    "\n",
    "            fake = netG(noisev).detach()\n",
    "\n",
    "            # train with real data\n",
    "            d_real = netD(real).mean()\n",
    "\n",
    "            # train with fake data\n",
    "            d_fake = netD(fake).mean()\n",
    "\n",
    "            # train with interpolates data\n",
    "            gradient_penalty = calc_gradient_penalty(netD, real, fake, b_size)\n",
    "\n",
    "             # final disc cost\n",
    "            d_cost = d_fake - d_real + gradient_penalty\n",
    "            d_cost.backward()\n",
    "            w_dist = d_fake  - d_real # wasserstein distance\n",
    "            optimizerD.step()\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        weights_saved = False\n",
    "        if (iters % 100 == 0): # save weights every % .... iters\n",
    "            #print('weights saved')\n",
    "            if ngpu > 1:\n",
    "                torch.save(netG.module.state_dict(), 'netG_state_dict2')\n",
    "                torch.save(netD.module.state_dict(), 'netD_state_dict2')\n",
    "            else:\n",
    "                torch.save(netG.state_dict(), 'netG_state_dict2')\n",
    "                torch.save(netD.state_dict(), 'netD_state_dict2')\n",
    "            \n",
    "        \n",
    "        if i % (16) == 0:\n",
    "            t2 = time.time()\n",
    "            print('[%d/%d][%d/%d] G loss: %.3f \\t D loss: %.3f \\t D(x) = %.3f \\t D(G(z)) = %.3f \\t w_dist = %.3f \\t t = %.3f \\t'% \n",
    "                      (epoch, num_epochs, i, len(dataloader), g_cost, d_cost, d_real, d_fake, w_dist, (t2-t1)))\n",
    "            t1 = time.time()\n",
    "                \n",
    "        iters += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the model\n",
    "\"\"\"\n",
    "iters = 0\n",
    "\n",
    "t1 = t.time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    q = np.random.randint(3, 6)\n",
    "    \n",
    "    if train_d_g_conditional_per_epoch:\n",
    "        if switch == True:\n",
    "            train_D = True\n",
    "            train_G = False\n",
    "            switch = False\n",
    "        else:\n",
    "            train_G = True\n",
    "            train_D = False\n",
    "            switch = True\n",
    "            \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_cpu = data.to(device) # for PIL images\n",
    "        b_size = real_cpu.size(0)\n",
    "\n",
    "        \"\"\"\n",
    "        https://github.com/soumith/ganhacks\n",
    "        implement random label range from 0.0-0.3 to 0.7-1.2 for fake and real respectively\n",
    "        \"\"\"\n",
    "        low = 0.01\n",
    "        high = 0.2 #0.3\n",
    "        fake_label = (low - high) * torch.rand(1) + high # uniform random dist between low and high\n",
    "        fake_label = fake_label.data[0] # Gets the variable out of the tensor\n",
    "\n",
    "        low = 0.8 #0.7\n",
    "        high = 1.0\n",
    "        real_label = (low - high) * torch.rand(1) + high # uniform random dist between low and high\n",
    "        real_label = real_label.data[0] # Gets the variable out of the tensor\n",
    "\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        \n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "\n",
    "        if swap_labels_randomly:\n",
    "            if i % q == 0:\n",
    "                labels_inverted = 'yes' \n",
    "                label.fill_(fake_label)\n",
    "            else:\n",
    "                labels_inverted = 'no'\n",
    "        \n",
    "        label.fill_(real_label)\n",
    "        labels_inverted = 'no'\n",
    "        label.fill_(real_label)\n",
    "        \n",
    "        #train_d_g_conditional_per_n_iters = True\n",
    "        #train_d_g_n_iters = 10\n",
    "        \n",
    "        if train_d_g_conditional_per_n_iters:\n",
    "            if previous_switch + train_d_g_n_iters < i:\n",
    "                train_G = True\n",
    "                train_D = False\n",
    "                previous_switch = i\n",
    "            else:\n",
    "                train_G = True\n",
    "                train_D = False\n",
    "            \n",
    "        \n",
    "        \n",
    "        if train_d_g_conditional:\n",
    "            if i > 1:\n",
    "                if D_G_z1 < d_g_conditional_threshold: # 45\n",
    "                    train_G = True\n",
    "                    train_D = False\n",
    "                else:\n",
    "                    train_D = True\n",
    "                    train_G = False\n",
    "                    \n",
    "        if train_D:\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            ## Train with all-real batch\n",
    "            netD.zero_grad()\n",
    "            \n",
    "        # Forward pass real batch through D\n",
    "        #print('real_cpu shape: ', real_cpu.shape)\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label) ## make this fake label sometimes\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "\n",
    "        #if i % 11: # show first image of the real dataset every ... iterations\n",
    "        #    print(fake.shape)\n",
    "        #    plt.imshow(fake.reshape(32, 32))\n",
    "        #    plt.show()\n",
    "        \n",
    "        # swap labels for the discriminator when i % q == 0 (so once every q-th batch)\n",
    "        #if i % q == 0:\n",
    "        #    label.fill_(real_label)\n",
    "        #else:\n",
    "        #    label.fill_(fake_label)\n",
    "\n",
    "        label.fill_(fake_label) ## make this real label sometimes\n",
    "        \n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        \n",
    "        if train_D:\n",
    "            optimizerD.step()\n",
    "        \n",
    "        if train_G:\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            \n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "        if train_G:\n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "                \n",
    "        t2 = t.time()\n",
    "        # Output training stats\n",
    "        \n",
    "        if train_G and train_D:\n",
    "            training_dg = 'D & G'\n",
    "        elif train_G:\n",
    "            training_dg = 'G'\n",
    "        elif train_D:\n",
    "            training_dg = '\\t D'\n",
    "        \n",
    "        if iters % (20) == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f\\t Time: %.2f \\t q: %s \\t training: %s'\n",
    "                    % (epoch, num_epochs, i, len(dataloader),\n",
    "                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2, (t2-t1), q, training_dg))\n",
    "            t1 = t.time()\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 20 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "        if (iters % 100 == 0): # save weights every % .... iters\n",
    "            if ngpu > 1:\n",
    "                torch.save(netG.module.state_dict(), 'netG_state_dict2')\n",
    "                torch.save(netD.module.state_dict(), 'netD_state_dict2')\n",
    "                print('weights saved')\n",
    "            else:\n",
    "                torch.save(netG.state_dict(), 'netG_state_dict2')\n",
    "                torch.save(netD.state_dict(), 'netD_state_dict2')\n",
    "                print('weights saved')\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
