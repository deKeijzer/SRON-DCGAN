{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  1\n",
      "Number of GPUs used:  1\n",
      "Number of images:  1\n",
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import time as t\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import model\n",
    "from keijzer_exogan import *\n",
    "\n",
    "# initialize random seeds\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Local variables\n",
    "\"\"\"\n",
    "workers = 0 # Number of workers for dataloader, 0 when to_vram is enabled\n",
    "batch_size = 1 # using one image ofcourse\n",
    "image_size = 32\n",
    "nz = 100 # size of latent vector\n",
    "n_iters = 1*10**4 # number of iterations to do for inpainting\n",
    "torch.backends.cudnn.benchmark=True # Uses udnn auto-tuner to find the best algorithm to use for your hardware, speeds up training by almost 50%\n",
    "lr = 1e-2\n",
    "lr_G = 2e-4\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "selected_gpus = [3] # Number of GPUs available. Use 0 for CPU mode.\n",
    "\n",
    "path = '/datb/16011015/ExoGAN_data/selection//' #notice how you dont put the last folder in here...\n",
    "images = np.load(path+'first_chunks_25_percent_images.npy')\n",
    "\n",
    "print('Batch size: ', batch_size)\n",
    "\n",
    "\n",
    "# Number of training epochs\n",
    "\n",
    "# Learning rate for optimizers\n",
    "ngpu = len(selected_gpus)\n",
    "print('Number of GPUs used: ', ngpu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load data and prepare DataLoader\n",
    "\"\"\"\n",
    "shuffle = True\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(images) # shuffles the images\n",
    "\n",
    "images = images[:int(len(images)*0.0000025)] # use only first ... percent of the data (0.05)\n",
    "print('Number of images: ', len(images))\n",
    "\n",
    "dataset = numpy_dataset(data=images, to_vram=True) # to_vram pins it to all GPU's\n",
    "#dataset = numpy_dataset(data=images, to_vram=True, transform=transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])) # to_vram pins it to all GPU's\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)\n",
    "\n",
    "\"\"\"\n",
    "Load and setup models\n",
    "\"\"\"\n",
    "# Initialize cuda\n",
    "device = torch.device(\"cuda:\"+str(selected_gpus[0]) if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Load models, set to evaluation mode since training is not needed (this also allows batchsize 1 to work with batchnorm2d layers)\n",
    "netG = model.Generator(ngpu).eval().to(device)\n",
    "netD = model.Discriminator(ngpu).eval().to(device)\n",
    "\n",
    "# Apply weights\n",
    "print('Loading weights...')\n",
    "try:\n",
    "    # Load saved weights\n",
    "    netG.load_state_dict(torch.load('netG_state_dict2', map_location=device)) #net.module..load_... for parallel model , net.load_... for single gpu model\n",
    "    netD.load_state_dict(torch.load('netD_state_dict2', map_location=device))\n",
    "except:\n",
    "    print('Could not load saved weights.')\n",
    "    sys.exit()\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, device_ids=selected_gpus, output_device=device)\n",
    "    netD = nn.DataParallel(netD, device_ids=selected_gpus, output_device=device)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define input training stuff (fancy this up)\n",
    "\"\"\"\n",
    "G = netG\n",
    "D = netD\n",
    "z = torch.randn(1, nz, 1, 1, requires_grad=True, device=device)\n",
    "\n",
    "lamb = 3e-1\n",
    "\n",
    "criteria = nn.BCELoss()\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) # should be sgd\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show a generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9305cc8fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z2 = torch.randn(1, nz, 1, 1, device=device)\n",
    "t = G(z2).detach().cpu()[0, 0, :, :]\n",
    "# TODO: why does this already look real?\n",
    "plt.imshow(t, cmap='gray') # plot the masked image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting\n",
    "The corrupted image $y$ is mapped to the closest $z$ in the latent representation space, this mapping is denoted as $\\hat{z}$.\n",
    "    \n",
    "$\\hat{z} = \\operatorname{arg\\,min}_z \\{ \\mathcal{L}_c(z |y, M) + \\mathcal{L}_p (z) \\}$\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathcal{L}_c(z |y, M) = || M \\bigodot G(z) - M \\bigodot y||_1 = || M \\bigodot (G(z)-y) ||_1 $\n",
    "\n",
    "with $\\mathcal{L}_c$ being contextual loss and $M$ being a binary mask with the same size as $y$,\n",
    "\n",
    "$\\mathcal{L}_p (z) = \\lambda \\operatorname{log}(1-D(G(z)))$\n",
    "\n",
    "with $\\mathcal{L}_p$ being perceptual loss and $D$ being the discriminator.\n",
    "  \n",
    "Once $G(\\hat{z})$ is generated, the final solution $\\hat{x}$ is calculated as\n",
    "\n",
    "$\\hat{x} = \\operatorname{arg\\, min}_x ||\\nabla x - \\nabla G(\\hat{z}) ||^2_2$  \n",
    "\n",
    "(substitute $x_i = y_i$ for $M_i = 1$).\n",
    "\n",
    "-----\n",
    "\n",
    "$|| ... ||$ is done by `torch.norm()`.  \n",
    "$... \\bigodot ...$ is done by `torch.mul()`.  \n",
    "-----\n",
    "TODO: Implement $\\hat{x} = \\operatorname{arg\\, min}_x ||\\nabla x - \\nabla G(\\hat{z}) ||^2_2$    \n",
    "Currently $\\hat{x} = G(\\hat{z}) \\bigodot (1 -M)+y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "real_images = []\n",
    "masked_images= []\n",
    "inpainted_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape:  torch.Size([1, 1, 32, 32])\n",
      "image.shape:  torch.Size([1, 1, 32, 32])\n",
      "mask.shape torch.Size([1, 1, 32, 32])\n",
      "masked image shape torch.Size([1, 1, 32, 32])\n",
      " iteration : 4402 , context_loss: 162.0080, perceptual_loss: 0.000000, total_loss: 162.007980"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-44c1f0fdeff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m\"\"\"Calculate losses\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_generated_masked\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmasked_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#what's p=1?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mdiscriminator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_generated_inpainted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m#print(\"Discriminator output: \", discriminator_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/SRON-DCGAN/notebooks/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Inpainting\n",
    "\"\"\"\n",
    "for i, data in enumerate(dataloader, 0): # batches per epoch\n",
    "    real_cpu = data.to(device)\n",
    "    b_size = real_cpu.size(0) # this is one ofc, it's one image we're trying to inpaint\n",
    "\n",
    "    print(\"data.shape: \", data.shape)\n",
    "    image = data.to(device) # select the image (Channel, Height, Width), this is the original unmasked input image\n",
    "    real_images.append(image)\n",
    "    print(\"image.shape: \", image.shape)\n",
    "\n",
    "    \"\"\"Mask the images manually, for testing pruposes\"\"\"\n",
    "    mask = torch.ones(size=image.shape).to(device) # create mask with 1's in the shape of image\n",
    "    \n",
    "    print(\"mask.shape\", mask.shape)\n",
    "\n",
    "    # use a random 'easy' mask\n",
    "    mask[:, :, 10:20, 5:15] = 0\n",
    "\n",
    "    masked_image = torch.mul(image, mask).to(device) #image bigodot mask\n",
    "    masked_images.append(masked_image)\n",
    "    print('masked image shape', masked_image.shape)\n",
    "    plt.imshow(masked_image.detach().cpu()[0, 0, :, :], cmap='gray') # plot the masked image\n",
    "\n",
    "    opt = optim.Adam([z], lr=lr)\n",
    "\n",
    "    # what's v and m?\n",
    "    v = torch.tensor(0, dtype=torch.float32, device=device)\n",
    "    m = torch.tensor(0, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "    \"\"\"Start the inpainting process\"\"\"\n",
    "    for iteration in range(n_iters):\n",
    "        if z.grad is not None:\n",
    "            z.grad.data.zero_()\n",
    "\n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "\n",
    "\n",
    "        image_generated = G(z) # generated image G(z)\n",
    "        image_generated_masked = torch.mul(image_generated, mask) # G(z) bigodot M\n",
    "        image_generated_inpainted = torch.mul(image_generated, (1-mask))+masked_image\n",
    "        \n",
    "        if (iteration % 100 == 0):\n",
    "            inpainted_images.append(image_generated_inpainted)\n",
    "\n",
    "        #print(\"image_generated_inpainted.shape : \",image_generated_inpainted.shape)\n",
    "\n",
    "        t = image_generated_inpainted.detach().cpu()[0, 0, :, :]\n",
    "\n",
    "        # TODO: why does this already look real?\n",
    "        plt.imshow(t, cmap='gray') # plot the masked image \n",
    "\n",
    "        \"\"\"Calculate losses\"\"\"\n",
    "        loss_context = torch.norm(image_generated_masked-masked_image, p=1) #what's p=1?\n",
    "        discriminator_output = netD(image_generated_inpainted)\n",
    "        #print(\"Discriminator output: \", discriminator_output)\n",
    "\n",
    "        labels = torch.full((b_size,), 1, device=device)\n",
    "        loss_perceptual = torch.log(1-discriminator_output)\n",
    "        \n",
    "        #print(loss_perceptual.data.cpu().numpy().flatten()[0])\n",
    "\n",
    "        total_loss = loss_context + lamb*loss_perceptual\n",
    "        \n",
    "        # grab the values from losses for printing\n",
    "        loss_perceptual = loss_perceptual.data.cpu().numpy().flatten()[0]\n",
    "        loss_context = loss_context.data.cpu().numpy().flatten()[0]\n",
    "\n",
    "\n",
    "\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss = total_loss.data.cpu().numpy().flatten()[0]\n",
    "        \n",
    "        print(\"\\r iteration : {:4} , context_loss: {:.4f}, perceptual_loss: {:4f}, total_loss: {:4f}\".format(iteration,loss_context,loss_perceptual, total_loss),end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = real_images[0].detach().cpu()[0, 0, :, :]\n",
    "real_masked = masked_images[0].detach().cpu()[0, 0, :, :]\n",
    "\n",
    "first_generated =inpainted_images[1].detach().cpu()[0, 0, :, :]\n",
    "last_generated = inpainted_images[-1].detach().cpu()[0, 0, :, :]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(real, cmap='gray')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(real_masked, cmap='gray')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(first_generated, cmap='gray')\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(last_generated, cmap='gray')\n",
    "\n",
    "plt.imshow\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['animation.embed_limit'] = 20**128\n",
    "\n",
    "#%%capture\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(i.detach().cpu()[0, 0, :, :], animated=True, cmap='gray')] for i in inpainted_images]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
