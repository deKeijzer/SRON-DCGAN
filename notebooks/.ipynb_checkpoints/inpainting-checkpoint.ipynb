{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import model\n",
    "from keijzer_exogan import *\n",
    "\n",
    "# initialize random seeds\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"} # Make sure the axis background of plots is white, this is usefull for the black theme in JupyterLab\n",
    "# Initialize default seaborn layout\n",
    "sns.set_palette(sns.hls_palette(8, l=.3, s=.8))\n",
    "sns.set(style='ticks')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Local variables\n",
    "\"\"\"\n",
    "workers = 0 # Number of workers for dataloader, 0 when to_vram is enabled\n",
    "batch_size = 1 # using one image ofcourse\n",
    "image_size = 32\n",
    "nz = 100 # size of latent vector\n",
    "n_iters = 25*10**3 # number of iterations to do for inpainting\n",
    "torch.backends.cudnn.benchmark=True # Uses udnn auto-tuner to find the best algorithm to use for your hardware, speeds up training by almost 50%\n",
    "\n",
    "lr = 1e-2\n",
    "lamb1 = 1e4 #1e4\n",
    "lamb2 = 1 # 1 , total_loss = lamb1*loss_context + lamb2*loss_perceptual\n",
    "\n",
    "lr_G = 2e-4\n",
    "beta1 = 0.5 # Beta1 hyperparam for Adam optimizers\n",
    "selected_gpus = [3] # Number of GPUs available. Use 0 for CPU mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/datb/16011015/ExoGAN_data/selection//' #notice how you dont put the last folder in here...\n",
    "\n",
    "images = np.load(path+'last_chunks_25_percent_images.npy').astype('float32')\n",
    "images = images[:25] # select first ... images\n",
    "\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  1\n",
      "Number of GPUs used:  1\n",
      "Number of images:  25\n"
     ]
    }
   ],
   "source": [
    "print('Batch size: ', batch_size)\n",
    "\n",
    "\n",
    "# Number of training epochs\n",
    "\n",
    "# Learning rate for optimizers\n",
    "ngpu = len(selected_gpus)\n",
    "print('Number of GPUs used: ', ngpu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load data and prepare DataLoader\n",
    "\"\"\"\n",
    "shuffle = True\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(images) # shuffles the images\n",
    "\n",
    "images = images[:25] # 1200 should take ~ 1 hour\n",
    "#images = images[:int(len(images)*0.005)]\n",
    "print('Number of images: ', len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gan_data//weights//netD_state_dict0_v2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-27ee34e97d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Load saved weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gan_data//weights//netG_state_dict0_v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#net.module..load_... for parallel model , net.load_... for single gpu model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gan_data//weights//netD_state_dict0_v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#    print('Could not load saved weights.')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gan_data//weights//netD_state_dict0_v2'"
     ]
    }
   ],
   "source": [
    "dataset = numpy_dataset(data=images, to_vram=True) # to_vram pins it to all GPU's\n",
    "#dataset = numpy_dataset(data=images, to_vram=True, transform=transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])) # to_vram pins it to all GPU's\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)\n",
    "\n",
    "\"\"\"\n",
    "Load and setup models\n",
    "\"\"\"\n",
    "# Initialize cuda\n",
    "device = torch.device(\"cuda:\"+str(selected_gpus[0]) if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Load models, set to evaluation mode since training is not needed (this also allows batchsize 1 to work with batchnorm2d layers)\n",
    "netG = model.Generator(ngpu).eval().to(device)\n",
    "netD = model.Discriminator(ngpu).eval().to(device)\n",
    "\n",
    "# Apply weights\n",
    "print('Loading weights...')\n",
    "try:\n",
    "    # Load saved weights\n",
    "    netG.load_state_dict(torch.load('gan_data//weights//netG_state_dict0_v2', map_location=device)) #net.module..load_... for parallel model , net.load_... for single gpu model\n",
    "    netD.load_state_dict(torch.load('gan_data//weights//netD_state_dict0_v2', map_location=device))\n",
    "except:\n",
    "    print('Could not load saved weights.')\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define input training stuff (fancy this up)\n",
    "\"\"\"\n",
    "G = netG\n",
    "D = netD\n",
    "z = torch.randn(1, nz, 1, 1, requires_grad=True, device=device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    G = nn.DataParallel(G, device_ids=selected_gpus, output_device=device)\n",
    "    D = nn.DataParallel(D, device_ids=selected_gpus, output_device=device)\n",
    "    #z = nn.DataParallel(z, device_ids=selected_gpus, output_device=device)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) # should be sgd\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(beta1, 0.999))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "z_tests = [torch.randn(1, nz, 1, 1, device=device) for _ in range(9)]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(9):\n",
    "    img = G(z_tests[i]).detach().cpu()[0, 0, :, :]\n",
    "    plt.subplot(3,3,i+1)\n",
    "    #scaler = MinMaxScaler((0, 1.2))\n",
    "    #img = scaler.fit_transform(img)\n",
    "    plt.imshow(img, cmap='gray', vmin=0, vmax=1.2)\n",
    "    #plt.imshow(img, cmap='gray')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.min(), img.max(), img.mean(), img.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure Z is different for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i,z_t in enumerate(z_tests):\n",
    "    z_t = z_t.detach().cpu().numpy()\n",
    "    \n",
    "    plt.subplot(3,3,i+1)\n",
    "    \n",
    "    plt.hist(z_t.flatten())\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #print(z_t[0].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show first 9 selected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(9):\n",
    "    try:\n",
    "        img = images[i]\n",
    "        plt.subplot(3,3,i+1)\n",
    "        plt.imshow(img[0, :, :], cmap='gray', vmin=0, vmax=1.2)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.min(), img.max(), img.mean(), img.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the weights for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [param.data.cpu().numpy().flatten() for param in netD.parameters()]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i,layer_weights in enumerate(weights):\n",
    "    print('Layer: %s \\t n_weights: %s \\t std: %.4f \\t mean: %.4f' % (i, len(layer_weights), layer_weights.std(), layer_weights.mean()))\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.title('netD layer %s weights' % i)\n",
    "    plt.hist(layer_weights, bins=100)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [param.data.cpu().numpy().flatten() for param in netG.parameters()] # where param.data are the weights of the i-th layer\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i,layer_weights in enumerate(weights):\n",
    "    print('Layer: %s \\t n_weights: %s \\t std: %.4f \\t mean: %.4f' % (i, len(layer_weights), layer_weights.std(), layer_weights.mean()))\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.title('netG layer %s weights' % i)\n",
    "    plt.hist(layer_weights, bins=100)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting\n",
    "The corrupted image $y$ is mapped to the closest $z$ in the latent representation space, this mapping is denoted as $\\hat{z}$.\n",
    "    \n",
    "$\\hat{z} = \\operatorname{arg\\,min}_z \\{ \\mathcal{L}_c(z |y, M) + \\mathcal{L}_p (z) \\}$\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathcal{L}_c(z |y, M) = || M \\bigodot G(z) - M \\bigodot y||_1 = || M \\bigodot (G(z)-y) ||_1 $\n",
    "\n",
    "with $\\mathcal{L}_c$ being contextual loss and $M$ being a binary mask with the same size as $y$,\n",
    "\n",
    "$\\mathcal{L}_p (z) = \\lambda \\operatorname{log}(1-D(G(z)))$\n",
    "\n",
    "with $\\mathcal{L}_p$ being perceptual loss and $D$ being the discriminator.\n",
    "  \n",
    "Once $G(\\hat{z})$ is generated, the final solution $\\hat{x}$ is calculated as\n",
    "\n",
    "$\\hat{x} = \\operatorname{arg\\, min}_x ||\\nabla x - \\nabla G(\\hat{z}) ||^2_2$  \n",
    "\n",
    "(substitute $x_i = y_i$ for $M_i = 1$).\n",
    "\n",
    "-----\n",
    "\n",
    "$|| ... ||$ is done by `torch.norm()`.  \n",
    "$... \\bigodot ...$ is done by `torch.mul()`.  \n",
    "-----\n",
    "TODO: Implement $\\hat{x} = \\operatorname{arg\\, min}_x ||\\nabla x - \\nabla G(\\hat{z}) ||^2_2$    \n",
    "Currently $\\hat{x} = G(\\hat{z}) \\bigodot (1 -M)+y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.read_csv('wavelengths_and_indices.csv')  # where x is the wavelength\n",
    "\n",
    "ind[(ind.x > 10) & (ind.x <= 14)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask():\n",
    "    \n",
    "    mask = np.full([1,1,32,32], 1) # init array with 0.5's\n",
    "    mask = torch.from_numpy(mask).to(device)\n",
    "    \n",
    "    #mask = torch.ones([1, 1, 32, 32]).to(device) # create mask with 1's in the shape of image\n",
    "    \n",
    "    #print(\"mask.shape\", mask.shape)\n",
    "\n",
    "    # use a random 'easy' mask\n",
    "    mask[:, :, :16, 25:] = 0\n",
    "    \n",
    "    # The noise doesn't matter for conceptual loss\n",
    "    mask[:, :, 19:, :] = 0\n",
    "    \n",
    "    \"\"\"Weighted mask\"\"\"\n",
    "    \n",
    "    # Normalization factors\n",
    "    mask[:, :, 16:19, :] = 2\n",
    "    \n",
    "    mask = mask.float() # make sure dtype is float, torch was complaining during inpainting that this is a double\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = create_mask().cpu()[0, 0, :, :]\n",
    "plt.imshow(m, cmap='gray', vmin=0, vmax=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = m.cpu().numpy().flatten()\n",
    "\n",
    "for i in range(len(t)):\n",
    "    if np.isnan(t[i]):\n",
    "        print('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "real_images = []\n",
    "masked_images= []\n",
    "inpainted_images = []\n",
    "final_inpainted_images = [] # last inpainted image for each input image\n",
    "n_iteration = []\n",
    "\n",
    "MSELoss = nn.MSELoss()\n",
    "L1Loss = nn.L1Loss() # MAE\n",
    "SmoothL1Loss = nn.SmoothL1Loss()\n",
    "\n",
    "\"\"\"\n",
    "Inpainting\n",
    "\"\"\"\n",
    "for i, data in enumerate(dataloader, 0): # batches per epoch\n",
    "    real_cpu = data.to(device)\n",
    "    b_size = real_cpu.size(0) # this is one ofc, it's one image we're trying to inpaint\n",
    "\n",
    "    #print(\"data.shape: \", data.shape)\n",
    "    image = data.to(device) # select the image (Channel, Height, Width), this is the original unmasked input image\n",
    "    \n",
    "    real_images.append(image)\n",
    "    #print(\"image.shape: \", image.shape)\n",
    "\n",
    "    \"\"\"Mask the image\"\"\"\n",
    "    mask = create_mask()\n",
    "\n",
    "    masked_image = torch.mul(image, mask).to(device) #image bigodot mask\n",
    "    masked_images.append(masked_image)\n",
    "    #print('masked image shape', masked_image.shape)\n",
    "    #plt.imshow(masked_image.detach().cpu()[0, 0, :, :], cmap='gray') # plot the masked image\n",
    "\n",
    "    opt = optim.Adam([z], lr=lr)\n",
    "\n",
    "    # what's v and m?\n",
    "    v = torch.tensor(0, dtype=torch.float32, device=device)\n",
    "    m = torch.tensor(0, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "    \"\"\"Start the inpainting process\"\"\"\n",
    "    early_stopping_n_iters = 0\n",
    "    early_stopping_min_loss = 999999 # set to random high number to initialize\n",
    "    for iteration in range(n_iters):\n",
    "        t1 = time.time()\n",
    "        if z.grad is not None:\n",
    "            z.grad.data.zero_()\n",
    "\n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "\n",
    "\n",
    "        image_generated = G(z) # generated image G(z)\n",
    "        image_generated_masked = torch.mul(image_generated, mask) # G(z) bigodot M\n",
    "        image_generated_inpainted = torch.mul(image_generated, (1-mask))+masked_image\n",
    "        \n",
    "        if (iteration % 100 == 0):\n",
    "            inpainted_images.append(image_generated_inpainted)\n",
    "\n",
    "        #print(\"image_generated_inpainted.shape : \",image_generated_inpainted.shape)\n",
    "\n",
    "        t = image_generated_inpainted.detach().cpu()[0, 0, :, :]\n",
    "\n",
    "        # TODO: why does this already look real?\n",
    "        #plt.imshow(t, cmap='gray') # plot the masked image \n",
    "\n",
    "        \"\"\"Calculate losses\"\"\"\n",
    "        #loss_context = torch.norm(image_generated_masked-masked_image, p=1) #what's p=1?\n",
    "        loss_context = lamb1*MSELoss(image_generated_masked,masked_image)\n",
    "        #loss_context = L1Loss(image_generated_masked, masked_image)*10\n",
    "        #loss_context = SmoothL1Loss(image_generated_masked, masked_image)*10\n",
    "        \n",
    "        discriminator_output = netD(image_generated_inpainted) - 0.005 # -0.005 offset so loss_perceptual doesn't become 1 when D(G(z)) == 1.000000\n",
    "        #print(\"Discriminator output: \", discriminator_output)\n",
    "\n",
    "        labels = torch.full((b_size,), 1, device=device)\n",
    "        loss_perceptual = lamb2*torch.log(1-discriminator_output)\n",
    "        \n",
    "        #if loss_perceptual == -np.inf:\n",
    "        #    #print('loss perceptual == -np.inf()')\n",
    "        #    loss_perceptual = torch.tensor(-10, dtype=torch.float32, device=device)\n",
    "        \n",
    "        #print(loss_perceptual.data.cpu().numpy().flatten()[0])\n",
    "\n",
    "        total_loss = loss_context + loss_perceptual\n",
    "        #total_loss = loss_context + 10*discriminator_output\n",
    "        \n",
    "        # grab the values from losses for printing\n",
    "        loss_perceptual = loss_perceptual.data.cpu().numpy().flatten()[0]\n",
    "        #loss_perceptual = 0\n",
    "        loss_context = loss_context.data.cpu().numpy().flatten()[0]\n",
    "\n",
    "\n",
    "\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss = total_loss.data.cpu().numpy().flatten()[0]\n",
    "        \n",
    "        \"\"\"Early stopping\"\"\" # TODO: \n",
    "        if iteration > 0:\n",
    "            delta_loss = early_stopping_min_loss - total_loss\n",
    "            delta_iters = iteration - iter1\n",
    "            \n",
    "            if (delta_loss < 0.001) or (total_loss > early_stopping_min_loss):\n",
    "                early_stopping_n_iters += 1\n",
    "            else:\n",
    "                #print('set to zero')\n",
    "                early_stopping_n_iters = 0\n",
    "            \n",
    "            if early_stopping_n_iters > 1000:\n",
    "                n_iteration.append(iteration)\n",
    "                break\n",
    "            \n",
    "        loss1 = total_loss\n",
    "        iter1 = iteration\n",
    "        \n",
    "        if total_loss < early_stopping_min_loss:\n",
    "            early_stopping_min_loss = total_loss\n",
    "            best_inpained_image = image_generated_inpainted.detach().cpu()\n",
    "            early_stopping_n_iters = 0\n",
    "            #print('min loss: ', early_stopping_min_loss)\n",
    "        \n",
    "        t2 = time.time()\n",
    "        \n",
    "        \"\"\"Calculate ETA\"\"\"\n",
    "        t_per_iter = t2 - t1 # time per iteration in seconds\n",
    "        \n",
    "        eta = t_per_iter * (n_iters - iteration) + t_per_iter* (len(images)-i+1) * n_iters # time left to finish epoch/image + time left to finish all epochs/images in SECONDS\n",
    "        eta_h = (eta/ 60) // 60 # divisor integer\n",
    "        eta_m = eta % 60 # get remainer\n",
    "        \n",
    "        if (iteration % 50 == 0):\n",
    "            print(\"\\r image [{}/{}] iteration : {:4} , context_loss: {:.3f}, perceptual_loss: {:3f}, total_loss: {:3f}, min L: {:3f}, {:1f}, D(G(z)): {:3f}, ETA: {:.0f}h {:.0f}m\".format(i+1, \n",
    "            len(images), iteration, loss_context,loss_perceptual, total_loss,early_stopping_min_loss, early_stopping_n_iters, discriminator_output.data.cpu().numpy().flatten()[0], eta_h, eta_m),end=\"\")\n",
    "            \n",
    "\n",
    "            \n",
    "        \"\"\"NaN monitor\"\"\"\n",
    "        #if (loss_context or loss_perceptual == np.nan()) and iteration >64:\n",
    "        #    print(r'='*10 + '     NaN     '+ '='*10)\n",
    "        #    print(loss_context, loss_percept  ual)\n",
    "            #break\n",
    "        \n",
    "    final_inpainted_images.append(best_inpained_image.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(real_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse transform ASPA to spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scale(X_, X_min, X_max):\n",
    "    \"\"\"\n",
    "    X_ is scaled X\n",
    "    X is unscaled X_\n",
    "    \"\"\"\n",
    "    X = X_ * (X_max - X_min) + X_min\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "aspa = real_images[i].detach().cpu().numpy()[0, 0, :, :]\n",
    "\n",
    "\n",
    "plt.imshow(aspa, cmap='gray', vmin=0, vmax=1.2)\n",
    "\n",
    "\n",
    "\n",
    "n_bins = 8 # amount of bins the spectrum got scaled in\n",
    "\"\"\"Image parts from the aspa, still encoded\"\"\"\n",
    "spectrum = aspa[:16, :25]\n",
    "mins_ = [aspa[16:17, i*4:i*4+4] for i in range(n_bins)]\n",
    "stds_ = [aspa[17:18, i*4:i*4+4] for i in range(n_bins)]\n",
    "maxs_ = [aspa[18:19, i*4:i*4+4] for i in range(n_bins)]\n",
    "\n",
    "\"\"\"Decode to arrays\"\"\"\n",
    "spectrum = spectrum.flatten()\n",
    "\n",
    "mins_ = [inverse_scale(mins_[i].mean(), 0.005, 0.03) for i in range(n_bins)]\n",
    "stds_ = [inverse_scale(stds_[i].mean(), 0.005, 0.03) for i in range(n_bins)]\n",
    "maxs_ = [inverse_scale(maxs_[i].mean(), 1e-7, 1e-4) for i in range(n_bins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wavelengths_and_indices.csv', header=None, skiprows=[0], usecols=[1]) # load wavelengths\n",
    "df.columns = ['x']\n",
    "df = df.loc[df['x'] <= 16] # select only wavelengths <= 16\n",
    "df['y'] = spectrum\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.x, df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"select spectrum bins\"\"\"\n",
    "bin1 = df[df.x <= 0.8]\n",
    "bin2 = df[(df.x > 0.8) & (df.x <= 1.3)] # select data between 2 and 4 micron\n",
    "bin3 = df[(df.x > 1.3) & (df.x <= 2)]\n",
    "bin4 = df[(df.x > 2) & (df.x <= 4)]\n",
    "bin5 = df[(df.x > 4) & (df.x <= 6)]\n",
    "bin6 = df[(df.x > 6) & (df.x <= 10)]\n",
    "bin7 = df[(df.x > 10) & (df.x <= 14)]\n",
    "bin8 = df[df.x > 14]\n",
    "\n",
    "bins = [bin8, bin7, bin6, bin5, bin4, bin3, bin2, bin1]\n",
    "for b in bins:\n",
    "    plt.plot(b['x'], b['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Inverse scale the bins\"\"\"\n",
    "# min max values for spectrum from ASPA v2 notebook\n",
    "min_values = [1.518400e+27, \n",
    "                  1.000000e+03, \n",
    "                  -1.842068e+01, \n",
    "                  5.592880e+07, \n",
    "                  -1.842068e+01, \n",
    "                  -1.842068e+01, \n",
    "                  -1.842068e+01]\n",
    "    \n",
    "max_values = [3.796000e+27, \n",
    "                  2.000000e+03, \n",
    "                  -2.302585e+00, \n",
    "                  1.048665e+08, \n",
    "                  -2.302585e+00, \n",
    "                  -2.302585e+00,\n",
    "                  -2.302585e+00]\n",
    "\n",
    "spectra_inverses = []\n",
    "for i in range(7):\n",
    "    bin_ = bins[i]\n",
    "    max_ = maxs_[i]\n",
    "    min_ = mins_[i]\n",
    "    \n",
    "    spectrum = bin_['y']\n",
    "    spectrum_inverse = inverse_scale(spectrum, min_values[i], max_values[i])\n",
    "    spectra_inverses.append(spectrum_inverse)\n",
    "\n",
    "spectra_inverses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = real_images[0].detach().cpu()[0, 0, :, :]\n",
    "real_masked = masked_images[0].detach().cpu()[0, 0, :, :]\n",
    "\n",
    "#first_generated =inpainted_images[1].detach().cpu()[0, 0, :, :]\n",
    "last_generated = inpainted_images[-1].detach().cpu()[0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22,10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(real, cmap='gray', vmin=0, vmax=1.2)\n",
    "plt.title('Ground truth')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(real_masked, cmap='gray', vmin=0, vmax=1.2)\n",
    "plt.title('Masked input')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(last_generated, cmap='gray', vmin=0, vmax=1.2)\n",
    "plt.title('Inpainted')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('plots/inpainting.png', dpi=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = real - last_generated\n",
    "difference[19:, :] = 0 # remove the noise, it doesn't matter\n",
    "\n",
    "sns.heatmap(difference, cmap='gray')\n",
    "plt.title('Ground truth - Inpainted ')\n",
    "\n",
    "difference  = difference.cpu().detach().numpy().flatten()\n",
    "\n",
    "mean, sigma, min_, max_ = difference.mean().item(), difference.std(), difference.min(), difference.max()\n",
    "print('mean: %.4f \\n std: %.4f \\n min: %.4f \\n max: %.4f' % (mean, sigma, min_, max_))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['animation.embed_limit'] = 20**128\n",
    "\n",
    "#%%capture\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(i.detach().cpu()[0, 0, :, :], animated=True, cmap='gray', vmin=0, vmax=1.2)] for i in inpainted_images]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=1000, blit=True)\n",
    "\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding ASPA\n",
    "Need to clean this up. Using dicts like this is not clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspa_inpainted = last_generated.numpy()\n",
    "aspa_real = real.numpy()\n",
    "\n",
    "aspa_inpainted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ExoGAN params with zero's\n",
    "inpainted_params = {\n",
    "    'planet_mass': 0,\n",
    "    'temp_profile': 0,\n",
    "    'ch4_mixratio': 0,\n",
    "    'planet_radius': 0,\n",
    "    'h2o_mixratio': 0,\n",
    "    'co2_mixratio': 0,\n",
    "    'co_mixratio': 0\n",
    "}\n",
    "\n",
    "real_params = inpainted_params.copy()\n",
    "params_errors = inpainted_params.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse scale functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scale_param(X_scaled, X_min, X_max):\n",
    "    return (X_scaled * (X_max-X_min)) + X_min\n",
    "\n",
    "def inverse_scale_params(params):\n",
    "    \"\"\"\n",
    "    Inverse scales the params array. \n",
    "    Params must be in fixed order, same order as it is encoded with.\n",
    "    \"\"\"\n",
    "    params = params.copy()\n",
    "    min_values = [1.518400e+27, \n",
    "                  1.000000e+03, \n",
    "                  -1.842068e+01, \n",
    "                  5.592880e+07, \n",
    "                  -1.842068e+01, \n",
    "                  -1.842068e+01, \n",
    "                  -1.842068e+01]\n",
    "    \n",
    "    max_values = [3.796000e+27, \n",
    "                  2.000000e+03, \n",
    "                  -2.302585e+00, \n",
    "                  1.048665e+08, \n",
    "                  -2.302585e+00, \n",
    "                  -2.302585e+00,\n",
    "                  -2.302585e+00]\n",
    "\n",
    "    params_inverse_scaled = []\n",
    "    for i in range(len(params)):\n",
    "        param = params[i]\n",
    "        min_= min_values[i]\n",
    "        max_ = max_values[i]\n",
    "\n",
    "        params_inverse_scaled.append(inverse_scale_param(param, min_, max_))\n",
    "    \n",
    "    return params_inverse_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainted params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainted_params_values = [aspa_inpainted[:16, 25+i:26+i] for i in range(len(inpainted_params))]\n",
    "inpainted_params_mean  = [i.mean() for i in inpainted_params_values]\n",
    "\n",
    "inpainted_params_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (0.5876265 * (-2.306--1.842068e+01))+-1.842068e+01\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainted_params_mean = inverse_scale_params(inpainted_params_mean)\n",
    "inpainted_params_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert mixtures from log back to regular scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,param in enumerate(inpainted_params):\n",
    "    if 'mixratio' in param:\n",
    "        inpainted_params[param] = (inpainted_params_mean[i]) # convert log back to regular scale\n",
    "    else:\n",
    "        inpainted_params[param] = inpainted_params_mean[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainted_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real params\n",
    "Ground truth params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_params_values = [aspa_real[:16, 25+i:26+i] for i in range(len(real_params))]\n",
    "real_params_mean  = [i.mean() for i in real_params_values]\n",
    "real_params_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_params_mean = inverse_scale_params(real_params_mean)\n",
    "real_params_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert mixtures from log back to regular scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,param in enumerate(real_params):\n",
    "    if 'mixratio' in param:\n",
    "        real_params[param] = (real_params_mean[i]) # convert log back to regular scale\n",
    "    else:\n",
    "        real_params[param] = real_params_mean[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage error\n",
    "\n",
    "% error = $\\frac{y-\\hat{y}}{\\hat{y}} \\cdot$ 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_errors = []\n",
    "for param in real_params:\n",
    "    y_hat = real_params[param]\n",
    "    y = inpainted_params[param]\n",
    "    \n",
    "    params_errors[param] = ((y - y_hat) / y_hat)*100\n",
    "\n",
    "params_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in inpainted_params:\n",
    "    real = real_params[param]\n",
    "    inpainted = inpainted_params[param]\n",
    "    error = params_errors[param]\n",
    "    \n",
    "    print('%s \\t Real value: %.5e \\t inpainted value: %.5e \\t perc. diff.: %.25f' % (param, real, inpainted, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionize this to output the MAPE of all inpainted images per metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real_images = []\n",
    "#final_inpainted_images = []\n",
    "\n",
    "reals = []\n",
    "inpainteds = []\n",
    "errors = []\n",
    "for i in range(len(real_images)-1):\n",
    "    aspa_real = real_images[i].detach().cpu()[0, 0, :, :].numpy() # size is 32,32 now\n",
    "    aspa_inpainted = final_inpainted_images[i].detach().cpu()[0, 0, :, :].numpy() # size is 32,32 now\n",
    "\n",
    "    \"\"\"Decode inpainted aspa\"\"\"\n",
    "    inpainted_params_values = [aspa_inpainted[:16, 25+j:26+j] for j in range(len(inpainted_params))]\n",
    "    inpainted_params_mean  = [k.mean() for k in inpainted_params_values]\n",
    "    inpainted_params_mean = inverse_scale_params(inpainted_params_mean)\n",
    "    \n",
    "    for a,param in enumerate(inpainted_params):\n",
    "        if 'mixratio' in param:\n",
    "            inpainted_params[param] = inpainted_params_mean[a] # convert log back to regular scale\n",
    "        else:\n",
    "            inpainted_params[param] = inpainted_params_mean[a]\n",
    "    \n",
    "    \"\"\"Decode real aspa\"\"\"\n",
    "    real_params_values = [aspa_real[:16, 25+b:26+b] for b in range(len(real_params))]\n",
    "    real_params_mean  = [c.mean() for c in real_params_values]\n",
    "    real_params_mean = inverse_scale_params(real_params_mean)\n",
    "    \n",
    "    for d,param in enumerate(real_params):\n",
    "        if 'mixratio' in param:\n",
    "            real_params[param] = real_params_mean[d] # convert log back to regular scale\n",
    "        else:\n",
    "            real_params[param] = real_params_mean[d]\n",
    "\n",
    "    \"\"\"Calculate percentage difference\"\"\"\n",
    "    params_errors = dict.fromkeys(params_errors, 0) # copy dicts' keys, set all values to zero\n",
    "    for param in real_params:\n",
    "        y_hat = real_params[param]\n",
    "        y = inpainted_params[param]\n",
    "        \n",
    "        params_errors[param] = ((y - y_hat) / y_hat)*100\n",
    "\n",
    "    reals.append(real_params.copy()) # !!!!! for some reason dicts need to be copied, to be able to append to a list... otherwise the dicts inside the lists get update too\n",
    "    inpainteds.append(inpainted_params.copy())\n",
    "    errors.append(params_errors.copy())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for error in errors:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create one large df from all the dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for i in range(len(errors)):\n",
    "    print(reals[i])\n",
    "    df1 = pd.DataFrame(errors[i], index=[0])\n",
    "    df1.columns = [i+'_error' for i in df1.columns]\n",
    "\n",
    "    df2 = pd.DataFrame(reals[i], index=[0])\n",
    "    df2.columns = [i+'_real' for i in df2.columns]\n",
    "\n",
    "    df3 = pd.DataFrame(inpainteds[i], index=[0])\n",
    "    df3.columns = [i+'_inpainted' for i in df3.columns]\n",
    "\n",
    "    df = pd.concat([df1, df2, df3], axis=1)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors of all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [param for param in real_params.keys()]\n",
    "param = params[2]\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[param+'_real']\n",
    "y = df[param+'_inpainted']\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.xlabel('real')\n",
    "plt.ylabel('inpainted')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[param+'_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=((25,10)))\n",
    "for i,param in enumerate(params):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.title(param+'_error')\n",
    "    plt.hist(df[param+'_error'], bins=20)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig('plots//Validation error hist of '+str(len(images))+' images.png', dpi=1200)\n",
    "print('saved')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=((25,10)))\n",
    "for i,param in enumerate(params):\n",
    "    x = df[param+'_error']\n",
    "    y = df[param+'_real']\n",
    "    \n",
    "    plt.subplot(3,3,i+1)\n",
    "\n",
    "    plt.plot(x, y, '.')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Validation error of '+str(len(images))+' images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = params[2]\n",
    "df[[param+'_real', param+'_inpainted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "its = np.array(n_iteration)\n",
    "plt.hist(its)\n",
    "plt.grid()\n",
    "\n",
    "its.mean(), its.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
