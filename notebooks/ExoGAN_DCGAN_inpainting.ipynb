{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from keijzer_exogan import *\n",
    "\n",
    "# Set random seem for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "#%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"} # Make sure the axis background of plots is white, this is usefull for the black theme in JupyterLab\n",
    "#sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  1\n"
     ]
    }
   ],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"/datc/opschaler/brian/celeba_complete\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 0 # 0 when to_vram is enabled\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 1 # 2**11\n",
    "print('Batch size: ', batch_size)\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 32\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 1\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 32\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 32\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 1 \n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 2e-4\n",
    "lr_G = 2e-4\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "selected_gpus = [0]\n",
    "ngpu = len(selected_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark the below cell as code to use the celeba dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Grayscale(1),\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark the below cell as code to use the ExoGAN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Jupyterlab\\\\SRON-DCGAN\\\\notebooks'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  1\n"
     ]
    }
   ],
   "source": [
    "##### Creating custom Dataset classes\n",
    "path = 'F:\\Jupyterlab\\SRON-DCGAN\\data\\\\' #notice how you dont put the last folder in here...\n",
    "images = np.load(path+'first_chunks_25_percent_images.npy')\n",
    "\n",
    "shuffle = True\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(images) # shuffles the images\n",
    "\n",
    "images = images[:1] # select the first image, lateron this has to change to is load a 'selected' image to inpaint (instead of working from this excisting dataset)\n",
    "print('Number of images: ', len(images))\n",
    "\n",
    "dataset = numpy_dataset(data=images, to_vram=False) # to_vram pins it to all GPU's\n",
    "#dataset = numpy_dataset(data=images, to_vram=True, transform=transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])) # to_vram pins it to all GPU's\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1dcb8d898d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dcb8d572b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decide which device we want to run on\n",
    "# Seems to be the main devive, e.g. cuda:0 wont work when only gpu [2,3] are selected... is [2,3] then do cuda:2\n",
    "device = torch.device(\"cuda:\"+str(selected_gpus[0]) if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        \"\"\"\n",
    "        where (in_channels, out_channels, \n",
    "        kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "        \"\"\"\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            #1\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            #4\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            \n",
    "            #7\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout2d(0.5),\n",
    "            \n",
    "            #10\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf*1, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*1),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Go from 1x64x64 to 1x32x32\n",
    "            nn.Conv2d(ngf, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*1),\n",
    "            nn.ReLU(True),\n",
    "                        #10\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            #nn.ConvTranspose2d( ngf * 2, ngf*1, 4, 2, 1, bias=False),\n",
    "            #nn.BatchNorm2d(ngf*1),\n",
    "            #nn.ReLU(True),\n",
    "            \n",
    "            #13\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf*1, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Dropout2d(p=0.5)\n",
      "    (7): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Dropout2d(p=0.5)\n",
      "    (11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace)\n",
      "    (17): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (18): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, device_ids=selected_gpus, output_device=device) # select only gpu 0, 2, 3\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Dropout2d(p=0.5)\n",
      "    (7): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Dropout2d(p=0.5)\n",
      "    (11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace)\n",
      "    (17): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (18): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "noise.shape\n",
    "summary(netG, (100,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 1, 4, 2, 1, bias=False),\n",
    "            #nn.BatchNorm2d(ndf * 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 1, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 1, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (7): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (10): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (13): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (14): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    print('netD to cuda')\n",
    "    netD = nn.DataParallel(netD, device_ids=selected_gpus, output_device=device) # select only gpu 0, 2, 3\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torchsummary import summary\n",
    "summary(netD, (1,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)) # should be sgd\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D = True\n",
    "train_G = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of Generator(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Dropout2d(p=0.5)\n",
       "    (7): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Dropout2d(p=0.5)\n",
       "    (11): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace)\n",
       "    (17): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (18): Tanh()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved weights\n",
    "netG.load_state_dict(torch.load('netG_state_dict', map_location=device)) #net.module..load_... for parallel model , net.load_... for single gpu model\n",
    "netD.load_state_dict(torch.load('netD_state_dict', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(netG, (100,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(netD, (100,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting\n",
    "TODO: Implement the following\n",
    "\n",
    "The corrupted image $y$ is mapped to the closest $z$ in the latent representation space, this mapping is denoted as $\\hat{z}$.\n",
    "    \n",
    "$\\hat{z} = \\operatorname{arg\\,min}_z \\{ \\mathcal{L}_c(z |y, M) + \\mathcal{L}_p (z) \\}$\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathcal{L}_c(z |y, M) = || M \\bigodot G(z) - M \\bigodot y||_1 = || M \\bigodot (G(z)-y) ||_1 $\n",
    "\n",
    "with $c$ being contextual loss and $M$ being a binary mask with the same size as $y$,\n",
    "\n",
    "$\\mathcal{L}_p (z) = \\lambda \\operatorname{log}(1-D(G(z)))$\n",
    "\n",
    "with $p$ being perceptual loss and $D$ being the discriminator.\n",
    "\n",
    "Once $G(\\hat{z})$ is generated, the final solution $\\hat{x}$ is calculated as\n",
    "\n",
    "$\\hat{x} = \\operatorname{arg\\, min}_x ||\\nabla x - \\nabla G(\\hat{z}) ||^2_2$  \n",
    "\n",
    "(substitute $x_i = y_i$ for $M_i = 1$).\n",
    "\n",
    "-----\n",
    "\n",
    "$|| ... ||$ is done by `torch.norm()`.  \n",
    "$... \\bigodot ...$ is done by `torch.mul()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (re)define variables (for clarity)\n",
    "\n",
    "G = netG\n",
    "D = netD\n",
    "z = torch.randn(1, nz, 1, 1, requires_grad=True, device=device)\n",
    "\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 1 # number of iters to do for inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs): # epochs\n",
    "    for i, data in enumerate(dataloader, 0): # batches per epoch\n",
    "        real_cpu = data.to(device)\n",
    "        b_size = real_cpu.size(0) # this is one ofc, it's one image we're trying to inpaint\n",
    "        \n",
    "        print(\"data.shape: \", data.shape)\n",
    "        image = data.to(device) # select the image (Channel, Height, Width), this is the original unmasked input image\n",
    "        print(\"image.shape: \", image.shape)\n",
    "        \n",
    "        \"\"\"Mask the images manually, for testing pruposes\"\"\"\n",
    "        mask = torch.ones(size=image.shape).to(device) # create mask with 1's in the shape of image\n",
    "        print(\"mask.shape\", mask.shape)\n",
    "        \n",
    "        # use a random 'easy' mask\n",
    "        mask[:, 5:15, 5:15] = 0\n",
    "        \n",
    "        masked_image = torch.mul(image, mask).to(device) #image bigodot mask\n",
    "        \n",
    "        #plt.imshow(masked_image.cpu()[0], cmap='gray') # plot the masked image\n",
    "        \n",
    "        opt = optim.Adam([z], lr=lr)\n",
    "        \n",
    "        # what's v and m?\n",
    "        v = torch.tensor(0, dtype=torch.float32, device=device)\n",
    "        m = torch.tensor(0, dtype=torch.float32, device=device)\n",
    "        \n",
    "        \n",
    "        \"\"\"Start the inpainting process\"\"\"\n",
    "        for iteration in range(n_iters):\n",
    "            if z.grad is not None:\n",
    "                z_hat.grad.data.zero_()\n",
    "            \n",
    "            G.zero_grad()\n",
    "            D.zero_grad()\n",
    "            \n",
    "            \n",
    "            image_generated = G(z) # generated image G(z)\n",
    "            image_generated_masked = torch.mul(image_generated, mask) # G(z) bigodot M\n",
    "            image_generated_inpainted = torch.mul(image_generated, (1-mask))+masked_image\n",
    "            \n",
    "            print(\"image_generated_inpainted.shape : \",image_generated_inpainted.shape)\n",
    "            \n",
    "            t = image_generated_inpainted.detach().cpu()[0, 0, :, :]\n",
    "            \n",
    "            # TODO: why does this already look real?\n",
    "            plt.imshow(t, cmap='gray') # plot the masked image \n",
    "            \n",
    "            \"\"\"Calculate losses\"\"\"\n",
    "            loss_context = torch.norm(masked_image-image_generated_masked, p=1) #what's p=1?\n",
    "            discriminator_output = netD(real_cpu)\n",
    "            print(discriminator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
