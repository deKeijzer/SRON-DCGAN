{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Batch size:  64\n",
      "Number of GPUs used:  2\n",
      "Number of images:  512\n",
      "Initializing cuda...\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import time as t\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import gradcheck\n",
    "import torchvision.utils as vutils\n",
    "import time as time\n",
    "\n",
    "from torch import autograd\n",
    "\n",
    "import model_netD_regression as model\n",
    "import keijzer_exogan as ke\n",
    "\n",
    "# initialize random seeds\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed) \n",
    "#torch.set_num_threads(16)\n",
    "\n",
    "\"\"\"\n",
    "Local variables\n",
    "\"\"\"\n",
    "selected_gpus = [0,1] # Selected GPUs\n",
    "\n",
    "path = '/datb/16011015/ExoGAN_data/selection//' # Storage location of the train/test data\n",
    "\n",
    "print('Loading data...')\n",
    "images = np.load(path+'first_chunks_25_percent_images_v4.1.npy').astype('float32')\n",
    "\n",
    "images = images[:512] # select first ... images\n",
    "\n",
    "use_saved_weights = False\n",
    "\n",
    "\"\"\"\n",
    "Local variables that generally stay unchanged\n",
    "\"\"\"\n",
    "batch_size = 64 # 64\n",
    "num_epochs = 100\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "beta1 = 0.5 # beta1 for Adam\n",
    "beta2 = 0.9 # beta2 for Adam\n",
    "\n",
    "workers = 0 # Number of workers for dataloader, 0 when to_vram is enabled\n",
    "image_size = 32\n",
    "\n",
    "torch.backends.cudnn.benchmark=True # Uses udnn auto-tuner to find the best algorithm to use for your hardware, speeds up training by almost 50%\n",
    "\n",
    "\n",
    "print('Batch size: ', batch_size)\n",
    "ngpu = len(selected_gpus)\n",
    "print('Number of GPUs used: ', ngpu)\n",
    "\n",
    "\"\"\"\n",
    "Load data and prepare DataLoader\n",
    "\"\"\"\n",
    "shuffle = True\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(images) # shuffles the images\n",
    "\n",
    "print('Number of images: ', len(images))\n",
    "\n",
    "dataset = ke.numpy_dataset(data=images, to_vram=True) # to_vram pins it to all GPU's\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers, pin_memory=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load and setup models\n",
    "\"\"\"\n",
    "print('Initializing cuda...')\n",
    "# Initialize cuda\n",
    "device = torch.device(\"cuda:\"+str(selected_gpus[0]) if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Load models\n",
    "netD = model.Discriminator(ngpu).to(device)\n",
    "\n",
    "# Apply weights\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netD.apply(weights_init)\n",
    "\n",
    "if use_saved_weights:\n",
    "    try:\n",
    "        # Load saved weights\n",
    "        netD.load_state_dict(torch.load('gan_data//weights//netD_regression', map_location=device))\n",
    "        print('Succesfully loaded saved weights.')\n",
    "    except:\n",
    "        print('Could not load saved weights, using new ones.')\n",
    "        pass\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, device_ids=selected_gpus, output_device=device)\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Define input training stuff (fancy this up)\n",
    "\"\"\"\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2)) # should be sgd\n",
    "\n",
    "def save_progress(experiment_name, variable_name, progress_list):\n",
    "    path = 'gan_data//training_progress//'\n",
    "    progress_list = np.array(progress_list).astype('float32')\n",
    "    #file = open(path+variable_name+\"_\"+experiment_name+'.npy', mode=\"a\")\n",
    "    file_name = path+variable_name+\"_\"+experiment_name+'.npy'\n",
    "    file = np.load(file_name, mmap_mode='r+')\n",
    "    file = np.append(file, progress_list)\n",
    "    np.save(file_name, file)\n",
    "    #file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][0/8] \t train_loss = 0.00093 \t val_loss = 99999.00000 \t t = 0.190\n",
      "[1/100][0/8] \t train_loss = 0.00223 \t val_loss = 0.00220 \t t = 1.200\n",
      "[2/100][0/8] \t train_loss = 0.00296 \t val_loss = 0.00281 \t t = 1.202\n",
      "[3/100][0/8] \t train_loss = 0.00110 \t val_loss = 0.00109 \t t = 1.206\n",
      "[4/100][0/8] \t train_loss = 0.00013 \t val_loss = 0.00018 \t t = 1.202\n",
      "[5/100][0/8] \t train_loss = 0.00480 \t val_loss = 0.00476 \t t = 1.203\n",
      "[6/100][0/8] \t train_loss = 0.00179 \t val_loss = 0.00146 \t t = 1.194\n"
     ]
    }
   ],
   "source": [
    "MSELoss = nn.MSELoss()\n",
    "val_loss = 99999\n",
    "t1 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        #print('='*15)\n",
    "        aspa = data.to(device)\n",
    "        b_size = aspa.size(0)\n",
    "\n",
    "        \"\"\"Decode real values from aspa\"\"\"\n",
    "        #magnitude = [1e27, 1e3, 1, 1e7, 1, 1, 1]\n",
    "        params_real = []\n",
    "        for j in range(b_size):\n",
    "            params = [aspa[j][:16, 25+i:26+i].mean() for i in range(7)] # pixel values of normalized params in the aspa\n",
    "            params_real.append(params)\n",
    "\n",
    "        real = torch.tensor(params_real).to(device) # shape (b_size, params)\n",
    "\n",
    "        #print('real: %s' % real)\n",
    "        \n",
    "        \n",
    "        if i < 0.7*len(dataloader):\n",
    "            \"\"\"\n",
    "            Train D on train data\n",
    "            \"\"\"\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad_(True)\n",
    "\n",
    "            netD.zero_grad()\n",
    "            output = netD(aspa) # shape (b_size, params)\n",
    "            #print('predicted: %s' % output)\n",
    "            train_loss = MSELoss(output, real)\n",
    "            train_loss.backward()\n",
    "            optimizerD.step()\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Validate on val data\n",
    "            \"\"\"\n",
    "            output = netD(aspa)\n",
    "            val_loss = MSELoss(output, real)\n",
    "    \n",
    "    \n",
    "        \"\"\"\n",
    "        Print statistics\n",
    "        \"\"\"\n",
    "        train_loss.detach().cpu()\n",
    "        #print(loss)\n",
    "\n",
    "        if i % (8) == 0:\n",
    "            t2 = time.time()\n",
    "            print('[%d/%d][%d/%d] \\t train_loss = %.5f \\t val_loss = %.5f \\t t = %.3f' % \n",
    "                      (epoch, num_epochs, i, len(dataloader), train_loss, val_loss, (t2-t1) ))\n",
    "            t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 images\n"
     ]
    }
   ],
   "source": [
    "path = '/datb/16011015/ExoGAN_data/selection//' #notice how you dont put the last folder in here...\n",
    "images = np.load(path+'last_chunks_25_percent_images_v4.1.npy') # 4.1 is a random selection of 5k images\n",
    "print('Loaded %s images' % len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5424745 , 0.4580737 , 0.45168817, 0.44521517, 0.5125383 ,\n",
       "       0.42110914, 0.4804378 ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspa = torch.tensor(images[0]).reshape(1,1,32,32).to(device).float()\n",
    "\n",
    "output = netD(aspa).view(-1).detach().cpu().numpy()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11111111, -1.        ,  0.33333334,  0.7777778 ,  0.5555556 ,\n",
       "       -0.11111111,  0.11111111], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspa = aspa.detach().cpu().numpy().reshape(32,32)\n",
    "reals = np.array([aspa[:16, 25+i:26+i].mean() for i in range(7)])\n",
    "reals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ch4_mixratio': -2.9190914034843445,\n",
       " 'co2_mixratio': -3.0261180102825165,\n",
       " 'co_mixratio': -2.818467751145363,\n",
       " 'h2o_mixratio': -2.7061159014701843,\n",
       " 'planet_mass': 3.274969970035553e+27,\n",
       " 'planet_radius': 91291553.1051606,\n",
       " 'temp_profile': 1729.0368527173996}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = ke.decode_params_from_list(output)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ch4_mixratio': -3.333333298563957,\n",
       " 'co2_mixratio': -4.888888891786337,\n",
       " 'co_mixratio': -4.111111108213663,\n",
       " 'h2o_mixratio': -2.5555554628372192,\n",
       " 'planet_mass': 2.5306666657239197e+27,\n",
       " 'planet_radius': 99428978.10187936,\n",
       " 'temp_profile': 1000.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reals = ke.decode_params_from_list(params)\n",
    "reals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
